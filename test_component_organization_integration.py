#!/usr/bin/env python3
"""
ì»´í¬ë„ŒíŠ¸ë³„ ë°ì´í„°ì…‹ ì¡°ì§í™” ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸
ì „ì²´ íŒŒì´í”„ë¼ì¸ì—ì„œ ì»´í¬ë„ŒíŠ¸ ì¡°ì§í™” ê¸°ëŠ¥ì´ ì˜¬ë°”ë¥´ê²Œ ì‘ë™í•˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.
"""

import json
import logging
import tempfile
import asyncio
from pathlib import Path
from typing import List, Dict, Any

from unsloth_dataset import (
    UnslothDatasetGenerator,
    DatasetConfig,
    ComponentOrganizer,
    OrganizationConfig
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def create_comprehensive_test_samples() -> List[Dict[str, Any]]:
    """í¬ê´„ì ì¸ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    samples = []
    
    # ê° ì»´í¬ë„ŒíŠ¸ë³„ë¡œ ë‹¤ì–‘í•œ ìƒ˜í”Œ ìƒì„±
    component_samples = {
        "grid": [
            {
                "instruction": "DataGridì—ì„œ ë°ì´í„° ë°”ì¸ë”©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "response": "DataGridì—ì„œ ë°ì´í„° ë°”ì¸ë”©ì€ DataSource ì†ì„±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. List<T>, DataTable, BindingSource ë“±ì„ ì§€ì›í•©ë‹ˆë‹¤.",
                "source": "md_staging/grid/page_001.md",
                "quality_score": 0.9,
                "metadata": {"component": "grid", "page": 1}
            },
            {
                "instruction": "Grid ì»¨íŠ¸ë¡¤ì—ì„œ ì»¬ëŸ¼ì„ ë™ì ìœ¼ë¡œ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì€?",
                "response": "GridColumnì„ ìƒì„±í•˜ê³  Columns.Add() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì ìœ¼ë¡œ ì»¬ëŸ¼ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "source": "md_staging/grid/page_002.md",
                "quality_score": 0.85,
                "metadata": {"component": "grid", "page": 2}
            },
            {
                "instruction": "DataGridì—ì„œ ì…€ í¸ì§‘ì„ í™œì„±í™”í•˜ëŠ” ë°©ë²•ì€?",
                "response": "AllowEditing ì†ì„±ì„ trueë¡œ ì„¤ì •í•˜ê³  EditModeë¥¼ ì§€ì •í•˜ì—¬ ì…€ í¸ì§‘ì„ í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "source": "md_staging/grid/page_003.md",
                "quality_score": 0.88,
                "metadata": {"component": "grid", "page": 3}
            }
        ],
        "chart": [
            {
                "instruction": "Chart ì»¨íŠ¸ë¡¤ë¡œ ë§‰ëŒ€ ê·¸ë˜í”„ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
                "response": "ChartSeriesë¥¼ ìƒì„±í•˜ê³  Typeì„ Columnìœ¼ë¡œ ì„¤ì •í•œ í›„ ë°ì´í„°ë¥¼ ë°”ì¸ë”©í•©ë‹ˆë‹¤.",
                "source": "md_staging/chart/page_001.md",
                "quality_score": 0.88,
                "metadata": {"component": "chart", "page": 1}
            },
            {
                "instruction": "ì°¨íŠ¸ì—ì„œ ë²”ë¡€ë¥¼ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•˜ëŠ” ë°©ë²•ì€?",
                "response": "Legend ì†ì„±ì„ í†µí•´ ìœ„ì¹˜, ìƒ‰ìƒ, í°íŠ¸ ë“±ì„ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "source": "md_staging/chart/page_002.md",
                "quality_score": 0.82,
                "metadata": {"component": "chart", "page": 2}
            },
            {
                "instruction": "ì°¨íŠ¸ì— ì• ë‹ˆë©”ì´ì…˜ íš¨ê³¼ë¥¼ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì€?",
                "response": "EnableAnimation ì†ì„±ì„ trueë¡œ ì„¤ì •í•˜ê³  AnimationDurationì„ ì§€ì •í•©ë‹ˆë‹¤.",
                "source": "md_staging/chart/page_003.md",
                "quality_score": 0.86,
                "metadata": {"component": "chart", "page": 3}
            }
        ],
        "DocIo": [
            {
                "instruction": "DocIOë¥¼ ì‚¬ìš©í•´ì„œ Word ë¬¸ì„œë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì€?",
                "response": "WordDocument í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆ ë¬¸ì„œë¥¼ ìƒì„±í•˜ê³  í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ ë“±ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "source": "md_staging/DocIo/page_001.md",
                "quality_score": 0.91,
                "metadata": {"component": "DocIo", "page": 1}
            },
            {
                "instruction": "Word ë¬¸ì„œì—ì„œ í‘œë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "response": "IWTable ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œë¥¼ ìƒì„±í•˜ê³  í–‰ê³¼ ì—´ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "source": "md_staging/DocIo/page_002.md",
                "quality_score": 0.87,
                "metadata": {"component": "DocIo", "page": 2}
            }
        ],
        "pdf": [
            {
                "instruction": "PDF ë¬¸ì„œë¥¼ ìƒì„±í•˜ëŠ” ê¸°ë³¸ ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
                "response": "PdfDocument í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆ PDFë¥¼ ìƒì„±í•˜ê³  í˜ì´ì§€ì™€ ì½˜í…ì¸ ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.",
                "source": "md_staging/pdf/page_001.md",
                "quality_score": 0.89,
                "metadata": {"component": "pdf", "page": 1}
            },
            {
                "instruction": "PDFì— ì´ë¯¸ì§€ë¥¼ ì‚½ì…í•˜ëŠ” ë°©ë²•ì€?",
                "response": "PdfBitmap í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ë¡œë“œí•˜ê³  DrawImage ë©”ì„œë“œë¡œ ì‚½ì…í•©ë‹ˆë‹¤.",
                "source": "md_staging/pdf/page_002.md",
                "quality_score": 0.84,
                "metadata": {"component": "pdf", "page": 2}
            }
        ],
        "XlsIO": [
            {
                "instruction": "XlsIOë¡œ Excel íŒŒì¼ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì€?",
                "response": "ExcelEngineì„ ìƒì„±í•˜ê³  IWorkbookì„ í†µí•´ ìƒˆ Excel íŒŒì¼ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "source": "md_staging/XlsIO/page_001.md",
                "quality_score": 0.92,
                "metadata": {"component": "XlsIO", "page": 1}
            },
            {
                "instruction": "Excelì—ì„œ ì°¨íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
                "response": "IChartShapeì„ ì‚¬ìš©í•˜ì—¬ ì›Œí¬ì‹œíŠ¸ì— ì°¨íŠ¸ë¥¼ ì¶”ê°€í•˜ê³  ë°ì´í„° ë²”ìœ„ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.",
                "source": "md_staging/XlsIO/page_002.md",
                "quality_score": 0.88,
                "metadata": {"component": "XlsIO", "page": 2}
            }
        ],
        "schedule": [
            {
                "instruction": "ìŠ¤ì¼€ì¤„ëŸ¬ì—ì„œ ì•½ì†ì„ ì¶”ê°€í•˜ëŠ” ë°©ë²•ì€?",
                "response": "ScheduleAppointment ê°ì²´ë¥¼ ìƒì„±í•˜ê³  Appointments ì»¬ë ‰ì…˜ì— ì¶”ê°€í•©ë‹ˆë‹¤.",
                "source": "md_staging/schedule/page_001.md",
                "quality_score": 0.84,
                "metadata": {"component": "schedule", "page": 1}
            }
        ],
        "Gauge": [
            {
                "instruction": "Gauge ì»¨íŠ¸ë¡¤ë¡œ ì›í˜• ê²Œì´ì§€ë¥¼ ë§Œë“œëŠ” ë°©ë²•ì€?",
                "response": "CircularGauge ì»¨íŠ¸ë¡¤ì„ ì‚¬ìš©í•˜ê³  Scaleê³¼ Pointerë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.",
                "source": "md_staging/Gauge/page_001.md",
                "quality_score": 0.81,
                "metadata": {"component": "Gauge", "page": 1}
            }
        ]
    }
    
    # ëª¨ë“  ì»´í¬ë„ŒíŠ¸ ìƒ˜í”Œì„ í•˜ë‚˜ì˜ ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•©
    for component_name, component_samples_list in component_samples.items():
        samples.extend(component_samples_list)
    
    # ë¯¸ë¶„ë¥˜ ìƒ˜í”Œ ì¶”ê°€
    samples.extend([
        {
            "instruction": "ì¼ë°˜ì ì¸ WinForms ê°œë°œ íŒì„ ì•Œë ¤ì£¼ì„¸ìš”.",
            "response": "ì„±ëŠ¥ ìµœì í™”, ë©”ëª¨ë¦¬ ê´€ë¦¬, ì‚¬ìš©ì ê²½í—˜ ê°œì„  ë“±ì˜ íŒì„ ì œê³µí•©ë‹ˆë‹¤.",
            "source": "general_tips.md",
            "quality_score": 0.75,
            "metadata": {"page": 1}
        },
        {
            "instruction": "Syncfusion ì»¨íŠ¸ë¡¤ì˜ í…Œë§ˆë¥¼ ë³€ê²½í•˜ëŠ” ë°©ë²•ì€?",
            "response": "ThemeManagerë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ì—­ í…Œë§ˆë¥¼ ì„¤ì •í•˜ê±°ë‚˜ ê°œë³„ ì»¨íŠ¸ë¡¤ì˜ í…Œë§ˆë¥¼ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            "source": "theming_guide.md",
            "quality_score": 0.78,
            "metadata": {"page": 1}
        }
    ])
    
    return samples


def test_component_organization_comprehensive():
    """í¬ê´„ì ì¸ ì»´í¬ë„ŒíŠ¸ ì¡°ì§í™” í…ŒìŠ¤íŠ¸"""
    logger.info("=== í¬ê´„ì ì¸ ì»´í¬ë„ŒíŠ¸ ì¡°ì§í™” í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    # í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìƒì„±
    samples = create_comprehensive_test_samples()
    logger.info(f"í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(samples)}")
    
    # ì¡°ì§í™” ì„¤ì •
    org_config = OrganizationConfig(
        min_samples_per_component=1,
        max_samples_per_component=10,
        enable_hierarchical_grouping=True,
        enable_category_balancing=True,
        priority_based_sampling=True,
        output_separate_files=True,
        include_mixed_category=True,
        mixed_category_ratio=0.1
    )
    
    # ì»´í¬ë„ŒíŠ¸ ì¡°ì§í™”ê¸° ìƒì„±
    organizer = ComponentOrganizer(org_config)
    
    # 1. ì»´í¬ë„ŒíŠ¸ë³„ ì¡°ì§í™”
    logger.info("--- 1. ì»´í¬ë„ŒíŠ¸ë³„ ì¡°ì§í™” ---")
    component_datasets = organizer.organize_by_components(samples)
    
    logger.info(f"ì¡°ì§í™”ëœ ì»´í¬ë„ŒíŠ¸ ìˆ˜: {len(component_datasets)}")
    for component_name, dataset in component_datasets.items():
        logger.info(f"  {component_name} ({dataset.category}): {len(dataset.samples)} ìƒ˜í”Œ, "
                   f"í‰ê·  í’ˆì§ˆ: {dataset.metadata.get('avg_quality_score', 0):.3f}")
    
    # 2. ì¹´í…Œê³ ë¦¬ë³„ ì¡°ì§í™”
    logger.info("--- 2. ì¹´í…Œê³ ë¦¬ë³„ ì¡°ì§í™” ---")
    category_datasets = organizer.organize_by_categories(component_datasets)
    
    logger.info(f"ì¡°ì§í™”ëœ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(category_datasets)}")
    for category, datasets_list in category_datasets.items():
        total_samples = sum(len(ds.samples) for ds in datasets_list)
        avg_quality = sum(ds.metadata.get("avg_quality_score", 0) for ds in datasets_list) / len(datasets_list)
        logger.info(f"  {category}: {len(datasets_list)} ì»´í¬ë„ŒíŠ¸, {total_samples} ìƒ˜í”Œ, "
                   f"í‰ê·  í’ˆì§ˆ: {avg_quality:.3f}")
    
    # 3. ê· í˜•ì¡íŒ ë¶„í• 
    logger.info("--- 3. ê· í˜•ì¡íŒ ë¶„í•  ---")
    train_datasets, val_datasets = organizer.create_balanced_splits(component_datasets, 0.8)
    
    logger.info("í›ˆë ¨/ê²€ì¦ ë¶„í•  ê²°ê³¼:")
    for component_name in component_datasets.keys():
        train_count = len(train_datasets[component_name].samples)
        val_count = len(val_datasets[component_name].samples)
        total_count = train_count + val_count
        train_ratio = train_count / total_count if total_count > 0 else 0
        logger.info(f"  {component_name}: í›ˆë ¨ {train_count}, ê²€ì¦ {val_count} (ë¹„ìœ¨: {train_ratio:.2f})")
    
    # 4. ì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” ë°ì´í„°ì…‹
    logger.info("--- 4. ì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” ë°ì´í„°ì…‹ ---")
    category_samples = organizer.create_category_specific_datasets(category_datasets, 5)
    
    logger.info("ì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” ë°ì´í„°ì…‹:")
    for category, samples_list in category_samples.items():
        components = set(sample.get("component", "unknown") for sample in samples_list)
        logger.info(f"  {category}: {len(samples_list)} ìƒ˜í”Œ, ì»´í¬ë„ŒíŠ¸: {', '.join(components)}")
    
    # 5. ì¡°ì§í™” ë¦¬í¬íŠ¸
    logger.info("--- 5. ì¡°ì§í™” ë¦¬í¬íŠ¸ ---")
    report = organizer.generate_organization_report(component_datasets, category_datasets)
    
    logger.info("ì¡°ì§í™” ë¦¬í¬íŠ¸ ìš”ì•½:")
    logger.info(f"  ì´ ì»´í¬ë„ŒíŠ¸: {report['summary']['total_components']}")
    logger.info(f"  ì´ ì¹´í…Œê³ ë¦¬: {report['summary']['total_categories']}")
    logger.info(f"  ì´ ìƒ˜í”Œ: {report['summary']['total_samples']}")
    
    if report['recommendations']:
        logger.info("ê¶Œì¥ì‚¬í•­:")
        for rec in report['recommendations']:
            logger.info(f"  - {rec}")
    
    # 6. ê²€ì¦
    logger.info("--- 6. ê²°ê³¼ ê²€ì¦ ---")
    
    # ëª¨ë“  ìƒ˜í”Œì´ ë¶„ë¥˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
    total_organized_samples = sum(len(ds.samples) for ds in component_datasets.values())
    assert total_organized_samples == len(samples), f"ìƒ˜í”Œ ìˆ˜ ë¶ˆì¼ì¹˜: {total_organized_samples} != {len(samples)}"
    
    # ê° ì¹´í…Œê³ ë¦¬ì— ì ì ˆí•œ ì»´í¬ë„ŒíŠ¸ê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
    expected_categories = {
        "data_display": ["grid"],
        "visualization": ["chart", "Gauge"],
        "document_processing": ["DocIo", "pdf", "XlsIO"],
        "scheduling": ["schedule"],
        "mixed": ["mixed"]
    }
    
    for category, expected_components in expected_categories.items():
        if category in category_datasets:
            actual_components = [ds.component_name for ds in category_datasets[category]]
            for expected_comp in expected_components:
                if expected_comp in component_datasets:
                    assert expected_comp in actual_components, f"ì»´í¬ë„ŒíŠ¸ {expected_comp}ê°€ ì¹´í…Œê³ ë¦¬ {category}ì— ì—†ìŒ"
    
    logger.info("âœ… ëª¨ë“  ê²€ì¦ í†µê³¼")
    logger.info("=== í¬ê´„ì ì¸ ì»´í¬ë„ŒíŠ¸ ì¡°ì§í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
    return True


def test_integrated_dataset_generation_with_organization():
    """ì»´í¬ë„ŒíŠ¸ ì¡°ì§í™”ê°€ í¬í•¨ëœ í†µí•© ë°ì´í„°ì…‹ ìƒì„± í…ŒìŠ¤íŠ¸"""
    logger.info("=== í†µí•© ë°ì´í„°ì…‹ ìƒì„± + ì»´í¬ë„ŒíŠ¸ ì¡°ì§í™” í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        # ë°ì´í„°ì…‹ ì„¤ì • (ì»´í¬ë„ŒíŠ¸ ì¡°ì§í™” í¬í•¨)
        config = DatasetConfig(
            target_count=50,
            max_seq_length=8192,
            train_test_split=0.8,
            formats=["sharegpt", "alpaca", "openai"],
            min_tokens=10,
            max_tokens=8192,
            output_dir=temp_dir,
            test_mode=True,
            enable_component_organization=True,
            create_component_specific_datasets=True,
            create_category_specific_datasets=True,
            component_organization=OrganizationConfig(
                min_samples_per_component=1,
                max_samples_per_component=20,
                enable_hierarchical_grouping=True,
                enable_category_balancing=True,
                priority_based_sampling=True,
                output_separate_files=True,
                include_mixed_category=True,
                mixed_category_ratio=0.1
            )
        )
        
        # ë°ì´í„°ì…‹ ìƒì„±ê¸° ìƒì„±
        generator = UnslothDatasetGenerator(config)
        
        # í…ŒìŠ¤íŠ¸ ìƒ˜í”Œë¡œ ë°ì´í„°ì…‹ ìƒì„±
        samples = create_comprehensive_test_samples()
        result = generator.generate_from_samples(samples, "comprehensive_test_dataset")
        
        # ê²°ê³¼ ê²€ì¦
        logger.info("ìƒì„±ëœ ë°ì´í„°ì…‹ ì •ë³´:")
        logger.info(f"  ê¸°ë³¸ í›ˆë ¨ ë°ì´í„° í¬ë§·: {len(result.train_data)}")
        logger.info(f"  ê¸°ë³¸ ê²€ì¦ ë°ì´í„° í¬ë§·: {len(result.validation_data)}")
        logger.info(f"  ì»´í¬ë„ŒíŠ¸ ë°ì´í„°ì…‹: {len(result.component_datasets)}")
        logger.info(f"  ì¹´í…Œê³ ë¦¬ ë°ì´í„°ì…‹: {len(result.category_datasets)}")
        
        # ê° í¬ë§·ë³„ ê¸°ë³¸ ë°ì´í„°ì…‹ í™•ì¸
        for format_type, train_data in result.train_data.items():
            val_data = result.validation_data.get(format_type, [])
            logger.info(f"  {format_type}: í›ˆë ¨ {len(train_data)}, ê²€ì¦ {len(val_data)}")
        
        # ì»´í¬ë„ŒíŠ¸ë³„ ë°ì´í„°ì…‹ í™•ì¸
        if result.component_train_data:
            logger.info("ì»´í¬ë„ŒíŠ¸ë³„ í›ˆë ¨ ë°ì´í„°:")
            for format_type, component_data in result.component_train_data.items():
                logger.info(f"  {format_type} í¬ë§·:")
                for component, samples_list in component_data.items():
                    logger.info(f"    {component}: {len(samples_list)} ìƒ˜í”Œ")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„°ì…‹ í™•ì¸
        if result.category_specific_data:
            logger.info("ì¹´í…Œê³ ë¦¬ë³„ íŠ¹í™” ë°ì´í„°:")
            for format_type, category_data in result.category_specific_data.items():
                logger.info(f"  {format_type} í¬ë§·:")
                for category, samples_list in category_data.items():
                    logger.info(f"    {category}: {len(samples_list)} ìƒ˜í”Œ")
        
        # íŒŒì¼ ì €ì¥ ë° ê²€ì¦
        saved_paths = generator.save_datasets(result, "comprehensive_test_dataset")
        logger.info(f"ì €ì¥ëœ íŒŒì¼ ìˆ˜: {len(saved_paths)}")
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        for file_type, file_path in saved_paths.items():
            assert Path(file_path).exists(), f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŒ: {file_path}"
        
        # ì»´í¬ë„ŒíŠ¸ë³„ íŒŒì¼ í™•ì¸
        component_files = [path for key, path in saved_paths.items() if "component_" in key]
        category_files = [path for key, path in saved_paths.items() if "category_" in key]
        
        logger.info(f"ì»´í¬ë„ŒíŠ¸ë³„ íŒŒì¼: {len(component_files)}")
        logger.info(f"ì¹´í…Œê³ ë¦¬ë³„ íŒŒì¼: {len(category_files)}")
        
        # ì¡°ì§í™” ë¦¬í¬íŠ¸ í™•ì¸
        if result.organization_report:
            logger.info("ì¡°ì§í™” ë¦¬í¬íŠ¸:")
            logger.info(f"  ì»´í¬ë„ŒíŠ¸ ìˆ˜: {result.organization_report['summary']['total_components']}")
            logger.info(f"  ì¹´í…Œê³ ë¦¬ ìˆ˜: {result.organization_report['summary']['total_categories']}")
            logger.info(f"  ì´ ìƒ˜í”Œ: {result.organization_report['summary']['total_samples']}")
        
        # ìƒ˜í”Œ íŒŒì¼ ë‚´ìš© ê²€ì¦
        logger.info("--- ìƒ˜í”Œ íŒŒì¼ ë‚´ìš© ê²€ì¦ ---")
        
        # ê¸°ë³¸ ë°ì´í„°ì…‹ íŒŒì¼ ê²€ì¦
        for format_type in config.formats:
            train_file = Path(temp_dir) / f"comprehensive_test_dataset_{format_type}_train" / "train.jsonl"
            if train_file.exists():
                with open(train_file, 'r', encoding='utf-8') as f:
                    train_samples = [json.loads(line) for line in f]
                logger.info(f"  {format_type} í›ˆë ¨ íŒŒì¼: {len(train_samples)} ìƒ˜í”Œ")
                
                # ì²« ë²ˆì§¸ ìƒ˜í”Œ êµ¬ì¡° í™•ì¸
                if train_samples:
                    sample = train_samples[0]
                    if format_type == "sharegpt":
                        assert "conversations" in sample, "ShareGPT í¬ë§·ì— conversations í•„ë“œê°€ ì—†ìŒ"
                    elif format_type == "alpaca":
                        assert "instruction" in sample and "output" in sample, "Alpaca í¬ë§· í•„ë“œê°€ ì—†ìŒ"
                    elif format_type == "openai":
                        assert "messages" in sample, "OpenAI í¬ë§·ì— messages í•„ë“œê°€ ì—†ìŒ"
        
        # ì»´í¬ë„ŒíŠ¸ë³„ íŒŒì¼ ê²€ì¦
        components_dir = Path(temp_dir) / "components"
        if components_dir.exists():
            for component_dir in components_dir.iterdir():
                if component_dir.is_dir():
                    logger.info(f"  ì»´í¬ë„ŒíŠ¸ '{component_dir.name}' ë””ë ‰í† ë¦¬ í™•ì¸ë¨")
                    
                    # ë©”íƒ€ë°ì´í„° íŒŒì¼ í™•ì¸
                    metadata_file = component_dir / "metadata.json"
                    if metadata_file.exists():
                        with open(metadata_file, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                        logger.info(f"    ë©”íƒ€ë°ì´í„°: {metadata.get('sample_count', 0)} ìƒ˜í”Œ")
        
        logger.info("âœ… ëª¨ë“  íŒŒì¼ ê²€ì¦ í†µê³¼")
        logger.info("=== í†µí•© ë°ì´í„°ì…‹ ìƒì„± + ì»´í¬ë„ŒíŠ¸ ì¡°ì§í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
        return True


def test_performance_and_scalability():
    """ì„±ëŠ¥ ë° í™•ì¥ì„± í…ŒìŠ¤íŠ¸"""
    logger.info("=== ì„±ëŠ¥ ë° í™•ì¥ì„± í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    import time
    
    # ëŒ€ëŸ‰ ìƒ˜í”Œ ìƒì„± (ì‹¤ì œ í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜)
    base_samples = create_comprehensive_test_samples()
    large_samples = []
    
    # ìƒ˜í”Œì„ ë³µì œí•˜ì—¬ ëŒ€ëŸ‰ ë°ì´í„°ì…‹ ìƒì„±
    for i in range(10):  # 10ë°° í™•ì¥
        for sample in base_samples:
            new_sample = sample.copy()
            new_sample["instruction"] = f"[ë³µì œ {i+1}] {sample['instruction']}"
            new_sample["metadata"] = {**sample.get("metadata", {}), "replica": i+1}
            large_samples.append(new_sample)
    
    logger.info(f"ëŒ€ëŸ‰ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜: {len(large_samples)}")
    
    # ì„±ëŠ¥ ì¸¡ì •
    start_time = time.time()
    
    # ì¡°ì§í™” ì„¤ì •
    org_config = OrganizationConfig(
        min_samples_per_component=5,
        max_samples_per_component=50,
        enable_hierarchical_grouping=True,
        enable_category_balancing=True,
        priority_based_sampling=True,
        output_separate_files=True,
        include_mixed_category=True,
        mixed_category_ratio=0.1
    )
    
    # ì»´í¬ë„ŒíŠ¸ ì¡°ì§í™”ê¸° ìƒì„±
    organizer = ComponentOrganizer(org_config)
    
    # ì¡°ì§í™” ìˆ˜í–‰
    component_datasets = organizer.organize_by_components(large_samples)
    category_datasets = organizer.organize_by_categories(component_datasets)
    
    # ì„±ëŠ¥ ì¸¡ì • ì™„ë£Œ
    end_time = time.time()
    processing_time = end_time - start_time
    
    logger.info(f"ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
    logger.info(f"ì´ˆë‹¹ ì²˜ë¦¬ ìƒ˜í”Œ ìˆ˜: {len(large_samples) / processing_time:.1f}")
    logger.info(f"ì¡°ì§í™”ëœ ì»´í¬ë„ŒíŠ¸ ìˆ˜: {len(component_datasets)}")
    logger.info(f"ì¡°ì§í™”ëœ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(category_datasets)}")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
    import psutil
    import os
    
    process = psutil.Process(os.getpid())
    memory_usage = process.memory_info().rss / 1024 / 1024  # MB
    logger.info(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_usage:.1f} MB")
    
    # ì„±ëŠ¥ ê¸°ì¤€ ê²€ì¦
    assert processing_time < 30, f"ì²˜ë¦¬ ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼: {processing_time:.2f}ì´ˆ"
    assert len(large_samples) / processing_time > 10, f"ì²˜ë¦¬ ì†ë„ê°€ ë„ˆë¬´ ëŠë¦¼: {len(large_samples) / processing_time:.1f} ìƒ˜í”Œ/ì´ˆ"
    
    logger.info("âœ… ì„±ëŠ¥ ë° í™•ì¥ì„± í…ŒìŠ¤íŠ¸ í†µê³¼")
    logger.info("=== ì„±ëŠ¥ ë° í™•ì¥ì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
    return True


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    logger.info("ì»´í¬ë„ŒíŠ¸ë³„ ë°ì´í„°ì…‹ ì¡°ì§í™” ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        test_component_organization_comprehensive()
        test_integrated_dataset_generation_with_organization()
        test_performance_and_scalability()
        
        logger.info("ğŸ‰ ëª¨ë“  í†µí•© í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        logger.info("ì»´í¬ë„ŒíŠ¸ë³„ ë°ì´í„°ì…‹ ì¡°ì§í™” ê¸°ëŠ¥ì´ ì˜¬ë°”ë¥´ê²Œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)