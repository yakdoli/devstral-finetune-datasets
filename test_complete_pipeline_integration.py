#!/usr/bin/env python3
"""
ì „ì²´ íŒŒì´í”„ë¼ì¸ ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸
ëª¨ë“  ëª¨ë“ˆì„ í†µí•©í•˜ì—¬ end-to-end í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
"""

import asyncio
import json
import logging
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class CompletePipelineIntegrationTest:
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.test_results = {}
        self.performance_metrics = {}
        self.start_time = None
        self.end_time = None  
  async def run_complete_test(self) -> Dict[str, Any]:
        """ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("=== ì „ì²´ íŒŒì´í”„ë¼ì¸ ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        self.start_time = datetime.now()
        
        try:
            # 1. í™˜ê²½ ë° ì˜ì¡´ì„± ê²€ì¦
            await self._test_environment_setup()
            
            # 2. MD íŒŒì¼ ì²˜ë¦¬ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸
            await self._test_md_processing()
            
            # 3. Qdrant ì—°ë™ í…ŒìŠ¤íŠ¸
            await self._test_qdrant_integration()
            
            # 4. OpenAI ì—°ë™ í…ŒìŠ¤íŠ¸
            await self._test_openai_integration()
            
            # 5. ë°ì´í„°ì…‹ ìƒì„± í…ŒìŠ¤íŠ¸
            await self._test_dataset_generation()
            
            # 6. í’ˆì§ˆ ê²€ì¦ í…ŒìŠ¤íŠ¸
            await self._test_quality_validation()
            
            # 7. ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
            await self._test_complete_pipeline()
            
            # 8. ì„±ëŠ¥ ê²€ì¦
            await self._test_performance()
            
            # 9. Unsloth í˜¸í™˜ì„± ê²€ì¦
            await self._test_unsloth_compatibility()
            
            self.end_time = datetime.now()
            
            # 10. ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
            return await self._generate_final_report()
            
        except Exception as e:
            logger.error(f"í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {"status": "failed", "error": str(e)}
    
    async def _test_environment_setup(self):
        """í™˜ê²½ ë° ì˜ì¡´ì„± ê²€ì¦"""
        logger.info("1. í™˜ê²½ ë° ì˜ì¡´ì„± ê²€ì¦ ì¤‘...")
        
        test_result = {
            "status": "success",
            "modules_available": {},
            "config_files": {},
            "directories": {}
        }
        
        # í•„ìˆ˜ ëª¨ë“ˆ ê°€ìš©ì„± í™•ì¸
        required_modules = [
            "md_processor", "openai_connector", "qdrant_connector",
            "unsloth_dataset", "quality_validator", "async_pipeline",
            "logging_system", "config"
        ]
        
        for module_name in required_modules:
            try:
                __import__(module_name)
                test_result["modules_available"][module_name] = True
                logger.info(f"  âœ“ {module_name} ëª¨ë“ˆ ì‚¬ìš© ê°€ëŠ¥")
            except ImportError as e:
                test_result["modules_available"][module_name] = False
                logger.warning(f"  âš  {module_name} ëª¨ë“ˆ ë¶ˆê°€ìš©: {e}")
        
        # ì„¤ì • íŒŒì¼ í™•ì¸
        config_files = ["config.yaml", "requirements.txt"]
        for config_file in config_files:
            if Path(config_file).exists():
                test_result["config_files"][config_file] = True
                logger.info(f"  âœ“ {config_file} íŒŒì¼ ì¡´ì¬")
            else:
                test_result["config_files"][config_file] = False
                logger.warning(f"  âš  {config_file} íŒŒì¼ ì—†ìŒ")
        
        # í•„ìˆ˜ ë””ë ‰í† ë¦¬ í™•ì¸
        required_dirs = ["md_staging", "output"]
        for dir_name in required_dirs:
            if Path(dir_name).exists():
                test_result["directories"][dir_name] = True
                logger.info(f"  âœ“ {dir_name} ë””ë ‰í† ë¦¬ ì¡´ì¬")
            else:
                test_result["directories"][dir_name] = False
                logger.warning(f"  âš  {dir_name} ë””ë ‰í† ë¦¬ ì—†ìŒ")
        
        self.test_results["environment_setup"] = test_result
        logger.info("âœ“ í™˜ê²½ ê²€ì¦ ì™„ë£Œ")    a
sync def _test_md_processing(self):
        """MD íŒŒì¼ ì²˜ë¦¬ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸"""
        logger.info("2. MD íŒŒì¼ ì²˜ë¦¬ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        test_result = {
            "status": "success",
            "files_processed": 0,
            "processing_time": 0,
            "errors": []
        }
        
        try:
            from md_processor import create_md_processor, MDProcessorConfig
            
            # í…ŒìŠ¤íŠ¸ìš© ì„¤ì •
            config = MDProcessorConfig(
                base_paths=["md_staging"],
                output_path="output/test_md_processing",
                max_files=10,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì œí•œ
                enable_api_extraction=True,
                enable_code_extraction=True
            )
            
            processor = create_md_processor(config)
            
            start_time = time.time()
            documents = await processor.process_documents_async()
            end_time = time.time()
            
            test_result["files_processed"] = len(documents)
            test_result["processing_time"] = end_time - start_time
            
            if documents:
                logger.info(f"  âœ“ {len(documents)}ê°œ ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ")
                logger.info(f"  âœ“ ì²˜ë¦¬ ì‹œê°„: {test_result['processing_time']:.2f}ì´ˆ")
            else:
                test_result["status"] = "warning"
                test_result["errors"].append("ì²˜ë¦¬ëœ ë¬¸ì„œê°€ ì—†ìŒ")
                logger.warning("  âš  ì²˜ë¦¬ëœ ë¬¸ì„œê°€ ì—†ìŒ")
            
        except Exception as e:
            test_result["status"] = "failed"
            test_result["errors"].append(str(e))
            logger.error(f"  âœ— MD ì²˜ë¦¬ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        self.test_results["md_processing"] = test_result
        logger.info("âœ“ MD ì²˜ë¦¬ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    async def _test_qdrant_integration(self):
        """Qdrant ì—°ë™ í…ŒìŠ¤íŠ¸"""
        logger.info("3. Qdrant ì—°ë™ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        test_result = {
            "status": "success",
            "connection_status": False,
            "search_results": 0,
            "response_time": 0,
            "errors": []
        }
        
        try:
            from qdrant_connector import create_qdrant_connector, QdrantConfig
            
            config = QdrantConfig(
                host="100.88.88.88",
                port=6333,
                collection_name="ws-7491d651ae044c78",
                timeout=10
            )
            
            connector = create_qdrant_connector(config)
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            start_time = time.time()
            connection_result = await connector.test_connection()
            end_time = time.time()
            
            test_result["connection_status"] = connection_result.get("status") == "success"
            test_result["response_time"] = end_time - start_time
            
            if test_result["connection_status"]:
                logger.info(f"  âœ“ Qdrant ì—°ê²° ì„±ê³µ (ì‘ë‹µì‹œê°„: {test_result['response_time']:.2f}ì´ˆ)")
                
                # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
                search_results = await connector.search_documents(
                    query="Syncfusion GridControl data binding",
                    limit=5
                )
                
                test_result["search_results"] = len(search_results)
                logger.info(f"  âœ“ ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ")
            else:
                test_result["status"] = "failed"
                test_result["errors"].append("Qdrant ì—°ê²° ì‹¤íŒ¨")
                logger.error("  âœ— Qdrant ì—°ê²° ì‹¤íŒ¨")
            
        except Exception as e:
            test_result["status"] = "failed"
            test_result["errors"].append(str(e))
            logger.error(f"  âœ— Qdrant ì—°ë™ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        self.test_results["qdrant_integration"] = test_result
        logger.info("âœ“ Qdrant ì—°ë™ í…ŒìŠ¤íŠ¸ ì™„ë£Œ") 
   async def _test_openai_integration(self):
        """OpenAI ì—°ë™ í…ŒìŠ¤íŠ¸"""
        logger.info("4. OpenAI ì—°ë™ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        test_result = {
            "status": "success",
            "connection_status": False,
            "conversations_generated": 0,
            "avg_response_time": 0,
            "errors": []
        }
        
        try:
            from openai_connector import create_openai_client, OpenAIAPIClientConfig
            
            config = OpenAIAPIClientConfig(
                endpoint="http://123.37.28.120:9997/v1",
                model="qwen2.5-vl-instruct",
                api_key="test-key",
                timeout=30
            )
            
            client = create_openai_client(config)
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            async with client as api_client:
                start_time = time.time()
                health = await api_client.test_connection()
                end_time = time.time()
                
                test_result["connection_status"] = health.get("status") == "success"
                test_result["avg_response_time"] = end_time - start_time
                
                if test_result["connection_status"]:
                    logger.info(f"  âœ“ OpenAI API ì—°ê²° ì„±ê³µ (ì‘ë‹µì‹œê°„: {test_result['avg_response_time']:.2f}ì´ˆ)")
                    
                    # ê°„ë‹¨í•œ ëŒ€í™” ìƒì„± í…ŒìŠ¤íŠ¸
                    test_prompt = [
                        {"role": "system", "content": "You are a helpful assistant for Syncfusion WinForms development."},
                        {"role": "user", "content": "How do I bind data to a GridControl?"}
                    ]
                    
                    response = await api_client.generate_response(test_prompt)
                    
                    if response and response.get("content"):
                        test_result["conversations_generated"] = 1
                        logger.info("  âœ“ í…ŒìŠ¤íŠ¸ ëŒ€í™” ìƒì„± ì„±ê³µ")
                    else:
                        test_result["status"] = "warning"
                        test_result["errors"].append("ëŒ€í™” ìƒì„± ì‹¤íŒ¨")
                        logger.warning("  âš  í…ŒìŠ¤íŠ¸ ëŒ€í™” ìƒì„± ì‹¤íŒ¨")
                else:
                    test_result["status"] = "failed"
                    test_result["errors"].append("OpenAI API ì—°ê²° ì‹¤íŒ¨")
                    logger.error("  âœ— OpenAI API ì—°ê²° ì‹¤íŒ¨")
            
        except Exception as e:
            test_result["status"] = "failed"
            test_result["errors"].append(str(e))
            logger.error(f"  âœ— OpenAI ì—°ë™ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        self.test_results["openai_integration"] = test_result
        logger.info("âœ“ OpenAI ì—°ë™ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    async def _test_dataset_generation(self):
        """ë°ì´í„°ì…‹ ìƒì„± í…ŒìŠ¤íŠ¸"""
        logger.info("5. ë°ì´í„°ì…‹ ìƒì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        test_result = {
            "status": "success",
            "formats_generated": {},
            "total_conversations": 0,
            "generation_time": 0,
            "errors": []
        }
        
        try:
            from unsloth_dataset import create_unsloth_generator, UnslothDatasetConfig
            
            # í…ŒìŠ¤íŠ¸ìš© ëŒ€í™” ë°ì´í„°
            test_conversations = [
                {
                    "id": "test_conv_001",
                    "conversations": [
                        {"from": "human", "value": "How do I create a GridControl in Syncfusion WinForms?"},
                        {"from": "gpt", "value": "To create a GridControl in Syncfusion WinForms, you need to add the Syncfusion.Windows.Forms.Grid namespace and create an instance of GridControl."}
                    ],
                    "metadata": {
                        "source_documents": ["sf_grid_001"],
                        "component": "GridControl",
                        "difficulty": "Beginner"
                    }
                }
            ]
            
            config = UnslothDatasetConfig(
                formats=["sharegpt", "alpaca", "openai"],
                output_dir="output/test_datasets",
                train_split=0.8,
                enable_validation=True
            )
            
            generator = create_unsloth_generator(config)
            
            start_time = time.time()
            result = await generator.generate_datasets_async(test_conversations)
            end_time = time.time()
            
            test_result["generation_time"] = end_time - start_time
            test_result["total_conversations"] = len(test_conversations)
            
            for format_name, dataset in result.datasets.items():
                test_result["formats_generated"][format_name] = len(dataset)
                logger.info(f"  âœ“ {format_name} í¬ë§·: {len(dataset)}ê°œ ë³€í™˜")
            
            logger.info(f"  âœ“ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ (ì‹œê°„: {test_result['generation_time']:.2f}ì´ˆ)")
            
        except Exception as e:
            test_result["status"] = "failed"
            test_result["errors"].append(str(e))
            logger.error(f"  âœ— ë°ì´í„°ì…‹ ìƒì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        self.test_results["dataset_generation"] = test_result
        logger.info("âœ“ ë°ì´í„°ì…‹ ìƒì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ")    asy
nc def _test_quality_validation(self):
        """í’ˆì§ˆ ê²€ì¦ í…ŒìŠ¤íŠ¸"""
        logger.info("6. í’ˆì§ˆ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        test_result = {
            "status": "success",
            "validation_checks": {},
            "quality_scores": {},
            "filtered_items": 0,
            "errors": []
        }
        
        try:
            from quality_validator import create_quality_validator, QualityValidatorConfig
            
            # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹
            test_datasets = {
                "sharegpt": [
                    {
                        "conversations": [
                            {"from": "human", "value": "How do I use GridControl?"},
                            {"from": "gpt", "value": "GridControl is a powerful data grid component in Syncfusion WinForms that allows you to display and edit data in a tabular format."}
                        ]
                    },
                    {
                        "conversations": [
                            {"from": "human", "value": "Test"},  # ë‚®ì€ í’ˆì§ˆ ì˜ˆì œ
                            {"from": "gpt", "value": "Test response"}
                        ]
                    }
                ]
            }
            
            config = QualityValidatorConfig(
                min_quality_score=0.6,
                enable_safety_filter=True,
                enable_duplicate_removal=True,
                enable_auto_correction=True
            )
            
            validator = create_quality_validator(config)
            
            # í’ˆì§ˆ ê²€ì¦ ì‹¤í–‰
            validation_results = await validator.validate_and_filter_async(test_datasets)
            
            for format_name, results in validation_results.items():
                valid_count = sum(1 for r in results if r.is_valid)
                total_count = len(results)
                
                test_result["validation_checks"][format_name] = {
                    "total": total_count,
                    "valid": valid_count,
                    "filtered": total_count - valid_count
                }
                
                if results:
                    avg_quality = sum(r.quality_metrics.get("overall_score", 0) for r in results) / len(results)
                    test_result["quality_scores"][format_name] = avg_quality
                
                logger.info(f"  âœ“ {format_name}: {valid_count}/{total_count} ìœ íš¨")
            
            test_result["filtered_items"] = sum(
                check["filtered"] for check in test_result["validation_checks"].values()
            )
            
            logger.info(f"  âœ“ í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ (í•„í„°ë§ëœ í•­ëª©: {test_result['filtered_items']}ê°œ)")
            
        except Exception as e:
            test_result["status"] = "failed"
            test_result["errors"].append(str(e))
            logger.error(f"  âœ— í’ˆì§ˆ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        self.test_results["quality_validation"] = test_result
        logger.info("âœ“ í’ˆì§ˆ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    async def _test_complete_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
        logger.info("7. ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        test_result = {
            "status": "success",
            "pipeline_steps": {},
            "total_processing_time": 0,
            "final_output": {},
            "errors": []
        }
        
        try:
            from async_pipeline import create_pipeline_orchestrator, PipelineConfig
            
            # í…ŒìŠ¤íŠ¸ìš© íŒŒì´í”„ë¼ì¸ ì„¤ì •
            config = PipelineConfig(
                # MD ì²˜ë¦¬ ì„¤ì •
                md_base_paths=["md_staging"],
                md_max_files=5,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì œí•œ
                
                # Qdrant ì„¤ì •
                qdrant_host="100.88.88.88",
                qdrant_port=6333,
                qdrant_collection="ws-7491d651ae044c78",
                
                # OpenAI ì„¤ì •
                openai_endpoint="http://123.37.28.120:9997/v1",
                openai_model="qwen2.5-vl-instruct",
                
                # ë°ì´í„°ì…‹ ì„¤ì •
                target_count=3,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì œí•œ
                formats=["sharegpt", "alpaca"],
                
                # ì„±ëŠ¥ ì„¤ì •
                batch_size=2,
                max_concurrent=2,
                test_mode=True
            )
            
            orchestrator = create_pipeline_orchestrator(config)
            
            start_time = time.time()
            
            # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            pipeline_result = await orchestrator.run_complete_pipeline()
            
            end_time = time.time()
            
            test_result["total_processing_time"] = end_time - start_time
            
            # ê° ë‹¨ê³„ ê²°ê³¼ í™•ì¸
            if pipeline_result.get("status") == "success":
                test_result["pipeline_steps"] = pipeline_result.get("step_results", {})
                test_result["final_output"] = pipeline_result.get("final_output", {})
                
                logger.info(f"  âœ“ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì„±ê³µ (ì‹œê°„: {test_result['total_processing_time']:.2f}ì´ˆ)")
                
                # ìµœì¢… ì¶œë ¥ ê²€ì¦
                final_datasets = test_result["final_output"].get("datasets", {})
                for format_name, dataset in final_datasets.items():
                    logger.info(f"    - {format_name}: {len(dataset)}ê°œ ëŒ€í™”")
            else:
                test_result["status"] = "failed"
                test_result["errors"].append("íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")
                logger.error("  âœ— íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨")
            
        except Exception as e:
            test_result["status"] = "failed"
            test_result["errors"].append(str(e))
            logger.error(f"  âœ— ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        self.test_results["complete_pipeline"] = test_result
        logger.info("âœ“ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")    asyn
c def _test_performance(self):
        """ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸"""
        logger.info("8. ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        test_result = {
            "status": "success",
            "throughput_metrics": {},
            "memory_usage": {},
            "scalability_test": {},
            "errors": []
        }
        
        try:
            import psutil
            import gc
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì‹œì‘
            process = psutil.Process()
            initial_memory = process.memory_info().rss / 1024 / 1024  # MB
            
            # ì²˜ë¦¬ëŸ‰ í…ŒìŠ¤íŠ¸ (ì‘ì€ ë°°ì¹˜ë¡œ í…ŒìŠ¤íŠ¸)
            batch_sizes = [1, 2, 4]
            throughput_results = {}
            
            for batch_size in batch_sizes:
                logger.info(f"  ë°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸ ì¤‘...")
                
                # í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œ ìƒì„±
                test_docs = [
                    {
                        "id": f"perf_test_{i}",
                        "content": f"Test document {i} for performance testing",
                        "component": "TestComponent"
                    }
                    for i in range(batch_size)
                ]
                
                start_time = time.time()
                
                # ê°„ë‹¨í•œ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
                processed_docs = []
                for doc in test_docs:
                    # ì‹¤ì œ ì²˜ë¦¬ ëŒ€ì‹  ì‹œë®¬ë ˆì´ì…˜
                    await asyncio.sleep(0.1)  # ì²˜ë¦¬ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
                    processed_docs.append({
                        **doc,
                        "processed": True,
                        "processing_time": 0.1
                    })
                
                end_time = time.time()
                processing_time = end_time - start_time
                
                throughput = len(test_docs) / processing_time
                throughput_results[batch_size] = {
                    "documents": len(test_docs),
                    "processing_time": processing_time,
                    "throughput": throughput
                }
                
                logger.info(f"    ë°°ì¹˜ í¬ê¸° {batch_size}: {throughput:.2f} docs/sec")
            
            test_result["throughput_metrics"] = throughput_results
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
            current_memory = process.memory_info().rss / 1024 / 1024  # MB
            memory_increase = current_memory - initial_memory
            
            test_result["memory_usage"] = {
                "initial_mb": initial_memory,
                "current_mb": current_memory,
                "increase_mb": memory_increase,
                "cpu_percent": process.cpu_percent()
            }
            
            logger.info(f"  âœ“ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {current_memory:.1f}MB (ì¦ê°€: {memory_increase:.1f}MB)")
            logger.info(f"  âœ“ CPU ì‚¬ìš©ë¥ : {process.cpu_percent():.1f}%")
            
            # í™•ì¥ì„± í…ŒìŠ¤íŠ¸ (ì‹œë®¬ë ˆì´ì…˜)
            max_concurrent = 4
            concurrent_results = {}
            
            for concurrent in [1, 2, max_concurrent]:
                logger.info(f"  ë™ì‹œ ì²˜ë¦¬ {concurrent}ê°œ í…ŒìŠ¤íŠ¸ ì¤‘...")
                
                start_time = time.time()
                
                # ë™ì‹œ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
                tasks = []
                for i in range(concurrent):
                    task = asyncio.create_task(asyncio.sleep(0.2))  # ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
                    tasks.append(task)
                
                await asyncio.gather(*tasks)
                
                end_time = time.time()
                processing_time = end_time - start_time
                
                concurrent_results[concurrent] = {
                    "concurrent_tasks": concurrent,
                    "total_time": processing_time,
                    "efficiency": concurrent / processing_time
                }
                
                logger.info(f"    ë™ì‹œ ì²˜ë¦¬ {concurrent}ê°œ: {processing_time:.2f}ì´ˆ")
            
            test_result["scalability_test"] = concurrent_results
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            gc.collect()
            
            logger.info("âœ“ ì„±ëŠ¥ ê²€ì¦ ì™„ë£Œ")
            
        except Exception as e:
            test_result["status"] = "failed"
            test_result["errors"].append(str(e))
            logger.error(f"  âœ— ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        self.test_results["performance"] = test_result
        logger.info("âœ“ ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    async def _test_unsloth_compatibility(self):
        """Unsloth í˜¸í™˜ì„± ê²€ì¦"""
        logger.info("9. Unsloth í˜¸í™˜ì„± ê²€ì¦ ì¤‘...")
        
        test_result = {
            "status": "success",
            "format_compatibility": {},
            "structure_validation": {},
            "field_validation": {},
            "errors": []
        }
        
        try:
            # Unsloth í˜¸í™˜ì„± ê²€ì¦ì„ ìœ„í•œ í…ŒìŠ¤íŠ¸ ë°ì´í„°
            test_datasets = {
                "sharegpt": [
                    {
                        "conversations": [
                            {"from": "human", "value": "How do I use Syncfusion GridControl?"},
                            {"from": "gpt", "value": "To use Syncfusion GridControl, you need to first add it to your form and then configure its properties."}
                        ]
                    }
                ],
                "alpaca": [
                    {
                        "instruction": "Explain how to use Syncfusion GridControl",
                        "input": "",
                        "output": "To use Syncfusion GridControl, you need to first add it to your form and then configure its properties."
                    }
                ],
                "openai": [
                    {
                        "messages": [
                            {"role": "user", "content": "How do I use Syncfusion GridControl?"},
                            {"role": "assistant", "content": "To use Syncfusion GridControl, you need to first add it to your form and then configure its properties."}
                        ]
                    }
                ]
            }
            
            # ê° í¬ë§·ë³„ í˜¸í™˜ì„± ê²€ì¦
            for format_name, dataset in test_datasets.items():
                logger.info(f"  {format_name} í¬ë§· ê²€ì¦ ì¤‘...")
                
                format_result = {
                    "valid_structure": True,
                    "required_fields": True,
                    "data_types": True,
                    "issues": []
                }
                
                for item in dataset:
                    # êµ¬ì¡° ê²€ì¦
                    if format_name == "sharegpt":
                        if "conversations" not in item:
                            format_result["valid_structure"] = False
                            format_result["issues"].append("conversations í•„ë“œ ëˆ„ë½")
                        else:
                            for conv in item["conversations"]:
                                if "from" not in conv or "value" not in conv:
                                    format_result["required_fields"] = False
                                    format_result["issues"].append("from ë˜ëŠ” value í•„ë“œ ëˆ„ë½")
                    
                    elif format_name == "alpaca":
                        required_fields = ["instruction", "input", "output"]
                        for field in required_fields:
                            if field not in item:
                                format_result["required_fields"] = False
                                format_result["issues"].append(f"{field} í•„ë“œ ëˆ„ë½")
                    
                    elif format_name == "openai":
                        if "messages" not in item:
                            format_result["valid_structure"] = False
                            format_result["issues"].append("messages í•„ë“œ ëˆ„ë½")
                        else:
                            for msg in item["messages"]:
                                if "role" not in msg or "content" not in msg:
                                    format_result["required_fields"] = False
                                    format_result["issues"].append("role ë˜ëŠ” content í•„ë“œ ëˆ„ë½")
                
                test_result["format_compatibility"][format_name] = format_result
                
                if not format_result["issues"]:
                    logger.info(f"    âœ“ {format_name} í¬ë§· í˜¸í™˜ì„± ê²€ì¦ í†µê³¼")
                else:
                    logger.warning(f"    âš  {format_name} í¬ë§· ì´ìŠˆ: {format_result['issues']}")
            
            # ì „ì²´ êµ¬ì¡° ê²€ì¦
            all_valid = all(
                result["valid_structure"] and result["required_fields"] and result["data_types"]
                for result in test_result["format_compatibility"].values()
            )
            
            if all_valid:
                logger.info("  âœ“ ëª¨ë“  í¬ë§·ì´ Unsloth í˜¸í™˜ì„± ìš”êµ¬ì‚¬í•­ì„ ë§Œì¡±")
            else:
                test_result["status"] = "warning"
                logger.warning("  âš  ì¼ë¶€ í¬ë§·ì—ì„œ í˜¸í™˜ì„± ì´ìŠˆ ë°œê²¬")
            
        except Exception as e:
            test_result["status"] = "failed"
            test_result["errors"].append(str(e))
            logger.error(f"  âœ— Unsloth í˜¸í™˜ì„± ê²€ì¦ ì‹¤íŒ¨: {e}")
        
        self.test_results["unsloth_compatibility"] = test_result
        logger.info("âœ“ Unsloth í˜¸í™˜ì„± ê²€ì¦ ì™„ë£Œ")    as
ync def _generate_final_report(self) -> Dict[str, Any]:
        """ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±"""
        logger.info("10. ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        total_time = (self.end_time - self.start_time).total_seconds()
        
        # ì „ì²´ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
        test_summary = {
            "total_tests": len(self.test_results),
            "passed": 0,
            "failed": 0,
            "warnings": 0
        }
        
        for test_name, result in self.test_results.items():
            status = result.get("status", "unknown")
            if status == "success":
                test_summary["passed"] += 1
            elif status == "failed":
                test_summary["failed"] += 1
            elif status == "warning":
                test_summary["warnings"] += 1
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìš”ì•½
        performance_summary = {}
        if "performance" in self.test_results:
            perf_data = self.test_results["performance"]
            if "throughput_metrics" in perf_data:
                max_throughput = max(
                    metrics["throughput"] 
                    for metrics in perf_data["throughput_metrics"].values()
                )
                performance_summary["max_throughput_docs_per_sec"] = max_throughput
            
            if "memory_usage" in perf_data:
                performance_summary["memory_usage_mb"] = perf_data["memory_usage"]["current_mb"]
        
        # ìµœì¢… ë¦¬í¬íŠ¸ êµ¬ì„±
        final_report = {
            "test_info": {
                "timestamp": self.start_time.isoformat(),
                "total_duration_seconds": total_time,
                "test_type": "complete_pipeline_integration"
            },
            "summary": test_summary,
            "performance_summary": performance_summary,
            "detailed_results": self.test_results,
            "recommendations": self._generate_recommendations()
        }
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"complete_pipeline_integration_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(final_report, f, indent=2, ensure_ascii=False)
        
        # ê²°ê³¼ ì¶œë ¥
        logger.info("=" * 60)
        logger.info("ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        logger.info("=" * 60)
        logger.info(f"ì´ í…ŒìŠ¤íŠ¸ ì‹œê°„: {total_time:.2f}ì´ˆ")
        logger.info(f"ì „ì²´ í…ŒìŠ¤íŠ¸: {test_summary['total_tests']}ê°œ")
        logger.info(f"ì„±ê³µ: {test_summary['passed']}ê°œ")
        logger.info(f"ì‹¤íŒ¨: {test_summary['failed']}ê°œ")
        logger.info(f"ê²½ê³ : {test_summary['warnings']}ê°œ")
        
        if performance_summary:
            logger.info(f"ìµœëŒ€ ì²˜ë¦¬ëŸ‰: {performance_summary.get('max_throughput_docs_per_sec', 'N/A'):.2f} docs/sec")
            logger.info(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {performance_summary.get('memory_usage_mb', 'N/A'):.1f}MB")
        
        logger.info(f"ìƒì„¸ ë¦¬í¬íŠ¸: {report_file}")
        logger.info("=" * 60)
        
        return final_report
    
    def _generate_recommendations(self) -> List[str]:
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ì— ëŒ€í•œ ê¶Œì¥ì‚¬í•­
        for test_name, result in self.test_results.items():
            if result.get("status") == "failed":
                if test_name == "environment_setup":
                    recommendations.append("í•„ìˆ˜ ëª¨ë“ˆ ì„¤ì¹˜ ë° ì„¤ì • íŒŒì¼ í™•ì¸ í•„ìš”")
                elif test_name == "qdrant_integration":
                    recommendations.append("Qdrant ì„œë²„ ì—°ê²° ìƒíƒœ ë° ë„¤íŠ¸ì›Œí¬ ì„¤ì • í™•ì¸ í•„ìš”")
                elif test_name == "openai_integration":
                    recommendations.append("OpenAI API ì—”ë“œí¬ì¸íŠ¸ ë° ì¸ì¦ ì„¤ì • í™•ì¸ í•„ìš”")
                elif test_name == "complete_pipeline":
                    recommendations.append("ì „ì²´ íŒŒì´í”„ë¼ì¸ ì„¤ì • ë° ëª¨ë“ˆ ê°„ ì—°ë™ í™•ì¸ í•„ìš”")
        
        # ì„±ëŠ¥ ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        if "performance" in self.test_results:
            perf_data = self.test_results["performance"]
            if "memory_usage" in perf_data:
                memory_mb = perf_data["memory_usage"].get("current_mb", 0)
                if memory_mb > 1000:  # 1GB ì´ìƒ
                    recommendations.append("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ìŒ - ë°°ì¹˜ í¬ê¸° ì¡°ì • ê³ ë ¤")
        
        # Unsloth í˜¸í™˜ì„± ê´€ë ¨ ê¶Œì¥ì‚¬í•­
        if "unsloth_compatibility" in self.test_results:
            compat_data = self.test_results["unsloth_compatibility"]
            if compat_data.get("status") == "warning":
                recommendations.append("Unsloth í˜¸í™˜ì„± ì´ìŠˆ í•´ê²° í•„ìš” - ë°ì´í„° í¬ë§· ê²€í† ")
        
        # ê¸°ë³¸ ê¶Œì¥ì‚¬í•­
        if not recommendations:
            recommendations.append("ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë¨ - í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„ ì™„ë£Œ")
        
        return recommendations


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ì „ì²´ íŒŒì´í”„ë¼ì¸ ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ì´ í…ŒìŠ¤íŠ¸ëŠ” ëª¨ë“  ëª¨ë“ˆì˜ í†µí•© ë™ì‘ì„ ê²€ì¦í•©ë‹ˆë‹¤.\n")
    
    tester = CompletePipelineIntegrationTest()
    
    try:
        final_report = await tester.run_complete_test()
        
        if final_report.get("summary", {}).get("failed", 0) == 0:
            print("\nğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print("ì‹œìŠ¤í…œì´ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print("\nâš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ì—ì„œ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("ìƒì„¸ ë¦¬í¬íŠ¸ë¥¼ í™•ì¸í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ì„¸ìš”.")
        
        # ê¶Œì¥ì‚¬í•­ ì¶œë ¥
        recommendations = final_report.get("recommendations", [])
        if recommendations:
            print("\nğŸ“‹ ê¶Œì¥ì‚¬í•­:")
            for i, rec in enumerate(recommendations, 1):
                print(f"  {i}. {rec}")
        
    except Exception as e:
        logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    
    print("\n=== ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")


if __name__ == "__main__":
    asyncio.run(main())