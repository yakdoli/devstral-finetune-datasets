#!/usr/bin/env python3
"""
ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸
ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì„±ëŠ¥ ë° í™•ì¥ì„± í…ŒìŠ¤íŠ¸
"""

import asyncio
import json
import logging
import time
import psutil
import gc
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
from concurrent.futures import ThreadPoolExecutor

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class PerformanceValidationTest:
    """ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.performance_metrics = {}
        self.baseline_metrics = {}
        self.test_results = {}
    
    async def run_performance_tests(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("=== ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        
        try:
            # 1. ê¸°ì¤€ ì„±ëŠ¥ ì¸¡ì •
            await self._measure_baseline_performance()
            
            # 2. ì²˜ë¦¬ëŸ‰ í…ŒìŠ¤íŠ¸
            await self._test_throughput_performance()
            
            # 3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸
            await self._test_memory_performance()
            
            # 4. ë™ì‹œì„± í…ŒìŠ¤íŠ¸
            await self._test_concurrency_performance()
            
            # 5. í™•ì¥ì„± í…ŒìŠ¤íŠ¸
            await self._test_scalability()
            
            # 6. ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
            await self._test_stress_performance()
            
            # 7. ìµœì¢… ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
            return await self._generate_performance_report()
            
        except Exception as e:
            logger.error(f"ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {"status": "failed", "error": str(e)}
    
    async def _measure_baseline_performance(self):
        """ê¸°ì¤€ ì„±ëŠ¥ ì¸¡ì •"""
        logger.info("1. ê¸°ì¤€ ì„±ëŠ¥ ì¸¡ì • ì¤‘...")
        
        process = psutil.Process()
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ê¸°ì¤€ ì¸¡ì •
        baseline = {
            "cpu_count": psutil.cpu_count(),
            "memory_total_gb": psutil.virtual_memory().total / (1024**3),
            "memory_available_gb": psutil.virtual_memory().available / (1024**3),
            "initial_memory_mb": process.memory_info().rss / (1024**2),
            "initial_cpu_percent": process.cpu_percent(interval=1)
        }
        
        self.baseline_metrics = baseline
        
        logger.info(f"  CPU ì½”ì–´: {baseline['cpu_count']}ê°œ")
        logger.info(f"  ì´ ë©”ëª¨ë¦¬: {baseline['memory_total_gb']:.1f}GB")
        logger.info(f"  ì‚¬ìš© ê°€ëŠ¥ ë©”ëª¨ë¦¬: {baseline['memory_available_gb']:.1f}GB")
        logger.info(f"  ì´ˆê¸° í”„ë¡œì„¸ìŠ¤ ë©”ëª¨ë¦¬: {baseline['initial_memory_mb']:.1f}MB")
        logger.info("âœ“ ê¸°ì¤€ ì„±ëŠ¥ ì¸¡ì • ì™„ë£Œ")
    
    async def _test_throughput_performance(self):
        """ì²˜ë¦¬ëŸ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("2. ì²˜ë¦¬ëŸ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        test_result = {
            "batch_performance": {},
            "document_processing": {},
            "conversation_generation": {}
        }
        
        # ë°°ì¹˜ í¬ê¸°ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        batch_sizes = [1, 5, 10, 20, 50]
        
        for batch_size in batch_sizes:
            logger.info(f"  ë°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
            test_documents = [
                {
                    "id": f"perf_doc_{i}",
                    "content": f"Performance test document {i} " * 50,  # ê¸´ ì½˜í…ì¸ 
                    "component": f"TestComponent{i % 5}",
                    "metadata": {"test": True, "batch_size": batch_size}
                }
                for i in range(batch_size)
            ]
            
            # ì²˜ë¦¬ ì‹œê°„ ì¸¡ì •
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss / (1024**2)
            
            # ë¬¸ì„œ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
            processed_docs = []
            for doc in test_documents:
                # ì‹¤ì œ ì²˜ë¦¬ ë¡œì§ ì‹œë®¬ë ˆì´ì…˜
                await asyncio.sleep(0.01)  # ì²˜ë¦¬ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜
                processed_doc = {
                    **doc,
                    "processed": True,
                    "processing_timestamp": datetime.now().isoformat(),
                    "word_count": len(doc["content"].split())
                }
                processed_docs.append(processed_doc)
            
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss / (1024**2)
            
            processing_time = end_time - start_time
            memory_increase = end_memory - start_memory
            throughput = len(test_documents) / processing_time
            
            test_result["batch_performance"][batch_size] = {
                "documents": len(test_documents),
                "processing_time": processing_time,
                "throughput_docs_per_sec": throughput,
                "memory_increase_mb": memory_increase,
                "avg_time_per_doc": processing_time / len(test_documents)
            }
            
            logger.info(f"    ì²˜ë¦¬ëŸ‰: {throughput:.2f} docs/sec")
            logger.info(f"    ë©”ëª¨ë¦¬ ì¦ê°€: {memory_increase:.1f}MB")
        
        self.test_results["throughput"] = test_result
        logger.info("âœ“ ì²˜ë¦¬ëŸ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")  
  async def _test_memory_performance(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("3. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        test_result = {
            "memory_scaling": {},
            "memory_leaks": {},
            "garbage_collection": {}
        }
        
        process = psutil.Process()
        initial_memory = process.memory_info().rss / (1024**2)
        
        # ë°ì´í„° í¬ê¸°ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í…ŒìŠ¤íŠ¸
        data_sizes = [100, 500, 1000, 2000]
        
        for data_size in data_sizes:
            logger.info(f"  ë°ì´í„° í¬ê¸° {data_size} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            gc.collect()  # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰
            before_memory = process.memory_info().rss / (1024**2)
            
            # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒì„±
            large_dataset = []
            for i in range(data_size):
                large_item = {
                    "id": f"large_item_{i}",
                    "content": "Large content data " * 100,  # í° ì½˜í…ì¸ 
                    "metadata": {
                        "index": i,
                        "timestamp": datetime.now().isoformat(),
                        "large_field": "x" * 1000  # 1KB ë¬¸ìì—´
                    },
                    "conversations": [
                        {"from": "human", "value": f"Question {i}"},
                        {"from": "gpt", "value": f"Answer {i} with detailed explanation " * 20}
                    ]
                }
                large_dataset.append(large_item)
            
            after_creation = process.memory_info().rss / (1024**2)
            
            # ë°ì´í„° ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
            processed_items = 0
            for item in large_dataset:
                # ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
                item["processed"] = True
                processed_items += 1
                
                # ì¤‘ê°„ ë©”ëª¨ë¦¬ ì²´í¬
                if processed_items % 500 == 0:
                    current_memory = process.memory_info().rss / (1024**2)
                    logger.debug(f"    ì²˜ë¦¬ ì¤‘ ë©”ëª¨ë¦¬: {current_memory:.1f}MB")
            
            after_processing = process.memory_info().rss / (1024**2)
            
            # ë°ì´í„° ì‚­ì œ í›„ ë©”ëª¨ë¦¬ í™•ì¸
            del large_dataset
            gc.collect()
            after_cleanup = process.memory_info().rss / (1024**2)
            
            test_result["memory_scaling"][data_size] = {
                "initial_memory_mb": before_memory,
                "after_creation_mb": after_creation,
                "after_processing_mb": after_processing,
                "after_cleanup_mb": after_cleanup,
                "creation_increase_mb": after_creation - before_memory,
                "processing_increase_mb": after_processing - after_creation,
                "cleanup_decrease_mb": after_processing - after_cleanup,
                "memory_per_item_kb": (after_creation - before_memory) * 1024 / data_size
            }
            
            logger.info(f"    ë©”ëª¨ë¦¬ ì¦ê°€: {after_creation - before_memory:.1f}MB")
            logger.info(f"    í•­ëª©ë‹¹ ë©”ëª¨ë¦¬: {(after_creation - before_memory) * 1024 / data_size:.2f}KB")
        
        # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í…ŒìŠ¤íŠ¸
        logger.info("  ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        leak_test_iterations = 10
        memory_samples = []
        
        for iteration in range(leak_test_iterations):
            # ë°˜ë³µì ì¸ ë°ì´í„° ìƒì„± ë° ì‚­ì œ
            temp_data = [{"data": "x" * 1000} for _ in range(100)]
            
            # ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
            for item in temp_data:
                item["processed"] = True
            
            # ë©”ëª¨ë¦¬ ì¸¡ì •
            current_memory = process.memory_info().rss / (1024**2)
            memory_samples.append(current_memory)
            
            # ë°ì´í„° ì‚­ì œ
            del temp_data
            gc.collect()
            
            await asyncio.sleep(0.1)  # ì§§ì€ ëŒ€ê¸°
        
        # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë¶„ì„
        memory_trend = []
        for i in range(1, len(memory_samples)):
            trend = memory_samples[i] - memory_samples[i-1]
            memory_trend.append(trend)
        
        avg_memory_increase = sum(memory_trend) / len(memory_trend)
        
        test_result["memory_leaks"] = {
            "iterations": leak_test_iterations,
            "memory_samples": memory_samples,
            "avg_increase_per_iteration": avg_memory_increase,
            "total_increase": memory_samples[-1] - memory_samples[0],
            "potential_leak": avg_memory_increase > 0.5  # 0.5MB ì´ìƒ ì¦ê°€ì‹œ ëˆ„ìˆ˜ ì˜ì‹¬
        }
        
        if test_result["memory_leaks"]["potential_leak"]:
            logger.warning(f"  âš  ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì˜ì‹¬: í‰ê·  {avg_memory_increase:.2f}MB/iteration ì¦ê°€")
        else:
            logger.info("  âœ“ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì—†ìŒ")
        
        self.test_results["memory"] = test_result
        logger.info("âœ“ ë©”ëª¨ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    async def _test_concurrency_performance(self):
        """ë™ì‹œì„± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("4. ë™ì‹œì„± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        test_result = {
            "concurrent_processing": {},
            "async_performance": {},
            "thread_performance": {}
        }
        
        # ë™ì‹œ ì²˜ë¦¬ ìˆ˜ì¤€ë³„ í…ŒìŠ¤íŠ¸
        concurrency_levels = [1, 2, 4, 8, 16]
        
        for concurrency in concurrency_levels:
            logger.info(f"  ë™ì‹œì„± ë ˆë²¨ {concurrency} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            # ë¹„ë™ê¸° ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            async def async_task(task_id: int):
                start_time = time.time()
                
                # ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
                await asyncio.sleep(0.1)  # I/O ëŒ€ê¸° ì‹œë®¬ë ˆì´ì…˜
                
                # CPU ì§‘ì•½ì  ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
                result = sum(i * i for i in range(1000))
                
                end_time = time.time()
                return {
                    "task_id": task_id,
                    "processing_time": end_time - start_time,
                    "result": result
                }
            
            # ë¹„ë™ê¸° ì‘ì—… ì‹¤í–‰
            start_time = time.time()
            
            tasks = [async_task(i) for i in range(concurrency)]
            async_results = await asyncio.gather(*tasks)
            
            async_end_time = time.time()
            async_total_time = async_end_time - start_time
            
            # ìŠ¤ë ˆë“œ í’€ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            def sync_task(task_id: int):
                start_time = time.time()
                
                # CPU ì§‘ì•½ì  ì‘ì—…
                result = sum(i * i for i in range(1000))
                time.sleep(0.1)  # ë¸”ë¡œí‚¹ ì‘ì—… ì‹œë®¬ë ˆì´ì…˜
                
                end_time = time.time()
                return {
                    "task_id": task_id,
                    "processing_time": end_time - start_time,
                    "result": result
                }
            
            # ìŠ¤ë ˆë“œ í’€ ì‹¤í–‰
            thread_start_time = time.time()
            
            with ThreadPoolExecutor(max_workers=concurrency) as executor:
                thread_futures = [executor.submit(sync_task, i) for i in range(concurrency)]
                thread_results = [future.result() for future in thread_futures]
            
            thread_end_time = time.time()
            thread_total_time = thread_end_time - thread_start_time
            
            # ê²°ê³¼ ë¶„ì„
            async_avg_time = sum(r["processing_time"] for r in async_results) / len(async_results)
            thread_avg_time = sum(r["processing_time"] for r in thread_results) / len(thread_results)
            
            test_result["concurrent_processing"][concurrency] = {
                "async_total_time": async_total_time,
                "async_avg_task_time": async_avg_time,
                "async_efficiency": concurrency / async_total_time,
                "thread_total_time": thread_total_time,
                "thread_avg_task_time": thread_avg_time,
                "thread_efficiency": concurrency / thread_total_time,
                "async_vs_thread_ratio": async_total_time / thread_total_time
            }
            
            logger.info(f"    ë¹„ë™ê¸°: {async_total_time:.2f}ì´ˆ")
            logger.info(f"    ìŠ¤ë ˆë“œ: {thread_total_time:.2f}ì´ˆ")
            logger.info(f"    íš¨ìœ¨ì„± ë¹„ìœ¨: {async_total_time / thread_total_time:.2f}")
        
        self.test_results["concurrency"] = test_result
        logger.info("âœ“ ë™ì‹œì„± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")  
  async def _test_scalability(self):
        """í™•ì¥ì„± í…ŒìŠ¤íŠ¸"""
        logger.info("5. í™•ì¥ì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        test_result = {
            "data_scaling": {},
            "user_scaling": {},
            "resource_scaling": {}
        }
        
        # ë°ì´í„° í¬ê¸° í™•ì¥ì„± í…ŒìŠ¤íŠ¸
        data_scales = [100, 500, 1000, 2500, 5000]
        
        for scale in data_scales:
            logger.info(f"  ë°ì´í„° ê·œëª¨ {scale} í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            start_time = time.time()
            start_memory = psutil.Process().memory_info().rss / (1024**2)
            
            # ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
            processed_count = 0
            batch_size = min(100, scale // 10)  # ë™ì  ë°°ì¹˜ í¬ê¸°
            
            for batch_start in range(0, scale, batch_size):
                batch_end = min(batch_start + batch_size, scale)
                batch_data = []
                
                # ë°°ì¹˜ ë°ì´í„° ìƒì„±
                for i in range(batch_start, batch_end):
                    item = {
                        "id": f"scale_test_{i}",
                        "content": f"Scalability test content {i}",
                        "metadata": {"batch": batch_start // batch_size}
                    }
                    batch_data.append(item)
                
                # ë°°ì¹˜ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
                for item in batch_data:
                    item["processed"] = True
                    processed_count += 1
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                if batch_start % (batch_size * 10) == 0:
                    gc.collect()
                
                await asyncio.sleep(0.001)  # ì§§ì€ ëŒ€ê¸°
            
            end_time = time.time()
            end_memory = psutil.Process().memory_info().rss / (1024**2)
            
            processing_time = end_time - start_time
            memory_increase = end_memory - start_memory
            throughput = processed_count / processing_time
            
            test_result["data_scaling"][scale] = {
                "processed_items": processed_count,
                "processing_time": processing_time,
                "throughput": throughput,
                "memory_increase_mb": memory_increase,
                "time_per_item_ms": (processing_time * 1000) / processed_count,
                "memory_per_item_kb": (memory_increase * 1024) / processed_count if memory_increase > 0 else 0
            }
            
            logger.info(f"    ì²˜ë¦¬ëŸ‰: {throughput:.1f} items/sec")
            logger.info(f"    í•­ëª©ë‹¹ ì‹œê°„: {(processing_time * 1000) / processed_count:.2f}ms")
        
        # í™•ì¥ì„± ë¶„ì„
        scales = list(test_result["data_scaling"].keys())
        throughputs = [test_result["data_scaling"][s]["throughput"] for s in scales]
        
        # ì„ í˜• í™•ì¥ì„± ë¶„ì„ (ì´ìƒì ìœ¼ë¡œëŠ” ì¼ì •í•œ ì²˜ë¦¬ëŸ‰ ìœ ì§€)
        throughput_variance = max(throughputs) - min(throughputs)
        throughput_stability = 1 - (throughput_variance / max(throughputs))
        
        test_result["scalability_analysis"] = {
            "throughput_variance": throughput_variance,
            "throughput_stability": throughput_stability,
            "scales_tested": scales,
            "max_throughput": max(throughputs),
            "min_throughput": min(throughputs),
            "is_scalable": throughput_stability > 0.7  # 70% ì´ìƒ ì•ˆì •ì„±
        }
        
        if test_result["scalability_analysis"]["is_scalable"]:
            logger.info(f"  âœ“ í™•ì¥ì„± ì–‘í˜¸ (ì•ˆì •ì„±: {throughput_stability:.1%})")
        else:
            logger.warning(f"  âš  í™•ì¥ì„± ì´ìŠˆ (ì•ˆì •ì„±: {throughput_stability:.1%})")
        
        self.test_results["scalability"] = test_result
        logger.info("âœ“ í™•ì¥ì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    async def _test_stress_performance(self):
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸"""
        logger.info("6. ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        test_result = {
            "high_load": {},
            "resource_limits": {},
            "recovery_test": {}
        }
        
        process = psutil.Process()
        
        # ê³ ë¶€í•˜ í…ŒìŠ¤íŠ¸
        logger.info("  ê³ ë¶€í•˜ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        stress_duration = 30  # 30ì´ˆ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸
        stress_start_time = time.time()
        
        # ë™ì‹œ ì‘ì—… ìƒì„±
        stress_tasks = []
        task_count = 50  # 50ê°œ ë™ì‹œ ì‘ì—…
        
        async def stress_task(task_id: int):
            task_start = time.time()
            iterations = 0
            
            while time.time() - stress_start_time < stress_duration:
                # CPU ì§‘ì•½ì  ì‘ì—…
                result = sum(i * i for i in range(100))
                
                # ë©”ëª¨ë¦¬ ì‚¬ìš©
                temp_data = [f"stress_data_{i}" for i in range(100)]
                
                # I/O ì‹œë®¬ë ˆì´ì…˜
                await asyncio.sleep(0.01)
                
                iterations += 1
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del temp_data
                
                if iterations % 100 == 0:
                    gc.collect()
            
            task_end = time.time()
            return {
                "task_id": task_id,
                "iterations": iterations,
                "duration": task_end - task_start
            }
        
        # ìŠ¤íŠ¸ë ˆìŠ¤ ì‘ì—… ì‹¤í–‰
        stress_tasks = [stress_task(i) for i in range(task_count)]
        
        # ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§
        resource_samples = []
        monitoring_task = asyncio.create_task(self._monitor_resources_during_stress(stress_duration, resource_samples))
        
        # ìŠ¤íŠ¸ë ˆìŠ¤ ì‘ì—… ë° ëª¨ë‹ˆí„°ë§ ì‹¤í–‰
        stress_results, _ = await asyncio.gather(
            asyncio.gather(*stress_tasks),
            monitoring_task
        )
        
        stress_end_time = time.time()
        total_stress_time = stress_end_time - stress_start_time
        
        # ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
        total_iterations = sum(r["iterations"] for r in stress_results)
        avg_iterations = total_iterations / len(stress_results)
        
        test_result["high_load"] = {
            "duration_seconds": total_stress_time,
            "concurrent_tasks": task_count,
            "total_iterations": total_iterations,
            "avg_iterations_per_task": avg_iterations,
            "iterations_per_second": total_iterations / total_stress_time,
            "resource_samples": resource_samples
        }
        
        # ë¦¬ì†ŒìŠ¤ í•œê³„ ë¶„ì„
        if resource_samples:
            max_cpu = max(sample["cpu_percent"] for sample in resource_samples)
            max_memory = max(sample["memory_mb"] for sample in resource_samples)
            avg_cpu = sum(sample["cpu_percent"] for sample in resource_samples) / len(resource_samples)
            avg_memory = sum(sample["memory_mb"] for sample in resource_samples) / len(resource_samples)
            
            test_result["resource_limits"] = {
                "max_cpu_percent": max_cpu,
                "max_memory_mb": max_memory,
                "avg_cpu_percent": avg_cpu,
                "avg_memory_mb": avg_memory,
                "cpu_utilization": avg_cpu / 100,
                "memory_pressure": max_memory > (self.baseline_metrics["memory_available_gb"] * 1024 * 0.8)
            }
            
            logger.info(f"    ìµœëŒ€ CPU ì‚¬ìš©ë¥ : {max_cpu:.1f}%")
            logger.info(f"    ìµœëŒ€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {max_memory:.1f}MB")
            logger.info(f"    ì´ˆë‹¹ ë°˜ë³µ ìˆ˜: {total_iterations / total_stress_time:.1f}")
        
        # ë³µêµ¬ í…ŒìŠ¤íŠ¸
        logger.info("  ì‹œìŠ¤í…œ ë³µêµ¬ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        recovery_start = time.time()
        
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ë° ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        
        # ë³µêµ¬ í›„ ì„±ëŠ¥ ì¸¡ì •
        recovery_tasks = [self._simple_performance_task(i) for i in range(10)]
        recovery_results = await asyncio.gather(*recovery_tasks)
        
        recovery_end = time.time()
        recovery_time = recovery_end - recovery_start
        
        recovery_avg_time = sum(r["processing_time"] for r in recovery_results) / len(recovery_results)
        
        test_result["recovery_test"] = {
            "recovery_time_seconds": recovery_time,
            "post_stress_avg_task_time": recovery_avg_time,
            "performance_degradation": recovery_avg_time > 0.2,  # 0.2ì´ˆ ì´ìƒì´ë©´ ì„±ëŠ¥ ì €í•˜
            "system_recovered": recovery_avg_time < 0.5  # 0.5ì´ˆ ë¯¸ë§Œì´ë©´ ë³µêµ¬ë¨
        }
        
        if test_result["recovery_test"]["system_recovered"]:
            logger.info("  âœ“ ì‹œìŠ¤í…œ ì •ìƒ ë³µêµ¬")
        else:
            logger.warning("  âš  ì‹œìŠ¤í…œ ë³µêµ¬ ì§€ì—°")
        
        self.test_results["stress"] = test_result
        logger.info("âœ“ ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    
    async def _monitor_resources_during_stress(self, duration: float, samples: List[Dict]):
        """ìŠ¤íŠ¸ë ˆìŠ¤ í…ŒìŠ¤íŠ¸ ì¤‘ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§"""
        start_time = time.time()
        process = psutil.Process()
        
        while time.time() - start_time < duration:
            sample = {
                "timestamp": time.time() - start_time,
                "cpu_percent": process.cpu_percent(),
                "memory_mb": process.memory_info().rss / (1024**2),
                "system_cpu_percent": psutil.cpu_percent(),
                "system_memory_percent": psutil.virtual_memory().percent
            }
            samples.append(sample)
            
            await asyncio.sleep(0.5)  # 0.5ì´ˆë§ˆë‹¤ ìƒ˜í”Œë§
    
    async def _simple_performance_task(self, task_id: int):
        """ê°„ë‹¨í•œ ì„±ëŠ¥ ì¸¡ì • ì‘ì—…"""
        start_time = time.time()
        
        # ê°„ë‹¨í•œ ì‘ì—… ìˆ˜í–‰
        result = sum(i for i in range(1000))
        await asyncio.sleep(0.01)
        
        end_time = time.time()
        
        return {
            "task_id": task_id,
            "processing_time": end_time - start_time,
            "result": result
        }    a
sync def _generate_performance_report(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
        logger.info("7. ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
        performance_scores = {}
        
        # ì²˜ë¦¬ëŸ‰ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
        if "throughput" in self.test_results:
            throughput_data = self.test_results["throughput"]["batch_performance"]
            max_throughput = max(data["throughput_docs_per_sec"] for data in throughput_data.values())
            performance_scores["throughput"] = min(100, max_throughput * 10)  # 10 docs/sec = 100ì 
        
        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ì ìˆ˜ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        if "memory" in self.test_results:
            memory_data = self.test_results["memory"]["memory_scaling"]
            avg_memory_per_item = sum(data["memory_per_item_kb"] for data in memory_data.values()) / len(memory_data)
            performance_scores["memory_efficiency"] = max(0, 100 - avg_memory_per_item)  # 1KB/item = 99ì 
        
        # ë™ì‹œì„± íš¨ìœ¨ì„± ì ìˆ˜
        if "concurrency" in self.test_results:
            concurrency_data = self.test_results["concurrency"]["concurrent_processing"]
            max_efficiency = max(data["async_efficiency"] for data in concurrency_data.values())
            performance_scores["concurrency"] = min(100, max_efficiency * 10)  # 10 tasks/sec = 100ì 
        
        # í™•ì¥ì„± ì ìˆ˜
        if "scalability" in self.test_results:
            scalability_data = self.test_results["scalability"]["scalability_analysis"]
            stability = scalability_data["throughput_stability"]
            performance_scores["scalability"] = stability * 100
        
        # ìŠ¤íŠ¸ë ˆìŠ¤ ë‚´ì„± ì ìˆ˜
        if "stress" in self.test_results:
            stress_data = self.test_results["stress"]
            recovery_score = 100 if stress_data["recovery_test"]["system_recovered"] else 50
            resource_score = 100 - min(100, stress_data["resource_limits"]["max_cpu_percent"])
            performance_scores["stress_resistance"] = (recovery_score + resource_score) / 2
        
        # ì „ì²´ ì„±ëŠ¥ ì ìˆ˜
        overall_score = sum(performance_scores.values()) / len(performance_scores) if performance_scores else 0
        
        # ì„±ëŠ¥ ë“±ê¸‰ ê²°ì •
        if overall_score >= 90:
            performance_grade = "A"
        elif overall_score >= 80:
            performance_grade = "B"
        elif overall_score >= 70:
            performance_grade = "C"
        elif overall_score >= 60:
            performance_grade = "D"
        else:
            performance_grade = "F"
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = []
        
        if performance_scores.get("throughput", 0) < 70:
            recommendations.append("ì²˜ë¦¬ëŸ‰ ê°œì„ ì„ ìœ„í•´ ë°°ì¹˜ í¬ê¸° ìµœì í™” í•„ìš”")
        
        if performance_scores.get("memory_efficiency", 0) < 70:
            recommendations.append("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” í•„ìš” - ë°ì´í„° êµ¬ì¡° ê°œì„  ê³ ë ¤")
        
        if performance_scores.get("concurrency", 0) < 70:
            recommendations.append("ë™ì‹œì„± ì²˜ë¦¬ ê°œì„  í•„ìš” - ë¹„ë™ê¸° ì²˜ë¦¬ ìµœì í™”")
        
        if performance_scores.get("scalability", 0) < 70:
            recommendations.append("í™•ì¥ì„± ê°œì„  í•„ìš” - ì•„í‚¤í…ì²˜ ì¬ê²€í† ")
        
        if performance_scores.get("stress_resistance", 0) < 70:
            recommendations.append("ìŠ¤íŠ¸ë ˆìŠ¤ ë‚´ì„± ê°œì„  í•„ìš” - ë¦¬ì†ŒìŠ¤ ê´€ë¦¬ ê°•í™”")
        
        if not recommendations:
            recommendations.append("ëª¨ë“  ì„±ëŠ¥ ì§€í‘œê°€ ì–‘í˜¸í•¨ - í˜„ì¬ ì„¤ì • ìœ ì§€")
        
        # ìµœì¢… ë¦¬í¬íŠ¸ êµ¬ì„±
        performance_report = {
            "test_info": {
                "timestamp": datetime.now().isoformat(),
                "test_type": "performance_validation",
                "baseline_metrics": self.baseline_metrics
            },
            "performance_scores": performance_scores,
            "overall_score": overall_score,
            "performance_grade": performance_grade,
            "detailed_results": self.test_results,
            "recommendations": recommendations,
            "summary": {
                "max_throughput_docs_per_sec": max(
                    data["throughput_docs_per_sec"] 
                    for data in self.test_results.get("throughput", {}).get("batch_performance", {}).values()
                ) if "throughput" in self.test_results else 0,
                "memory_efficiency_kb_per_item": sum(
                    data["memory_per_item_kb"] 
                    for data in self.test_results.get("memory", {}).get("memory_scaling", {}).values()
                ) / len(self.test_results.get("memory", {}).get("memory_scaling", {})) if "memory" in self.test_results else 0,
                "max_concurrent_efficiency": max(
                    data["async_efficiency"] 
                    for data in self.test_results.get("concurrency", {}).get("concurrent_processing", {}).values()
                ) if "concurrency" in self.test_results else 0,
                "scalability_stability": self.test_results.get("scalability", {}).get("scalability_analysis", {}).get("throughput_stability", 0),
                "stress_recovery_success": self.test_results.get("stress", {}).get("recovery_test", {}).get("system_recovered", False)
            }
        }
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"performance_validation_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(performance_report, f, indent=2, ensure_ascii=False)
        
        # ê²°ê³¼ ì¶œë ¥
        logger.info("=" * 60)
        logger.info("ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        logger.info("=" * 60)
        logger.info(f"ì „ì²´ ì„±ëŠ¥ ì ìˆ˜: {overall_score:.1f}/100 (ë“±ê¸‰: {performance_grade})")
        logger.info(f"ì²˜ë¦¬ëŸ‰ ì ìˆ˜: {performance_scores.get('throughput', 0):.1f}/100")
        logger.info(f"ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: {performance_scores.get('memory_efficiency', 0):.1f}/100")
        logger.info(f"ë™ì‹œì„± íš¨ìœ¨ì„±: {performance_scores.get('concurrency', 0):.1f}/100")
        logger.info(f"í™•ì¥ì„±: {performance_scores.get('scalability', 0):.1f}/100")
        logger.info(f"ìŠ¤íŠ¸ë ˆìŠ¤ ë‚´ì„±: {performance_scores.get('stress_resistance', 0):.1f}/100")
        logger.info("")
        logger.info("ê¶Œì¥ì‚¬í•­:")
        for i, rec in enumerate(recommendations, 1):
            logger.info(f"  {i}. {rec}")
        logger.info(f"\nìƒì„¸ ë¦¬í¬íŠ¸: {report_file}")
        logger.info("=" * 60)
        
        return performance_report


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ì´ í…ŒìŠ¤íŠ¸ëŠ” ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ íŠ¹ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.\n")
    
    tester = PerformanceValidationTest()
    
    try:
        performance_report = await tester.run_performance_tests()
        
        overall_score = performance_report.get("overall_score", 0)
        grade = performance_report.get("performance_grade", "F")
        
        if grade in ["A", "B"]:
            print(f"\nğŸ‰ ì„±ëŠ¥ ê²€ì¦ ì™„ë£Œ! ì „ì²´ ì ìˆ˜: {overall_score:.1f}/100 (ë“±ê¸‰: {grade})")
            print("ì‹œìŠ¤í…œì´ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.")
        elif grade == "C":
            print(f"\nâœ… ì„±ëŠ¥ ê²€ì¦ ì™„ë£Œ! ì „ì²´ ì ìˆ˜: {overall_score:.1f}/100 (ë“±ê¸‰: {grade})")
            print("ì‹œìŠ¤í…œ ì„±ëŠ¥ì´ ì–‘í˜¸í•˜ì§€ë§Œ ê°œì„  ì—¬ì§€ê°€ ìˆìŠµë‹ˆë‹¤.")
        else:
            print(f"\nâš ï¸ ì„±ëŠ¥ ê²€ì¦ ì™„ë£Œ! ì „ì²´ ì ìˆ˜: {overall_score:.1f}/100 (ë“±ê¸‰: {grade})")
            print("ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥
        summary = performance_report.get("summary", {})
        print(f"\nğŸ“Š ì£¼ìš” ì„±ëŠ¥ ì§€í‘œ:")
        print(f"  ìµœëŒ€ ì²˜ë¦¬ëŸ‰: {summary.get('max_throughput_docs_per_sec', 0):.1f} docs/sec")
        print(f"  ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: {summary.get('memory_efficiency_kb_per_item', 0):.1f} KB/item")
        print(f"  ë™ì‹œì„± íš¨ìœ¨ì„±: {summary.get('max_concurrent_efficiency', 0):.1f}")
        print(f"  í™•ì¥ì„± ì•ˆì •ì„±: {summary.get('scalability_stability', 0):.1%}")
        print(f"  ìŠ¤íŠ¸ë ˆìŠ¤ ë³µêµ¬: {'ì„±ê³µ' if summary.get('stress_recovery_success', False) else 'ì‹¤íŒ¨'}")
        
    except Exception as e:
        logger.error(f"ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"\nâŒ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    print("\n=== ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")


if __name__ == "__main__":
    asyncio.run(main())