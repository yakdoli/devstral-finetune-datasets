# Unsloth í˜¸í™˜ ë°ì´í„°ì…‹ ìƒì„±ê¸°

Unsloth í”„ë ˆì„ì›Œí¬ì— ìµœì í™”ëœ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ í¬ë§·ì„ ìƒì„±í•˜ëŠ” Python ëª¨ë“ˆì…ë‹ˆë‹¤. ê¸°ì¡´ì— êµ¬ì¶•ëœ ëª¨ë“  ëª¨ë“ˆ(MD ì²˜ë¦¬, Qdrant ì—°ë™, OpenAI API ì—°ë™)ê³¼ì˜ ì™„ë²½í•œ í†µí•©ì„ ì§€ì›í•©ë‹ˆë‹¤.

## ì£¼ìš” íŠ¹ì§•

### ğŸš€ í•µì‹¬ ê¸°ëŠ¥
- **ë‹¤ì¤‘ í¬ë§· ì§€ì›**: ShareGPT, Alpaca, OpenAI í¬ë§· ìƒì„±
- **Unsloth ìµœì í™”**: 4-bit ì–‘ìí™” í›ˆë ¨ì„ ìœ„í•œ ìµœì í™”
- **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**: ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬ ì§€ì›
- **í’ˆì§ˆ ë³´ì¥**: response_template ë§ˆì»¤, EOS í† í° ì²˜ë¦¬
- **í†µê³„ ì •ë³´**: ë°ì´í„°ì…‹ ë©”íƒ€ë°ì´í„° ë° í’ˆì§ˆ ì§€í‘œ ìƒì„±

### ğŸ¯ Unsloth íŠ¹í™” ìš”êµ¬ì‚¬í•­ ì¶©ì¡±
- **ShareGPT í¬ë§·**: ë‹¤ì¤‘ í„´ ëŒ€í™” (system/human/gpt ì—­í• )
- **Alpaca í¬ë§·**: instruction/input/output êµ¬ì¡°
- **OpenAI í¬ë§·**: messages ë°°ì—´ êµ¬ì¡°
- **í† í° ìµœì í™”**: 50-4096 í† í° ë²”ìœ„
- **ì‹œí€€ìŠ¤ ê¸¸ì´**: ìµœëŒ€ 4096 í† í° (Qwen2.5-7B-Instruct ìµœì í™”)
- **EOS í† í°**: `</s>` ë˜ëŠ” `<|im_end|>` ì²˜ë¦¬

## ì„¤ì¹˜

```bash
# ê¸°ë³¸ ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt

# ì„ íƒì  ì˜ì¡´ì„± (ì™¸ë¶€ ëª¨ë“ˆê³¼ì˜ í†µí•©)
pip install -r optional_requirements.txt
```

## ë¹ ë¥¸ ì‹œì‘

### ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from unsloth_dataset import UnslothDatasetGenerator, DatasetConfig

# ë°ì´í„°ì…‹ ìƒì„±ê¸° ì„¤ì •
config = DatasetConfig(
    target_count=5000,
    max_seq_length=4096,
    train_test_split=0.9,
    formats=["sharegpt", "alpaca", "openai"],
    min_tokens=50,
    max_tokens=4096,
    eos_token="</s>",
    remove_duplicates=True,
    quality_threshold=0.7,
    batch_size=10,
    max_workers=2,
    seed=42,
    output_dir="output/dataset",
    include_metadata=True,
    shuffle_data=True
)

# ìƒì„±ê¸° ì´ˆê¸°í™”
generator = UnslothDatasetGenerator(config)

# ë°ì´í„°ì…‹ ìƒì„±
result = generator.generate_from_samples(samples, "my_dataset")

# íŒŒì¼ ì €ì¥
saved_paths = generator.save_datasets(result, "my_dataset")
```

### ì™¸ë¶€ ëª¨ë“ˆê³¼ì˜ í†µí•©

```python
from md_processor import create_processor
from qdrant_connector import create_integrated_processor
from openai_connector import create_openai_connector
from unsloth_dataset import UnslothDatasetGenerator

# ë°ì´í„° ì†ŒìŠ¤ ì´ˆê¸°í™”
md_processor = create_processor()
qdrant_connector = create_integrated_processor()
openai_connector = create_openai_connector()

# ë°ì´í„°ì…‹ ìƒì„±ê¸° ì´ˆê¸°í™”
generator = UnslothDatasetGenerator(
    target_count=5000,
    max_seq_length=4096,
    train_test_split=0.9,
    formats=["sharegpt", "alpaca", "openai"]
)

# ë°ì´í„°ì…‹ ìƒì„±
datasets = generator.generate_datasets(
    md_documents=md_processor.process_documents(),
    qdrant_documents=qdrant_connector.search_documents(),
    conversations=openai_connector.generate_conversations()
)

# ê²°ê³¼ ì €ì¥
generator.save_datasets(datasets, "syncfusion_unsloth_dataset")
```

## ì§€ì›ë˜ëŠ” ë°ì´í„° í¬ë§·

### ShareGPT í¬ë§·
```json
{
    "conversations": [
        {"from": "system", "value": "Syncfusion WinForms ê¸°ìˆ  ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
        {"from": "human", "value": "DataGrid ì»´í¬ë„ŒíŠ¸ ì‚¬ìš©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”."},
        {"from": "gpt", "value": "DataGridëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì‚¬ìš©í•©ë‹ˆë‹¤..."},
        {"from": "human", "value": "ë°ì´í„° ë°”ì¸ë”©ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?"},
        {"from": "gpt", "value": "ë°ì´í„° ë°”ì¸ë”©ì€..."}
    ]
}
```

### Alpaca í¬ë§·
```json
{
    "instruction": "Syncfusion WinForms DataGrid ì‚¬ìš©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    "input": "ì´ˆë³´ìë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë‹¨ê³„ë³„ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    "output": "DataGrid ì»´í¬ë„ŒíŠ¸ ì‚¬ìš©ë²•:\n1. í”„ë¡œì íŠ¸ì— ì°¸ì¡° ì¶”ê°€..."
}
```

### OpenAI í¬ë§·
```json
{
    "messages": [
        {"role": "system", "content": "Syncfusion WinForms ì „ë¬¸ê°€"},
        {"role": "user", "content": "DataGrid ì‚¬ìš©ë²• ì„¤ëª…"},
        {"role": "assistant", "content": "DataGrid ì‚¬ìš©ë²•ì€..."}
    ]
}
```

## êµ¬ì¡°

```
unsloth_dataset/
â”œâ”€â”€ __init__.py                 # ëª¨ë“ˆ ì´ˆê¸°í™”
â”œâ”€â”€ generator.py                # ë©”ì¸ ë°ì´í„°ì…‹ ìƒì„±ê¸°
â”œâ”€â”€ validator.py                # Unsloth í˜¸í™˜ì„± ê²€ì¦ê¸°
â”œâ”€â”€ statistics.py               # ë°ì´í„°ì…‹ í†µê³„ ìƒì„±ê¸°
â”œâ”€â”€ utils.py                    # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤
â”œâ”€â”€ formatters/                 # í¬ë§·ë³„ ìƒì„±ê¸°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ sharegpt_formatter.py   # ShareGPT í¬ë§· ìƒì„±ê¸°
â”‚   â”œâ”€â”€ alpaca_formatter.py     # Alpaca í¬ë§· ìƒì„±ê¸°
â”‚   â””â”€â”€ openai_formatter.py     # OpenAI í¬ë§· ìƒì„±ê¸°
â””â”€â”€ README.md                   # ì´ ë¬¸ì„œ
```

## API ë¬¸ì„œ

### UnslothDatasetGenerator

ë©”ì¸ ë°ì´í„°ì…‹ ìƒì„±ê¸° í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

#### ìƒì„±ì

```python
UnslothDatasetGenerator(config: DatasetConfig)
```

**ë§¤ê°œë³€ìˆ˜:**
- `config`: ë°ì´í„°ì…‹ ìƒì„± ì„¤ì •

#### ë©”ì„œë“œ

##### `generate_from_samples(samples, dataset_name)`
ìƒ˜í”Œ ë°ì´í„°ë¡œë¶€í„° ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.

**ë§¤ê°œë³€ìˆ˜:**
- `samples`: ìƒ˜í”Œ ë°ì´í„° ëª©ë¡
- `dataset_name`: ë°ì´í„°ì…‹ ì´ë¦„

**ë°˜í™˜ê°’:**
- `DatasetGenerationResult`: ìƒì„± ê²°ê³¼ ê°ì²´

##### `generate_datasets(md_documents, qdrant_documents, conversations)`
ì™¸ë¶€ ëª¨ë“ˆê³¼ì˜ í†µí•©ì„ í†µí•œ ë°ì´í„°ì…‹ ìƒì„±ì…ë‹ˆë‹¤.

**ë§¤ê°œë³€ìˆ˜:**
- `md_documents`: MD ì²˜ë¦¬ëœ ë¬¸ì„œ ëª©ë¡
- `qdrant_documents`: Qdrant ê²€ìƒ‰ ë¬¸ì„œ ëª©ë¡
- `conversations`: OpenAI ìƒì„± ëŒ€í™” ëª©ë¡

**ë°˜í™˜ê°’:**
- `DatasetGenerationResult`: ìƒì„± ê²°ê³¼ ê°ì²´

##### `save_datasets(result, base_name)`
ìƒì„±ëœ ë°ì´í„°ì…‹ì„ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

**ë§¤ê°œë³€ìˆ˜:**
- `result`: ìƒì„± ê²°ê³¼ ê°ì²´
- `base_name`: ê¸°ë³¸ íŒŒì¼ ì´ë¦„

**ë°˜í™˜ê°’:**
- `dict`: ì €ì¥ëœ íŒŒì¼ ê²½ë¡œ ëª©ë¡

### DatasetConfig

ë°ì´í„°ì…‹ ìƒì„± ì„¤ì • í´ë˜ìŠ¤ì…ë‹ˆë‹¤.

#### ì†ì„±

- `target_count`: ëª©í‘œ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ê°’: 1000)
- `max_seq_length`: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸ê°’: 4096)
- `train_test_split`: í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.9)
- `formats`: ìƒì„±í•  í¬ë§· ëª©ë¡ (ê¸°ë³¸ê°’: ["sharegpt", "alpaca", "openai"])
- `min_tokens`: ìµœì†Œ í† í° ìˆ˜ (ê¸°ë³¸ê°’: 50)
- `max_tokens`: ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸ê°’: 4096)
- `eos_token`: EOS í† í° (ê¸°ë³¸ê°’: "</s>")
- `remove_duplicates`: ì¤‘ë³µ ì œê±° ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
- `quality_threshold`: í’ˆì§ˆ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.7)
- `batch_size`: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 10)
- `max_workers`: ìµœëŒ€ ì‘ì—…ì ìˆ˜ (ê¸°ë³¸ê°’: 2)
- `seed`: ëœë¤ ì‹œë“œ (ê¸°ë³¸ê°’: 42)
- `output_dir`: ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸ê°’: "output")
- `include_metadata`: ë©”íƒ€ë°ì´í„° í¬í•¨ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
- `shuffle_data`: ë°ì´í„° ì…”í”Œ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
- `progress_interval`: ì§„í–‰ í‘œì‹œ ê°„ê²© (ê¸°ë³¸ê°’: 100)

## í’ˆì§ˆ ë³´ì¥

### í† í° ìˆ˜ ê²€ì¦
- 50-4096 í† í° ë²”ìœ„ ì¤€ìˆ˜
- ì‹œí€€ìŠ¤ ê¸¸ì´ ìµœì í™”

### ëŒ€í™” êµ¬ì¡° ê²€ì¦
- ì˜¬ë°”ë¥¸ ì—­í•  ìˆœì„œ í™•ì¸
- í•„ìˆ˜ í•„ë“œ ê²€ì¦

### ì¤‘ë³µ ì œê±°
- ìœ ì‚¬í•œ ëŒ€í™” í•„í„°ë§
- í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ íƒì§€

### EOS í† í° ì²˜ë¦¬
- ì ì ˆí•œ ì¢…ë£Œ í† í° ì¶”ê°€
- í¬ë§·ë³„ ìµœì í™”

### ì¸ì½”ë”© ê²€ì¦
- UTF-8 í˜¸í™˜ì„± í™•ì¸
- íŠ¹ìˆ˜ ë¬¸ì ì²˜ë¦¬

## ë©”íƒ€ë°ì´í„° ìƒì„±

### ë°ì´í„°ì…‹ í†µê³„
- ìƒ˜í”Œ ìˆ˜, í‰ê·  í† í° ìˆ˜, í„´ ë¶„í¬
- í¬ë§·ë³„ í†µê³„ ì •ë³´

### í’ˆì§ˆ ì§€í‘œ
- ë‹¤ì–‘ì„± ì ìˆ˜, í’ˆì§ˆ ì ìˆ˜
- í¬ë§·ë³„ í’ˆì§ˆ í‰ê°€

### ì†ŒìŠ¤ ì •ë³´
- ì‚¬ìš©ëœ ë¬¸ì„œ ë° ìƒì„± ì„¤ì •
- ë°ì´í„° ì†ŒìŠ¤ ì¶”ì 

### Unsloth í˜¸í™˜ì„±
- ê²€ì¦ ê²°ê³¼ ë¦¬í¬íŠ¸
- ìµœì í™” ê¶Œì¥ì‚¬í•­

## ì„±ëŠ¥ ìµœì í™”

### ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
- ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ íŒŒì¼ ì²˜ë¦¬
- ë°°ì¹˜ ì²˜ë¦¬ ì§€ì›
- ë©”ëª¨ë¦¬ ê´€ë¦¬ ìµœì í™”

### ë³‘ë ¬ ì²˜ë¦¬
- ë©€í‹°ìŠ¤ë ˆë”© ì§€ì›
- ë¹„ë™ê¸° ì²˜ë¦¬
- ì‘ì—… ë¶„í• 

### ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
- ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬
- ì ì§„ì  ë¡œë”©
- ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€

## í…ŒìŠ¤íŠ¸

```bash
# ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python -m pytest tests/

# í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python test_unsloth_dataset.py

# íŠ¹ì • í…ŒìŠ¤íŠ¸ ì‹¤í–‰
python -m pytest tests/test_formatters.py
```

## ì˜ˆì œ

ìì„¸í•œ ì‚¬ìš© ì˜ˆì œëŠ” `example_usage.py` íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œ

1. **ë©”ëª¨ë¦¬ ë¶€ì¡±**
   - `batch_size` ê°’ì„ ì¤„ì´ì„¸ìš”
   - `max_workers` ê°’ì„ ì¡°ì ˆí•˜ì„¸ìš”
   - ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•˜ì„¸ìš”

2. **í† í° ìˆ˜ ì´ˆê³¼**
   - `max_tokens` ê°’ì„ ì¡°ì ˆí•˜ì„¸ìš”
   - í…ìŠ¤íŠ¸ë¥¼ ë¶„í• í•˜ì—¬ ì²˜ë¦¬í•˜ì„¸ìš”
   - ìš”ì•½ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì„¸ìš”

3. **í¬ë§· ë³€í™˜ ì‹¤íŒ¨**
   - ì…ë ¥ ë°ì´í„° êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”
   - í•„ë“œ ì´ë¦„ì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•˜ì„¸ìš”
   - ìœ íš¨ì„± ê²€ì‚¬ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”

### ë¡œê¹…

```python
import logging

# ë¡œê¹… ë ˆë²¨ ì„¤ì •
logging.basicConfig(level=logging.INFO)

# íŠ¹ì • ëª¨ë“ˆì˜ ë¡œê¹… ì„¤ì •
logging.getLogger('unsloth_dataset').setLevel(logging.DEBUG)
```

## ê¸°ì—¬

ê¸°ì—¬ëŠ” í™˜ì˜í•©ë‹ˆë‹¤! ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¼ì£¼ì„¸ìš”:

1. ì´ ì €ì¥ì†Œë¥¼ í¬í¬í•˜ì„¸ìš”
2. ê¸°ëŠ¥ ë¸Œëœì¹˜ë¥¼ ìƒì„±í•˜ì„¸ìš” (`git checkout -b feature/amazing-feature`)
3. ë³€ê²½ ì‚¬í•­ì„ ì»¤ë°‹í•˜ì„¸ìš” (`git commit -m 'Add amazing feature'`)
4. ë¸Œëœì¹˜ë¥¼ í‘¸ì‹œí•˜ì„¸ìš” (`git push origin feature/amazing-feature`)
5. Pull Requestë¥¼ ìƒì„±í•˜ì„¸ìš”

## ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ `LICENSE` íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ì§€ì›

ë¬¸ì˜ì‚¬í•­ì´ ìˆìœ¼ë©´ ë‹¤ìŒ ë°©ë²•ìœ¼ë¡œ ì—°ë½ì£¼ì„¸ìš”:

- ì´ìŠˆ íŠ¸ë˜ì»¤: [GitHub Issues](https://github.com/your-repo/issues)
- ì´ë©”ì¼: support@example.com

## ì—…ë°ì´íŠ¸ ë¡œê·¸

### v1.0.0 (2024-01-01)
- ì´ˆê¸° ë²„ì „ ì¶œì‹œ
- ShareGPT, Alpaca, OpenAI í¬ë§· ì§€ì›
- ê¸°ë³¸ í†µí•© ê¸°ëŠ¥ êµ¬í˜„
- í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì™„ì„±

### v1.1.0 (2024-01-15)
- ì„±ëŠ¥ ìµœì í™”
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê°œì„ 
- ìƒˆë¡œìš´ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ì¶”ê°€
- ë²„ê·¸ ìˆ˜ì •

## ê´€ë ¨ í”„ë¡œì íŠ¸

- [MD Processor](../md_processor/) - MD ë¬¸ì„œ ì²˜ë¦¬ ëª¨ë“ˆ
- [Qdrant Connector](../qdrant_connector/) - Qdrant ë²¡í„° DB ì—°ë™ ëª¨ë“ˆ
- [OpenAI Connector](../openai_connector/) - OpenAI API ì—°ë™ ëª¨ë“ˆ

## í¬ë ˆë”§

ì´ í”„ë¡œì íŠ¸ëŠ” ë‹¤ìŒ í”„ë¡œì íŠ¸ë“¤ì˜ ì˜í–¥ì„ ë°›ì•˜ìŠµë‹ˆë‹¤:

- [Unsloth](https://github.com/unslothai/unsloth) - ê³ ì„±ëŠ¥ ì–¸ì–´ ëª¨ë¸ í›ˆë ¨
- [ShareGPT](https://sharegpt.com/) - ëŒ€í™” ë°ì´í„°ì…‹
- [Alpaca](https://github.com/tatsu-lab/stanford_alpaca) - ì§€ì‹œ ë°ì´í„°ì…‹
- [OpenAI API](https://platform.openai.com/) - ìƒì„±í˜• AI API