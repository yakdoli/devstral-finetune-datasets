#!/usr/bin/env python3
"""
ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°
ëª¨ë“  í†µí•© í…ŒìŠ¤íŠ¸ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ê³  ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
"""

import asyncio
import json
import logging
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class FinalIntegrationTestRunner:
    """ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°"""
    
    def __init__(self):
        self.test_results = {}
        self.start_time = None
        self.end_time = None
        self.test_modules = [
            "test_complete_pipeline_integration",
            "test_performance_validation", 
            "test_unsloth_compatibility"
        ]
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("=" * 80)
        logger.info("ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘")
        logger.info("=" * 80)
        
        self.start_time = datetime.now()
        
        try:
            # 1. ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸
            await self._run_pipeline_integration_test()
            
            # 2. ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸
            await self._run_performance_validation_test()
            
            # 3. Unsloth í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
            await self._run_unsloth_compatibility_test()
            
            self.end_time = datetime.now()
            
            # 4. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
            return await self._generate_comprehensive_report()
            
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {"status": "failed", "error": str(e)}
    
    async def _run_pipeline_integration_test(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("1. ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        
        test_result = {
            "status": "not_run",
            "start_time": datetime.now().isoformat(),
            "duration": 0,
            "report_file": None,
            "error": None
        }
        
        try:
            start_time = time.time()
            
            # í…ŒìŠ¤íŠ¸ ëª¨ë“ˆ ë™ì  ì„í¬íŠ¸ ë° ì‹¤í–‰
            try:
                from test_complete_pipeline_integration import CompletePipelineIntegrationTest
                
                tester = CompletePipelineIntegrationTest()
                result = await tester.run_complete_test()
                
                test_result["status"] = result.get("status", "unknown")
                test_result["detailed_results"] = result
                
                # ë¦¬í¬íŠ¸ íŒŒì¼ ì°¾ê¸°
                report_files = list(Path(".").glob("complete_pipeline_integration_report_*.json"))
                if report_files:
                    test_result["report_file"] = str(report_files[-1])  # ê°€ì¥ ìµœê·¼ íŒŒì¼
                
                logger.info("  âœ“ ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
                
            except ImportError as e:
                logger.warning(f"  âš  íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ëª¨ë“ˆ ì—†ìŒ: {e}")
                test_result["status"] = "skipped"
                test_result["error"] = f"Module not found: {e}"
            
            end_time = time.time()
            test_result["duration"] = end_time - start_time
            
        except Exception as e:
            test_result["status"] = "failed"
            test_result["error"] = str(e)
            logger.error(f"  âœ— íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        self.test_results["pipeline_integration"] = test_result
    
    async def _run_performance_validation_test(self):
        """ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("2. ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        
        test_result = {
            "status": "not_run",
            "start_time": datetime.now().isoformat(),
            "duration": 0,
            "report_file": None,
            "error": None
        }
        
        try:
            start_time = time.time()
            
            try:
                from test_performance_validation import PerformanceValidationTest
                
                tester = PerformanceValidationTest()
                result = await tester.run_performance_tests()
                
                test_result["status"] = "success" if result.get("overall_score", 0) > 0 else "failed"
                test_result["detailed_results"] = result
                
                # ë¦¬í¬íŠ¸ íŒŒì¼ ì°¾ê¸°
                report_files = list(Path(".").glob("performance_validation_report_*.json"))
                if report_files:
                    test_result["report_file"] = str(report_files[-1])
                
                logger.info("  âœ“ ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
                
            except ImportError as e:
                logger.warning(f"  âš  ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸ ëª¨ë“ˆ ì—†ìŒ: {e}")
                test_result["status"] = "skipped"
                test_result["error"] = f"Module not found: {e}"
            
            end_time = time.time()
            test_result["duration"] = end_time - start_time
            
        except Exception as e:
            test_result["status"] = "failed"
            test_result["error"] = str(e)
            logger.error(f"  âœ— ì„±ëŠ¥ ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        self.test_results["performance_validation"] = test_result
    
    async def _run_unsloth_compatibility_test(self):
        """Unsloth í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("3. Unsloth í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        
        test_result = {
            "status": "not_run",
            "start_time": datetime.now().isoformat(),
            "duration": 0,
            "report_file": None,
            "error": None
        }
        
        try:
            start_time = time.time()
            
            try:
                from test_unsloth_compatibility import UnslothCompatibilityTest
                
                tester = UnslothCompatibilityTest()
                result = await tester.run_compatibility_tests()
                
                test_result["status"] = "success" if result.get("overall_score", 0) > 0 else "failed"
                test_result["detailed_results"] = result
                
                # ë¦¬í¬íŠ¸ íŒŒì¼ ì°¾ê¸°
                report_files = list(Path(".").glob("unsloth_compatibility_report_*.json"))
                if report_files:
                    test_result["report_file"] = str(report_files[-1])
                
                logger.info("  âœ“ Unsloth í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
                
            except ImportError as e:
                logger.warning(f"  âš  Unsloth í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ëª¨ë“ˆ ì—†ìŒ: {e}")
                test_result["status"] = "skipped"
                test_result["error"] = f"Module not found: {e}"
            
            end_time = time.time()
            test_result["duration"] = end_time - start_time
            
        except Exception as e:
            test_result["status"] = "failed"
            test_result["error"] = str(e)
            logger.error(f"  âœ— Unsloth í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        self.test_results["unsloth_compatibility"] = test_result
    
    async def _generate_comprehensive_report(self) -> Dict[str, Any]:
        """ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±"""
        logger.info("4. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        total_duration = (self.end_time - self.start_time).total_seconds()
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
        test_summary = {
            "total_tests": len(self.test_results),
            "passed": 0,
            "failed": 0,
            "skipped": 0,
            "total_duration": total_duration
        }
        
        for test_name, result in self.test_results.items():
            status = result.get("status", "unknown")
            if status == "success":
                test_summary["passed"] += 1
            elif status == "failed":
                test_summary["failed"] += 1
            elif status == "skipped":
                test_summary["skipped"] += 1
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        performance_metrics = {}
        if "performance_validation" in self.test_results:
            perf_result = self.test_results["performance_validation"].get("detailed_results", {})
            performance_metrics = {
                "overall_score": perf_result.get("overall_score", 0),
                "performance_grade": perf_result.get("performance_grade", "N/A"),
                "max_throughput": perf_result.get("summary", {}).get("max_throughput_docs_per_sec", 0),
                "memory_efficiency": perf_result.get("summary", {}).get("memory_efficiency_kb_per_item", 0),
                "scalability_stability": perf_result.get("summary", {}).get("scalability_stability", 0)
            }
        
        # í˜¸í™˜ì„± ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        compatibility_metrics = {}
        if "unsloth_compatibility" in self.test_results:
            compat_result = self.test_results["unsloth_compatibility"].get("detailed_results", {})
            compatibility_metrics = {
                "overall_score": compat_result.get("overall_score", 0),
                "compatibility_grade": compat_result.get("compatibility_grade", "N/A"),
                "ready_for_training": compat_result.get("summary", {}).get("ready_for_training", False),
                "data_structure_valid": compat_result.get("summary", {}).get("data_structure_valid", False),
                "format_compliant": compat_result.get("summary", {}).get("format_compliant", False)
            }
        
        # íŒŒì´í”„ë¼ì¸ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        pipeline_metrics = {}
        if "pipeline_integration" in self.test_results:
            pipeline_result = self.test_results["pipeline_integration"].get("detailed_results", {})
            if isinstance(pipeline_result, dict):
                pipeline_metrics = {
                    "status": pipeline_result.get("status", "unknown"),
                    "modules_tested": len(pipeline_result.get("detailed_results", {})),
                    "environment_ready": pipeline_result.get("detailed_results", {}).get("environment_setup", {}).get("status") == "success"
                }
        
        # ì „ì²´ ì‹œìŠ¤í…œ ì¤€ë¹„ë„ í‰ê°€
        system_readiness = self._evaluate_system_readiness(
            test_summary, performance_metrics, compatibility_metrics, pipeline_metrics
        )
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = self._generate_comprehensive_recommendations(
            test_summary, performance_metrics, compatibility_metrics, pipeline_metrics
        )
        
        # ì¢…í•© ë¦¬í¬íŠ¸ êµ¬ì„±
        comprehensive_report = {
            "test_info": {
                "timestamp": self.start_time.isoformat(),
                "total_duration_seconds": total_duration,
                "test_type": "final_integration_comprehensive",
                "test_modules": self.test_modules
            },
            "summary": test_summary,
            "system_readiness": system_readiness,
            "performance_metrics": performance_metrics,
            "compatibility_metrics": compatibility_metrics,
            "pipeline_metrics": pipeline_metrics,
            "detailed_test_results": self.test_results,
            "recommendations": recommendations,
            "report_files": self._collect_report_files()
        }
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"final_integration_comprehensive_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, indent=2, ensure_ascii=False)
        
        # ê²°ê³¼ ì¶œë ¥
        self._print_comprehensive_results(comprehensive_report, report_file)
        
        return comprehensive_report
    
    def _evaluate_system_readiness(self, test_summary, performance_metrics, compatibility_metrics, pipeline_metrics) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì¤€ë¹„ë„ í‰ê°€"""
        
        # ê¸°ë³¸ ì ìˆ˜ ê³„ì‚°
        test_success_rate = test_summary["passed"] / test_summary["total_tests"] if test_summary["total_tests"] > 0 else 0
        performance_score = performance_metrics.get("overall_score", 0) / 100
        compatibility_score = compatibility_metrics.get("overall_score", 0) / 100
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        weights = {
            "test_success": 0.3,
            "performance": 0.35,
            "compatibility": 0.35
        }
        
        overall_readiness = (
            test_success_rate * weights["test_success"] +
            performance_score * weights["performance"] +
            compatibility_score * weights["compatibility"]
        ) * 100
        
        # ì¤€ë¹„ë„ ë“±ê¸‰ ê²°ì •
        if overall_readiness >= 95:
            readiness_grade = "Production Ready"
        elif overall_readiness >= 90:
            readiness_grade = "Near Production Ready"
        elif overall_readiness >= 80:
            readiness_grade = "Development Ready"
        elif overall_readiness >= 70:
            readiness_grade = "Testing Ready"
        elif overall_readiness >= 60:
            readiness_grade = "Basic Functionality"
        else:
            readiness_grade = "Not Ready"
        
        # ê°œë³„ ì¤€ë¹„ë„ ì²´í¬
        readiness_checks = {
            "tests_passing": test_summary["failed"] == 0,
            "performance_acceptable": performance_metrics.get("overall_score", 0) >= 70,
            "compatibility_verified": compatibility_metrics.get("ready_for_training", False),
            "pipeline_functional": pipeline_metrics.get("status") == "success",
            "environment_configured": pipeline_metrics.get("environment_ready", False)
        }
        
        return {
            "overall_score": overall_readiness,
            "readiness_grade": readiness_grade,
            "individual_checks": readiness_checks,
            "critical_issues": test_summary["failed"] > 0,
            "performance_issues": performance_metrics.get("overall_score", 0) < 70,
            "compatibility_issues": not compatibility_metrics.get("ready_for_training", False)
        }
    
    def _generate_comprehensive_recommendations(self, test_summary, performance_metrics, compatibility_metrics, pipeline_metrics) -> List[str]:
        """ì¢…í•© ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ ê´€ë ¨
        if test_summary["failed"] > 0:
            recommendations.append(f"ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ {test_summary['failed']}ê°œ í•´ê²° í•„ìš” - ìƒì„¸ ë¡œê·¸ í™•ì¸")
        
        # ì„±ëŠ¥ ê´€ë ¨
        perf_score = performance_metrics.get("overall_score", 0)
        if perf_score < 70:
            recommendations.append(f"ì„±ëŠ¥ ê°œì„  í•„ìš” (í˜„ì¬: {perf_score:.1f}/100) - ì²˜ë¦¬ëŸ‰ ë° ë©”ëª¨ë¦¬ ìµœì í™”")
        elif perf_score < 85:
            recommendations.append("ì„±ëŠ¥ ìµœì í™” ê¶Œì¥ - ë” ë‚˜ì€ ì‚¬ìš©ì ê²½í—˜ì„ ìœ„í•´")
        
        # í˜¸í™˜ì„± ê´€ë ¨
        compat_score = compatibility_metrics.get("overall_score", 0)
        if not compatibility_metrics.get("ready_for_training", False):
            recommendations.append("Unsloth í˜¸í™˜ì„± ì´ìŠˆ í•´ê²° í•„ìš” - ë°ì´í„° í¬ë§· ë° êµ¬ì¡° ê²€í† ")
        elif compat_score < 90:
            recommendations.append("í˜¸í™˜ì„± ê°œì„  ê¶Œì¥ - ë” ì•ˆì •ì ì¸ í›ˆë ¨ì„ ìœ„í•´")
        
        # íŒŒì´í”„ë¼ì¸ ê´€ë ¨
        if pipeline_metrics.get("status") != "success":
            recommendations.append("íŒŒì´í”„ë¼ì¸ í†µí•© ì´ìŠˆ í•´ê²° í•„ìš” - ëª¨ë“ˆ ê°„ ì—°ë™ í™•ì¸")
        
        if not pipeline_metrics.get("environment_ready", False):
            recommendations.append("í™˜ê²½ ì„¤ì • ì™„ë£Œ í•„ìš” - í•„ìˆ˜ ëª¨ë“ˆ ë° ì„¤ì • íŒŒì¼ í™•ì¸")
        
        # ì „ì²´ì ì¸ ê¶Œì¥ì‚¬í•­
        if not recommendations:
            recommendations.append("ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼ - í”„ë¡œë•ì…˜ ë°°í¬ ì¤€ë¹„ ì™„ë£Œ")
            recommendations.append("ì •ê¸°ì ì¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë° í’ˆì§ˆ ê²€ì¦ ê¶Œì¥")
        else:
            recommendations.append("ì´ìŠˆ í•´ê²° í›„ ì¬í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ê¶Œì¥")
        
        return recommendations
    
    def _collect_report_files(self) -> Dict[str, str]:
        """ìƒì„±ëœ ë¦¬í¬íŠ¸ íŒŒì¼ë“¤ ìˆ˜ì§‘"""
        report_files = {}
        
        for test_name, result in self.test_results.items():
            report_file = result.get("report_file")
            if report_file and Path(report_file).exists():
                report_files[test_name] = report_file
        
        return report_files
    
    def _print_comprehensive_results(self, report: Dict[str, Any], report_file: str):
        """ì¢…í•© ê²°ê³¼ ì¶œë ¥"""
        logger.info("=" * 80)
        logger.info("ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸ ì¢…í•© ê²°ê³¼")
        logger.info("=" * 80)
        
        # ê¸°ë³¸ ì •ë³´
        summary = report["summary"]
        readiness = report["system_readiness"]
        
        logger.info(f"ì´ í…ŒìŠ¤íŠ¸ ì‹œê°„: {summary['total_duration']:.1f}ì´ˆ")
        logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼: {summary['passed']}ê°œ ì„±ê³µ, {summary['failed']}ê°œ ì‹¤íŒ¨, {summary['skipped']}ê°œ ê±´ë„ˆëœ€")
        logger.info(f"ì‹œìŠ¤í…œ ì¤€ë¹„ë„: {readiness['overall_score']:.1f}/100 ({readiness['readiness_grade']})")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        perf = report["performance_metrics"]
        if perf:
            logger.info(f"ì„±ëŠ¥ ì ìˆ˜: {perf.get('overall_score', 0):.1f}/100 (ë“±ê¸‰: {perf.get('performance_grade', 'N/A')})")
            logger.info(f"ìµœëŒ€ ì²˜ë¦¬ëŸ‰: {perf.get('max_throughput', 0):.1f} docs/sec")
        
        # í˜¸í™˜ì„± ë©”íŠ¸ë¦­
        compat = report["compatibility_metrics"]
        if compat:
            logger.info(f"í˜¸í™˜ì„± ì ìˆ˜: {compat.get('overall_score', 0):.1f}/100 (ë“±ê¸‰: {compat.get('compatibility_grade', 'N/A')})")
            logger.info(f"Unsloth í›ˆë ¨ ì¤€ë¹„: {'ì™„ë£Œ' if compat.get('ready_for_training', False) else 'ë¯¸ì™„ë£Œ'}")
        
        # ì¤€ë¹„ë„ ì²´í¬
        logger.info("")
        logger.info("ì‹œìŠ¤í…œ ì¤€ë¹„ë„ ì²´í¬:")
        checks = readiness["individual_checks"]
        for check_name, status in checks.items():
            status_icon = "âœ“" if status else "âœ—"
            check_display = check_name.replace("_", " ").title()
            logger.info(f"  {status_icon} {check_display}")
        
        # ê¶Œì¥ì‚¬í•­
        logger.info("")
        logger.info("ê¶Œì¥ì‚¬í•­:")
        for i, rec in enumerate(report["recommendations"], 1):
            logger.info(f"  {i}. {rec}")
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ë“¤
        logger.info("")
        logger.info("ìƒì„±ëœ ë¦¬í¬íŠ¸ íŒŒì¼:")
        logger.info(f"  ì¢…í•© ë¦¬í¬íŠ¸: {report_file}")
        for test_name, file_path in report["report_files"].items():
            logger.info(f"  {test_name}: {file_path}")
        
        logger.info("=" * 80)


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ì´ í…ŒìŠ¤íŠ¸ëŠ” ì „ì²´ ì‹œìŠ¤í…œì˜ ì¤€ë¹„ë„ë¥¼ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.")
    print("í…ŒìŠ¤íŠ¸ ì™„ë£Œê¹Œì§€ ìˆ˜ ë¶„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n")
    
    runner = FinalIntegrationTestRunner()
    
    try:
        comprehensive_report = await runner.run_all_tests()
        
        readiness = comprehensive_report.get("system_readiness", {})
        overall_score = readiness.get("overall_score", 0)
        grade = readiness.get("readiness_grade", "Unknown")
        
        if overall_score >= 90:
            print(f"\nğŸ‰ ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ì‹œìŠ¤í…œ ì¤€ë¹„ë„: {overall_score:.1f}/100")
            print(f"ìƒíƒœ: {grade}")
            print("ì‹œìŠ¤í…œì´ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!")
        elif overall_score >= 70:
            print(f"\nâœ… ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ì‹œìŠ¤í…œ ì¤€ë¹„ë„: {overall_score:.1f}/100")
            print(f"ìƒíƒœ: {grade}")
            print("ì‹œìŠ¤í…œì´ ê¸°ë³¸ì ì¸ ê¸°ëŠ¥ì„ ìˆ˜í–‰í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            print(f"\nâš ï¸ ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ì‹œìŠ¤í…œ ì¤€ë¹„ë„: {overall_score:.1f}/100")
            print(f"ìƒíƒœ: {grade}")
            print("ì‹œìŠ¤í…œ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì¤‘ìš”í•œ ì´ìŠˆ ì•Œë¦¼
        if readiness.get("critical_issues", False):
            print("\nğŸš¨ ì¤‘ìš”: ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸ê°€ ìˆìŠµë‹ˆë‹¤. ìƒì„¸ ë¡œê·¸ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
        if readiness.get("compatibility_issues", False):
            print("\nâš ï¸ ì£¼ì˜: Unsloth í˜¸í™˜ì„± ì´ìŠˆê°€ ìˆìŠµë‹ˆë‹¤. ë°ì´í„° í¬ë§·ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
        
    except KeyboardInterrupt:
        print("\n\ní…ŒìŠ¤íŠ¸ê°€ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"\nâŒ ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        sys.exit(1)
    
    print("\n=== ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")


if __name__ == "__main__":
    asyncio.run(main())