#!/usr/bin/env python3
"""
Unsloth í”„ë ˆì„ì›Œí¬ í˜¸í™˜ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸
ìƒì„±ëœ ë°ì´í„°ì…‹ì´ Unslothì—ì„œ ì •ìƒì ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥í•œì§€ ê²€ì¦
"""

import asyncio
import json
import logging
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class UnslothCompatibilityTest:
    """Unsloth í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.test_results = {}
        self.compatibility_issues = []
        self.test_datasets = {}
    
    async def run_compatibility_tests(self) -> Dict[str, Any]:
        """í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("=== Unsloth í˜¸í™˜ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
        
        try:
            # 1. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„±
            await self._generate_test_datasets()
            
            # 2. ë°ì´í„° êµ¬ì¡° ê²€ì¦
            await self._test_data_structure()
            
            # 3. í•„ë“œ ìœ íš¨ì„± ê²€ì¦
            await self._test_field_validation()
            
            # 4. í¬ë§·ë³„ í˜¸í™˜ì„± ê²€ì¦
            await self._test_format_compatibility()
            
            # 5. ë°ì´í„° í’ˆì§ˆ ê²€ì¦
            await self._test_data_quality()
            
            # 6. í† í°í™” í˜¸í™˜ì„± ê²€ì¦
            await self._test_tokenization_compatibility()
            
            # 7. ì‹¤ì œ Unsloth ë¡œë”© í…ŒìŠ¤íŠ¸ (ì‹œë®¬ë ˆì´ì…˜)
            await self._test_unsloth_loading()
            
            # 8. ìµœì¢… í˜¸í™˜ì„± ë¦¬í¬íŠ¸ ìƒì„±
            return await self._generate_compatibility_report()
            
        except Exception as e:
            logger.error(f"í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {"status": "failed", "error": str(e)}
    
    async def _generate_test_datasets(self):
        """í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹ ìƒì„±"""
        logger.info("1. í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")
        
        # ShareGPT í¬ë§· í…ŒìŠ¤íŠ¸ ë°ì´í„°
        sharegpt_data = [
            {
                "conversations": [
                    {"from": "human", "value": "How do I create a GridControl in Syncfusion WinForms?"},
                    {"from": "gpt", "value": "To create a GridControl in Syncfusion WinForms, you need to:\n1. Add the Syncfusion.Windows.Forms.Grid namespace\n2. Create an instance of GridControl\n3. Set up data binding\n\nHere's a simple example:\n```csharp\nGridControl gridControl = new GridControl();\ngridControl.DataSource = dataTable;\n```"}
                ]
            },
            {
                "conversations": [
                    {"from": "human", "value": "What are the key properties of ChartControl?"},
                    {"from": "gpt", "value": "The key properties of ChartControl include:\n- Series: Collection of data series\n- ChartType: Type of chart (Line, Bar, Pie, etc.)\n- Title: Chart title\n- Legend: Legend configuration\n- Axes: X and Y axis settings"}
                ]
            },
            {
                "conversations": [
                    {"from": "human", "value": "How to handle events in TreeViewAdv?"},
                    {"from": "gpt", "value": "TreeViewAdv provides several events:\n- NodeClick: When a node is clicked\n- NodeExpanded: When a node is expanded\n- NodeCollapsed: When a node is collapsed\n- NodeChecked: When a node checkbox is changed\n\nExample:\n```csharp\ntreeViewAdv.NodeClick += (sender, e) => {\n    // Handle node click\n};\n```"}
                ]
            }
        ]
        
        # Alpaca í¬ë§· í…ŒìŠ¤íŠ¸ ë°ì´í„°
        alpaca_data = [
            {
                "instruction": "Explain how to create a GridControl in Syncfusion WinForms",
                "input": "",
                "output": "To create a GridControl in Syncfusion WinForms, you need to:\n1. Add the Syncfusion.Windows.Forms.Grid namespace\n2. Create an instance of GridControl\n3. Set up data binding\n\nHere's a simple example:\n```csharp\nGridControl gridControl = new GridControl();\ngridControl.DataSource = dataTable;\n```"
            },
            {
                "instruction": "List the key properties of ChartControl",
                "input": "Syncfusion WinForms ChartControl",
                "output": "The key properties of ChartControl include:\n- Series: Collection of data series\n- ChartType: Type of chart (Line, Bar, Pie, etc.)\n- Title: Chart title\n- Legend: Legend configuration\n- Axes: X and Y axis settings"
            }
        ]
        
        # OpenAI í¬ë§· í…ŒìŠ¤íŠ¸ ë°ì´í„°
        openai_data = [
            {
                "messages": [
                    {"role": "user", "content": "How do I create a GridControl in Syncfusion WinForms?"},
                    {"role": "assistant", "content": "To create a GridControl in Syncfusion WinForms, you need to:\n1. Add the Syncfusion.Windows.Forms.Grid namespace\n2. Create an instance of GridControl\n3. Set up data binding\n\nHere's a simple example:\n```csharp\nGridControl gridControl = new GridControl();\ngridControl.DataSource = dataTable;\n```"}
                ]
            },
            {
                "messages": [
                    {"role": "user", "content": "What are the key properties of ChartControl?"},
                    {"role": "assistant", "content": "The key properties of ChartControl include:\n- Series: Collection of data series\n- ChartType: Type of chart (Line, Bar, Pie, etc.)\n- Title: Chart title\n- Legend: Legend configuration\n- Axes: X and Y axis settings"}
                ]
            }
        ]
        
        self.test_datasets = {
            "sharegpt": sharegpt_data,
            "alpaca": alpaca_data,
            "openai": openai_data
        }
        
        logger.info(f"âœ“ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ:")
        for format_name, data in self.test_datasets.items():
            logger.info(f"  - {format_name}: {len(data)}ê°œ ìƒ˜í”Œ")
    
    async def _test_data_structure(self):
        """ë°ì´í„° êµ¬ì¡° ê²€ì¦"""
        logger.info("2. ë°ì´í„° êµ¬ì¡° ê²€ì¦ ì¤‘...")
        
        test_result = {
            "status": "success",
            "format_results": {},
            "structure_issues": []
        }
        
        for format_name, dataset in self.test_datasets.items():
            logger.info(f"  {format_name} í¬ë§· êµ¬ì¡° ê²€ì¦ ì¤‘...")
            
            format_result = {
                "valid_items": 0,
                "invalid_items": 0,
                "structure_errors": []
            }
            
            for i, item in enumerate(dataset):
                try:
                    if format_name == "sharegpt":
                        # ShareGPT êµ¬ì¡° ê²€ì¦
                        if not isinstance(item, dict):
                            raise ValueError("Item must be a dictionary")
                        
                        if "conversations" not in item:
                            raise ValueError("Missing 'conversations' field")
                        
                        if not isinstance(item["conversations"], list):
                            raise ValueError("'conversations' must be a list")
                        
                        for j, conv in enumerate(item["conversations"]):
                            if not isinstance(conv, dict):
                                raise ValueError(f"Conversation {j} must be a dictionary")
                            
                            if "from" not in conv or "value" not in conv:
                                raise ValueError(f"Conversation {j} missing 'from' or 'value' field")
                            
                            if conv["from"] not in ["human", "gpt", "system"]:
                                raise ValueError(f"Invalid 'from' value: {conv['from']}")
                    
                    elif format_name == "alpaca":
                        # Alpaca êµ¬ì¡° ê²€ì¦
                        required_fields = ["instruction", "input", "output"]
                        for field in required_fields:
                            if field not in item:
                                raise ValueError(f"Missing required field: {field}")
                            
                            if not isinstance(item[field], str):
                                raise ValueError(f"Field '{field}' must be a string")
                    
                    elif format_name == "openai":
                        # OpenAI êµ¬ì¡° ê²€ì¦
                        if "messages" not in item:
                            raise ValueError("Missing 'messages' field")
                        
                        if not isinstance(item["messages"], list):
                            raise ValueError("'messages' must be a list")
                        
                        for j, msg in enumerate(item["messages"]):
                            if not isinstance(msg, dict):
                                raise ValueError(f"Message {j} must be a dictionary")
                            
                            if "role" not in msg or "content" not in msg:
                                raise ValueError(f"Message {j} missing 'role' or 'content' field")
                            
                            if msg["role"] not in ["user", "assistant", "system"]:
                                raise ValueError(f"Invalid role: {msg['role']}")
                    
                    format_result["valid_items"] += 1
                    
                except Exception as e:
                    format_result["invalid_items"] += 1
                    format_result["structure_errors"].append(f"Item {i}: {str(e)}")
                    logger.warning(f"    êµ¬ì¡° ì˜¤ë¥˜ (Item {i}): {e}")
            
            test_result["format_results"][format_name] = format_result
            
            if format_result["invalid_items"] == 0:
                logger.info(f"    âœ“ {format_name}: ëª¨ë“  í•­ëª© êµ¬ì¡° ìœ íš¨")
            else:
                logger.warning(f"    âš  {format_name}: {format_result['invalid_items']}ê°œ í•­ëª© êµ¬ì¡° ì˜¤ë¥˜")
                test_result["structure_issues"].extend(format_result["structure_errors"])
        
        if test_result["structure_issues"]:
            test_result["status"] = "warning"
        
        self.test_results["data_structure"] = test_result
        logger.info("âœ“ ë°ì´í„° êµ¬ì¡° ê²€ì¦ ì™„ë£Œ")    
async def _test_field_validation(self):
        """í•„ë“œ ìœ íš¨ì„± ê²€ì¦"""
        logger.info("3. í•„ë“œ ìœ íš¨ì„± ê²€ì¦ ì¤‘...")
        
        test_result = {
            "status": "success",
            "field_validation": {},
            "validation_issues": []
        }
        
        for format_name, dataset in self.test_datasets.items():
            logger.info(f"  {format_name} í¬ë§· í•„ë“œ ê²€ì¦ ì¤‘...")
            
            field_result = {
                "empty_fields": 0,
                "invalid_types": 0,
                "encoding_issues": 0,
                "length_issues": 0,
                "content_quality": {}
            }
            
            for i, item in enumerate(dataset):
                try:
                    if format_name == "sharegpt":
                        for conv in item["conversations"]:
                            # ë¹ˆ í•„ë“œ ê²€ì‚¬
                            if not conv["value"].strip():
                                field_result["empty_fields"] += 1
                            
                            # ê¸¸ì´ ê²€ì‚¬ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸´ ë‚´ìš©)
                            if len(conv["value"]) < 10:
                                field_result["length_issues"] += 1
                            elif len(conv["value"]) > 4000:  # í† í° ì œí•œ ê³ ë ¤
                                field_result["length_issues"] += 1
                            
                            # ì¸ì½”ë”© ê²€ì‚¬
                            try:
                                conv["value"].encode('utf-8')
                            except UnicodeEncodeError:
                                field_result["encoding_issues"] += 1
                    
                    elif format_name == "alpaca":
                        # í•„ìˆ˜ í•„ë“œ ê²€ì‚¬
                        for field in ["instruction", "output"]:
                            if not item[field].strip():
                                field_result["empty_fields"] += 1
                            
                            # ê¸¸ì´ ê²€ì‚¬
                            if len(item[field]) < 5:
                                field_result["length_issues"] += 1
                            elif len(item[field]) > 4000:
                                field_result["length_issues"] += 1
                            
                            # ì¸ì½”ë”© ê²€ì‚¬
                            try:
                                item[field].encode('utf-8')
                            except UnicodeEncodeError:
                                field_result["encoding_issues"] += 1
                    
                    elif format_name == "openai":
                        for msg in item["messages"]:
                            # ë¹ˆ í•„ë“œ ê²€ì‚¬
                            if not msg["content"].strip():
                                field_result["empty_fields"] += 1
                            
                            # ê¸¸ì´ ê²€ì‚¬
                            if len(msg["content"]) < 5:
                                field_result["length_issues"] += 1
                            elif len(msg["content"]) > 4000:
                                field_result["length_issues"] += 1
                            
                            # ì¸ì½”ë”© ê²€ì‚¬
                            try:
                                msg["content"].encode('utf-8')
                            except UnicodeEncodeError:
                                field_result["encoding_issues"] += 1
                
                except Exception as e:
                    logger.warning(f"    í•„ë“œ ê²€ì¦ ì˜¤ë¥˜ (Item {i}): {e}")
                    field_result["invalid_types"] += 1
            
            # ì½˜í…ì¸  í’ˆì§ˆ ë¶„ì„
            total_items = len(dataset)
            field_result["content_quality"] = {
                "empty_field_ratio": field_result["empty_fields"] / total_items if total_items > 0 else 0,
                "length_issue_ratio": field_result["length_issues"] / total_items if total_items > 0 else 0,
                "encoding_issue_ratio": field_result["encoding_issues"] / total_items if total_items > 0 else 0
            }
            
            test_result["field_validation"][format_name] = field_result
            
            # ì´ìŠˆ ìš”ì•½
            issues = []
            if field_result["empty_fields"] > 0:
                issues.append(f"{field_result['empty_fields']}ê°œ ë¹ˆ í•„ë“œ")
            if field_result["length_issues"] > 0:
                issues.append(f"{field_result['length_issues']}ê°œ ê¸¸ì´ ì´ìŠˆ")
            if field_result["encoding_issues"] > 0:
                issues.append(f"{field_result['encoding_issues']}ê°œ ì¸ì½”ë”© ì´ìŠˆ")
            
            if issues:
                logger.warning(f"    âš  {format_name}: {', '.join(issues)}")
                test_result["validation_issues"].extend([f"{format_name}: {issue}" for issue in issues])
            else:
                logger.info(f"    âœ“ {format_name}: ëª¨ë“  í•„ë“œ ìœ íš¨")
        
        if test_result["validation_issues"]:
            test_result["status"] = "warning"
        
        self.test_results["field_validation"] = test_result
        logger.info("âœ“ í•„ë“œ ìœ íš¨ì„± ê²€ì¦ ì™„ë£Œ")
    
    async def _test_format_compatibility(self):
        """í¬ë§·ë³„ í˜¸í™˜ì„± ê²€ì¦"""
        logger.info("4. í¬ë§·ë³„ í˜¸í™˜ì„± ê²€ì¦ ì¤‘...")
        
        test_result = {
            "status": "success",
            "format_compatibility": {},
            "compatibility_issues": []
        }
        
        # Unslothì—ì„œ ì§€ì›í•˜ëŠ” í¬ë§· ìŠ¤í™
        unsloth_specs = {
            "sharegpt": {
                "required_fields": ["conversations"],
                "conversation_fields": ["from", "value"],
                "valid_roles": ["human", "gpt", "system"],
                "max_turns": 50,
                "max_length": 4096
            },
            "alpaca": {
                "required_fields": ["instruction", "input", "output"],
                "optional_fields": ["text"],
                "max_length": 4096
            },
            "openai": {
                "required_fields": ["messages"],
                "message_fields": ["role", "content"],
                "valid_roles": ["user", "assistant", "system"],
                "max_messages": 50,
                "max_length": 4096
            }
        }
        
        for format_name, dataset in self.test_datasets.items():
            logger.info(f"  {format_name} í¬ë§· í˜¸í™˜ì„± ê²€ì¦ ì¤‘...")
            
            if format_name not in unsloth_specs:
                logger.warning(f"    âš  {format_name}: Unsloth ìŠ¤í™ ì •ë³´ ì—†ìŒ")
                continue
            
            spec = unsloth_specs[format_name]
            format_result = {
                "compliant_items": 0,
                "non_compliant_items": 0,
                "compliance_issues": []
            }
            
            for i, item in enumerate(dataset):
                compliance_issues = []
                
                try:
                    if format_name == "sharegpt":
                        # í•„ìˆ˜ í•„ë“œ í™•ì¸
                        for field in spec["required_fields"]:
                            if field not in item:
                                compliance_issues.append(f"Missing required field: {field}")
                        
                        if "conversations" in item:
                            conversations = item["conversations"]
                            
                            # ëŒ€í™” ìˆ˜ ì œí•œ í™•ì¸
                            if len(conversations) > spec["max_turns"]:
                                compliance_issues.append(f"Too many turns: {len(conversations)} > {spec['max_turns']}")
                            
                            # ê° ëŒ€í™” ê²€ì¦
                            for j, conv in enumerate(conversations):
                                for field in spec["conversation_fields"]:
                                    if field not in conv:
                                        compliance_issues.append(f"Conversation {j} missing field: {field}")
                                
                                if "from" in conv and conv["from"] not in spec["valid_roles"]:
                                    compliance_issues.append(f"Invalid role: {conv['from']}")
                                
                                if "value" in conv and len(conv["value"]) > spec["max_length"]:
                                    compliance_issues.append(f"Content too long: {len(conv['value'])} > {spec['max_length']}")
                    
                    elif format_name == "alpaca":
                        # í•„ìˆ˜ í•„ë“œ í™•ì¸
                        for field in spec["required_fields"]:
                            if field not in item:
                                compliance_issues.append(f"Missing required field: {field}")
                        
                        # ê¸¸ì´ ì œí•œ í™•ì¸
                        for field in ["instruction", "output"]:
                            if field in item and len(item[field]) > spec["max_length"]:
                                compliance_issues.append(f"{field} too long: {len(item[field])} > {spec['max_length']}")
                    
                    elif format_name == "openai":
                        # í•„ìˆ˜ í•„ë“œ í™•ì¸
                        for field in spec["required_fields"]:
                            if field not in item:
                                compliance_issues.append(f"Missing required field: {field}")
                        
                        if "messages" in item:
                            messages = item["messages"]
                            
                            # ë©”ì‹œì§€ ìˆ˜ ì œí•œ í™•ì¸
                            if len(messages) > spec["max_messages"]:
                                compliance_issues.append(f"Too many messages: {len(messages)} > {spec['max_messages']}")
                            
                            # ê° ë©”ì‹œì§€ ê²€ì¦
                            for j, msg in enumerate(messages):
                                for field in spec["message_fields"]:
                                    if field not in msg:
                                        compliance_issues.append(f"Message {j} missing field: {field}")
                                
                                if "role" in msg and msg["role"] not in spec["valid_roles"]:
                                    compliance_issues.append(f"Invalid role: {msg['role']}")
                                
                                if "content" in msg and len(msg["content"]) > spec["max_length"]:
                                    compliance_issues.append(f"Content too long: {len(msg['content'])} > {spec['max_length']}")
                    
                    if compliance_issues:
                        format_result["non_compliant_items"] += 1
                        format_result["compliance_issues"].extend([f"Item {i}: {issue}" for issue in compliance_issues])
                    else:
                        format_result["compliant_items"] += 1
                
                except Exception as e:
                    format_result["non_compliant_items"] += 1
                    format_result["compliance_issues"].append(f"Item {i}: Validation error - {str(e)}")
            
            test_result["format_compatibility"][format_name] = format_result
            
            total_items = len(dataset)
            compliance_rate = format_result["compliant_items"] / total_items if total_items > 0 else 0
            
            if compliance_rate == 1.0:
                logger.info(f"    âœ“ {format_name}: 100% í˜¸í™˜ì„±")
            elif compliance_rate >= 0.9:
                logger.info(f"    âœ“ {format_name}: {compliance_rate:.1%} í˜¸í™˜ì„± (ì–‘í˜¸)")
            else:
                logger.warning(f"    âš  {format_name}: {compliance_rate:.1%} í˜¸í™˜ì„± (ê°œì„  í•„ìš”)")
                test_result["compatibility_issues"].extend(format_result["compliance_issues"])
        
        if test_result["compatibility_issues"]:
            test_result["status"] = "warning"
        
        self.test_results["format_compatibility"] = test_result
        logger.info("âœ“ í¬ë§·ë³„ í˜¸í™˜ì„± ê²€ì¦ ì™„ë£Œ")    asy
nc def _test_data_quality(self):
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        logger.info("5. ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì¤‘...")
        
        test_result = {
            "status": "success",
            "quality_metrics": {},
            "quality_issues": []
        }
        
        for format_name, dataset in self.test_datasets.items():
            logger.info(f"  {format_name} í¬ë§· í’ˆì§ˆ ê²€ì¦ ì¤‘...")
            
            quality_metrics = {
                "avg_content_length": 0,
                "content_diversity": 0,
                "technical_accuracy": 0,
                "language_quality": 0,
                "code_examples": 0,
                "total_items": len(dataset)
            }
            
            content_lengths = []
            unique_contents = set()
            code_example_count = 0
            technical_terms = ["Syncfusion", "GridControl", "ChartControl", "TreeViewAdv", "WinForms", "C#"]
            
            for item in dataset:
                try:
                    # ì½˜í…ì¸  ì¶”ì¶œ
                    contents = []
                    if format_name == "sharegpt":
                        contents = [conv["value"] for conv in item["conversations"]]
                    elif format_name == "alpaca":
                        contents = [item["instruction"], item["output"]]
                    elif format_name == "openai":
                        contents = [msg["content"] for msg in item["messages"]]
                    
                    # ê¸¸ì´ ë¶„ì„
                    for content in contents:
                        content_lengths.append(len(content))
                        unique_contents.add(content.lower().strip())
                        
                        # ì½”ë“œ ì˜ˆì œ ê²€ì‚¬
                        if "```" in content or "code" in content.lower():
                            code_example_count += 1
                        
                        # ê¸°ìˆ ì  ì •í™•ì„± ê²€ì‚¬ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)
                        technical_score = sum(1 for term in technical_terms if term.lower() in content.lower())
                        quality_metrics["technical_accuracy"] += technical_score
                
                except Exception as e:
                    logger.warning(f"    í’ˆì§ˆ ë¶„ì„ ì˜¤ë¥˜: {e}")
            
            # í’ˆì§ˆ ë©”íŠ¸ë¦­ ê³„ì‚°
            if content_lengths:
                quality_metrics["avg_content_length"] = sum(content_lengths) / len(content_lengths)
            
            total_contents = len(content_lengths)
            if total_contents > 0:
                quality_metrics["content_diversity"] = len(unique_contents) / total_contents
                quality_metrics["code_examples"] = code_example_count / total_contents
                quality_metrics["technical_accuracy"] = quality_metrics["technical_accuracy"] / total_contents
            
            # ì–¸ì–´ í’ˆì§ˆ í‰ê°€ (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
            language_quality_score = 0
            for content in unique_contents:
                # ë¬¸ì¥ êµ¬ì¡° í™•ì¸
                if content.count('.') > 0 or content.count('!') > 0 or content.count('?') > 0:
                    language_quality_score += 1
                
                # ì ì ˆí•œ ê¸¸ì´ í™•ì¸
                if 20 <= len(content) <= 2000:
                    language_quality_score += 1
            
            if len(unique_contents) > 0:
                quality_metrics["language_quality"] = language_quality_score / (len(unique_contents) * 2)
            
            test_result["quality_metrics"][format_name] = quality_metrics
            
            # í’ˆì§ˆ ì´ìŠˆ í™•ì¸
            issues = []
            if quality_metrics["avg_content_length"] < 50:
                issues.append("í‰ê·  ì½˜í…ì¸  ê¸¸ì´ê°€ ë„ˆë¬´ ì§§ìŒ")
            elif quality_metrics["avg_content_length"] > 3000:
                issues.append("í‰ê·  ì½˜í…ì¸  ê¸¸ì´ê°€ ë„ˆë¬´ ê¹€")
            
            if quality_metrics["content_diversity"] < 0.8:
                issues.append(f"ì½˜í…ì¸  ë‹¤ì–‘ì„± ë¶€ì¡± ({quality_metrics['content_diversity']:.1%})")
            
            if quality_metrics["technical_accuracy"] < 1.0:
                issues.append("ê¸°ìˆ ì  ì •í™•ì„± ë¶€ì¡±")
            
            if quality_metrics["code_examples"] < 0.3:
                issues.append("ì½”ë“œ ì˜ˆì œ ë¶€ì¡±")
            
            if issues:
                logger.warning(f"    âš  {format_name}: {', '.join(issues)}")
                test_result["quality_issues"].extend([f"{format_name}: {issue}" for issue in issues])
            else:
                logger.info(f"    âœ“ {format_name}: í’ˆì§ˆ ê¸°ì¤€ ë§Œì¡±")
            
            logger.info(f"      í‰ê·  ê¸¸ì´: {quality_metrics['avg_content_length']:.0f}ì")
            logger.info(f"      ë‹¤ì–‘ì„±: {quality_metrics['content_diversity']:.1%}")
            logger.info(f"      ê¸°ìˆ  ì •í™•ì„±: {quality_metrics['technical_accuracy']:.1f}")
            logger.info(f"      ì½”ë“œ ì˜ˆì œ: {quality_metrics['code_examples']:.1%}")
        
        if test_result["quality_issues"]:
            test_result["status"] = "warning"
        
        self.test_results["data_quality"] = test_result
        logger.info("âœ“ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ")
    
    async def _test_tokenization_compatibility(self):
        """í† í°í™” í˜¸í™˜ì„± ê²€ì¦"""
        logger.info("6. í† í°í™” í˜¸í™˜ì„± ê²€ì¦ ì¤‘...")
        
        test_result = {
            "status": "success",
            "tokenization_results": {},
            "tokenization_issues": []
        }
        
        # ê°„ë‹¨í•œ í† í° ì¶”ì • í•¨ìˆ˜ (ì‹¤ì œ í† í¬ë‚˜ì´ì € ì—†ì´)
        def estimate_tokens(text: str) -> int:
            # ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì • (ì˜ì–´: 4ìë‹¹ 1í† í°, í•œêµ­ì–´: 2ìë‹¹ 1í† í°)
            english_chars = sum(1 for c in text if ord(c) < 128)
            korean_chars = len(text) - english_chars
            return (english_chars // 4) + (korean_chars // 2)
        
        for format_name, dataset in self.test_datasets.items():
            logger.info(f"  {format_name} í¬ë§· í† í°í™” ê²€ì¦ ì¤‘...")
            
            tokenization_result = {
                "total_items": len(dataset),
                "avg_tokens": 0,
                "max_tokens": 0,
                "min_tokens": float('inf'),
                "over_limit_items": 0,
                "token_distribution": {}
            }
            
            token_counts = []
            token_limit = 4096  # ì¼ë°˜ì ì¸ í† í° ì œí•œ
            
            for i, item in enumerate(dataset):
                try:
                    item_tokens = 0
                    
                    if format_name == "sharegpt":
                        for conv in item["conversations"]:
                            item_tokens += estimate_tokens(conv["value"])
                    elif format_name == "alpaca":
                        item_tokens += estimate_tokens(item["instruction"])
                        item_tokens += estimate_tokens(item["input"])
                        item_tokens += estimate_tokens(item["output"])
                    elif format_name == "openai":
                        for msg in item["messages"]:
                            item_tokens += estimate_tokens(msg["content"])
                    
                    token_counts.append(item_tokens)
                    
                    if item_tokens > token_limit:
                        tokenization_result["over_limit_items"] += 1
                        logger.debug(f"    Item {i}: {item_tokens} í† í° (ì œí•œ ì´ˆê³¼)")
                
                except Exception as e:
                    logger.warning(f"    í† í°í™” ì˜¤ë¥˜ (Item {i}): {e}")
            
            if token_counts:
                tokenization_result["avg_tokens"] = sum(token_counts) / len(token_counts)
                tokenization_result["max_tokens"] = max(token_counts)
                tokenization_result["min_tokens"] = min(token_counts)
                
                # í† í° ë¶„í¬ ë¶„ì„
                ranges = [(0, 512), (512, 1024), (1024, 2048), (2048, 4096), (4096, float('inf'))]
                for range_start, range_end in ranges:
                    count = sum(1 for tokens in token_counts if range_start <= tokens < range_end)
                    range_name = f"{range_start}-{range_end if range_end != float('inf') else 'âˆ'}"
                    tokenization_result["token_distribution"][range_name] = count
            
            test_result["tokenization_results"][format_name] = tokenization_result
            
            # í† í°í™” ì´ìŠˆ í™•ì¸
            issues = []
            if tokenization_result["over_limit_items"] > 0:
                issues.append(f"{tokenization_result['over_limit_items']}ê°œ í•­ëª©ì´ í† í° ì œí•œ ì´ˆê³¼")
            
            if tokenization_result["avg_tokens"] > token_limit * 0.8:
                issues.append("í‰ê·  í† í° ìˆ˜ê°€ ì œí•œì— ê·¼ì ‘")
            
            if issues:
                logger.warning(f"    âš  {format_name}: {', '.join(issues)}")
                test_result["tokenization_issues"].extend([f"{format_name}: {issue}" for issue in issues])
            else:
                logger.info(f"    âœ“ {format_name}: í† í°í™” í˜¸í™˜ì„± ì–‘í˜¸")
            
            logger.info(f"      í‰ê·  í† í°: {tokenization_result['avg_tokens']:.0f}")
            logger.info(f"      ìµœëŒ€ í† í°: {tokenization_result['max_tokens']}")
            logger.info(f"      ì œí•œ ì´ˆê³¼: {tokenization_result['over_limit_items']}ê°œ")
        
        if test_result["tokenization_issues"]:
            test_result["status"] = "warning"
        
        self.test_results["tokenization"] = test_result
        logger.info("âœ“ í† í°í™” í˜¸í™˜ì„± ê²€ì¦ ì™„ë£Œ")
    
    async def _test_unsloth_loading(self):
        """Unsloth ë¡œë”© í…ŒìŠ¤íŠ¸ (ì‹œë®¬ë ˆì´ì…˜)"""
        logger.info("7. Unsloth ë¡œë”© í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        test_result = {
            "status": "success",
            "loading_results": {},
            "loading_issues": []
        }
        
        # ì„ì‹œ íŒŒì¼ì— ë°ì´í„°ì…‹ ì €ì¥í•˜ê³  ë¡œë”© ì‹œë®¬ë ˆì´ì…˜
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            
            for format_name, dataset in self.test_datasets.items():
                logger.info(f"  {format_name} í¬ë§· ë¡œë”© í…ŒìŠ¤íŠ¸ ì¤‘...")
                
                loading_result = {
                    "file_created": False,
                    "file_readable": False,
                    "json_valid": False,
                    "structure_valid": False,
                    "loadable": False
                }
                
                try:
                    # JSONL íŒŒì¼ ìƒì„±
                    file_path = temp_path / f"test_{format_name}.jsonl"
                    
                    with open(file_path, 'w', encoding='utf-8') as f:
                        for item in dataset:
                            json.dump(item, f, ensure_ascii=False)
                            f.write('\n')
                    
                    loading_result["file_created"] = True
                    logger.debug(f"    íŒŒì¼ ìƒì„± ì™„ë£Œ: {file_path}")
                    
                    # íŒŒì¼ ì½ê¸° í…ŒìŠ¤íŠ¸
                    with open(file_path, 'r', encoding='utf-8') as f:
                        lines = f.readlines()
                    
                    loading_result["file_readable"] = True
                    logger.debug(f"    íŒŒì¼ ì½ê¸° ì™„ë£Œ: {len(lines)}ì¤„")
                    
                    # JSON ìœ íš¨ì„± í…ŒìŠ¤íŠ¸
                    loaded_items = []
                    for line_num, line in enumerate(lines):
                        try:
                            item = json.loads(line.strip())
                            loaded_items.append(item)
                        except json.JSONDecodeError as e:
                            logger.warning(f"    JSON íŒŒì‹± ì˜¤ë¥˜ (ì¤„ {line_num + 1}): {e}")
                            raise
                    
                    loading_result["json_valid"] = True
                    logger.debug(f"    JSON íŒŒì‹± ì™„ë£Œ: {len(loaded_items)}ê°œ í•­ëª©")
                    
                    # êµ¬ì¡° ìœ íš¨ì„± ì¬ê²€ì¦
                    structure_valid = True
                    for i, item in enumerate(loaded_items):
                        try:
                            if format_name == "sharegpt":
                                if "conversations" not in item:
                                    raise ValueError("Missing conversations field")
                                for conv in item["conversations"]:
                                    if "from" not in conv or "value" not in conv:
                                        raise ValueError("Invalid conversation structure")
                            elif format_name == "alpaca":
                                required_fields = ["instruction", "input", "output"]
                                for field in required_fields:
                                    if field not in item:
                                        raise ValueError(f"Missing field: {field}")
                            elif format_name == "openai":
                                if "messages" not in item:
                                    raise ValueError("Missing messages field")
                                for msg in item["messages"]:
                                    if "role" not in msg or "content" not in msg:
                                        raise ValueError("Invalid message structure")
                        except Exception as e:
                            logger.warning(f"    êµ¬ì¡° ê²€ì¦ ì˜¤ë¥˜ (Item {i}): {e}")
                            structure_valid = False
                            break
                    
                    loading_result["structure_valid"] = structure_valid
                    
                    # ì „ì²´ ë¡œë”© ê°€ëŠ¥ì„± íŒë‹¨
                    loading_result["loadable"] = all([
                        loading_result["file_created"],
                        loading_result["file_readable"],
                        loading_result["json_valid"],
                        loading_result["structure_valid"]
                    ])
                    
                    if loading_result["loadable"]:
                        logger.info(f"    âœ“ {format_name}: Unsloth ë¡œë”© ê°€ëŠ¥")
                    else:
                        logger.warning(f"    âš  {format_name}: Unsloth ë¡œë”© ë¶ˆê°€")
                        test_result["loading_issues"].append(f"{format_name}: ë¡œë”© ì‹¤íŒ¨")
                
                except Exception as e:
                    logger.error(f"    âœ— {format_name}: ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ - {e}")
                    loading_result["loadable"] = False
                    test_result["loading_issues"].append(f"{format_name}: {str(e)}")
                
                test_result["loading_results"][format_name] = loading_result
        
        if test_result["loading_issues"]:
            test_result["status"] = "warning"
        
        self.test_results["unsloth_loading"] = test_result
        logger.info("âœ“ Unsloth ë¡œë”© í…ŒìŠ¤íŠ¸ ì™„ë£Œ")    asyn
c def _generate_compatibility_report(self) -> Dict[str, Any]:
        """í˜¸í™˜ì„± ë¦¬í¬íŠ¸ ìƒì„±"""
        logger.info("8. í˜¸í™˜ì„± ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
        
        # ì „ì²´ í˜¸í™˜ì„± ì ìˆ˜ ê³„ì‚°
        compatibility_scores = {}
        
        # ë°ì´í„° êµ¬ì¡° ì ìˆ˜
        if "data_structure" in self.test_results:
            structure_data = self.test_results["data_structure"]["format_results"]
            structure_scores = []
            for format_name, result in structure_data.items():
                total = result["valid_items"] + result["invalid_items"]
                score = (result["valid_items"] / total * 100) if total > 0 else 0
                structure_scores.append(score)
            compatibility_scores["data_structure"] = sum(structure_scores) / len(structure_scores) if structure_scores else 0
        
        # í•„ë“œ ìœ íš¨ì„± ì ìˆ˜
        if "field_validation" in self.test_results:
            field_data = self.test_results["field_validation"]["field_validation"]
            field_scores = []
            for format_name, result in field_data.items():
                quality = result["content_quality"]
                score = 100 * (1 - quality["empty_field_ratio"] - quality["length_issue_ratio"] - quality["encoding_issue_ratio"])
                field_scores.append(max(0, score))
            compatibility_scores["field_validation"] = sum(field_scores) / len(field_scores) if field_scores else 0
        
        # í¬ë§· í˜¸í™˜ì„± ì ìˆ˜
        if "format_compatibility" in self.test_results:
            format_data = self.test_results["format_compatibility"]["format_compatibility"]
            format_scores = []
            for format_name, result in format_data.items():
                total = result["compliant_items"] + result["non_compliant_items"]
                score = (result["compliant_items"] / total * 100) if total > 0 else 0
                format_scores.append(score)
            compatibility_scores["format_compatibility"] = sum(format_scores) / len(format_scores) if format_scores else 0
        
        # ë°ì´í„° í’ˆì§ˆ ì ìˆ˜
        if "data_quality" in self.test_results:
            quality_data = self.test_results["data_quality"]["quality_metrics"]
            quality_scores = []
            for format_name, metrics in quality_data.items():
                score = (
                    min(100, metrics["content_diversity"] * 100) * 0.3 +
                    min(100, metrics["technical_accuracy"] * 20) * 0.3 +
                    min(100, metrics["language_quality"] * 100) * 0.2 +
                    min(100, metrics["code_examples"] * 100) * 0.2
                )
                quality_scores.append(score)
            compatibility_scores["data_quality"] = sum(quality_scores) / len(quality_scores) if quality_scores else 0
        
        # í† í°í™” í˜¸í™˜ì„± ì ìˆ˜
        if "tokenization" in self.test_results:
            token_data = self.test_results["tokenization"]["tokenization_results"]
            token_scores = []
            for format_name, result in token_data.items():
                total = result["total_items"]
                over_limit = result["over_limit_items"]
                score = ((total - over_limit) / total * 100) if total > 0 else 0
                token_scores.append(score)
            compatibility_scores["tokenization"] = sum(token_scores) / len(token_scores) if token_scores else 0
        
        # Unsloth ë¡œë”© ì ìˆ˜
        if "unsloth_loading" in self.test_results:
            loading_data = self.test_results["unsloth_loading"]["loading_results"]
            loading_scores = []
            for format_name, result in loading_data.items():
                score = 100 if result["loadable"] else 0
                loading_scores.append(score)
            compatibility_scores["unsloth_loading"] = sum(loading_scores) / len(loading_scores) if loading_scores else 0
        
        # ì „ì²´ í˜¸í™˜ì„± ì ìˆ˜
        overall_score = sum(compatibility_scores.values()) / len(compatibility_scores) if compatibility_scores else 0
        
        # í˜¸í™˜ì„± ë“±ê¸‰ ê²°ì •
        if overall_score >= 95:
            compatibility_grade = "A+"
        elif overall_score >= 90:
            compatibility_grade = "A"
        elif overall_score >= 85:
            compatibility_grade = "B+"
        elif overall_score >= 80:
            compatibility_grade = "B"
        elif overall_score >= 75:
            compatibility_grade = "C+"
        elif overall_score >= 70:
            compatibility_grade = "C"
        elif overall_score >= 60:
            compatibility_grade = "D"
        else:
            compatibility_grade = "F"
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        recommendations = []
        
        if compatibility_scores.get("data_structure", 0) < 90:
            recommendations.append("ë°ì´í„° êµ¬ì¡° ê°œì„  í•„ìš” - í•„ìˆ˜ í•„ë“œ ë° íƒ€ì… ê²€ì¦ ê°•í™”")
        
        if compatibility_scores.get("field_validation", 0) < 90:
            recommendations.append("í•„ë“œ ìœ íš¨ì„± ê°œì„  í•„ìš” - ë¹ˆ í•„ë“œ ë° ì¸ì½”ë”© ì´ìŠˆ í•´ê²°")
        
        if compatibility_scores.get("format_compatibility", 0) < 90:
            recommendations.append("í¬ë§· í˜¸í™˜ì„± ê°œì„  í•„ìš” - Unsloth ìŠ¤í™ ì¤€ìˆ˜ ê°•í™”")
        
        if compatibility_scores.get("data_quality", 0) < 80:
            recommendations.append("ë°ì´í„° í’ˆì§ˆ ê°œì„  í•„ìš” - ì½˜í…ì¸  ë‹¤ì–‘ì„± ë° ê¸°ìˆ ì  ì •í™•ì„± í–¥ìƒ")
        
        if compatibility_scores.get("tokenization", 0) < 90:
            recommendations.append("í† í°í™” ìµœì í™” í•„ìš” - ì½˜í…ì¸  ê¸¸ì´ ì¡°ì •")
        
        if compatibility_scores.get("unsloth_loading", 0) < 100:
            recommendations.append("Unsloth ë¡œë”© ì´ìŠˆ í•´ê²° í•„ìš” - íŒŒì¼ í¬ë§· ë° êµ¬ì¡° ê²€í† ")
        
        if not recommendations:
            recommendations.append("ëª¨ë“  í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ í†µê³¼ - Unslothì—ì„œ ì‚¬ìš© ì¤€ë¹„ ì™„ë£Œ")
        
        # ìµœì¢… ë¦¬í¬íŠ¸ êµ¬ì„±
        compatibility_report = {
            "test_info": {
                "timestamp": datetime.now().isoformat(),
                "test_type": "unsloth_compatibility",
                "formats_tested": list(self.test_datasets.keys()),
                "total_samples": sum(len(dataset) for dataset in self.test_datasets.values())
            },
            "compatibility_scores": compatibility_scores,
            "overall_score": overall_score,
            "compatibility_grade": compatibility_grade,
            "detailed_results": self.test_results,
            "recommendations": recommendations,
            "summary": {
                "data_structure_valid": compatibility_scores.get("data_structure", 0) >= 90,
                "fields_valid": compatibility_scores.get("field_validation", 0) >= 90,
                "format_compliant": compatibility_scores.get("format_compatibility", 0) >= 90,
                "quality_acceptable": compatibility_scores.get("data_quality", 0) >= 80,
                "tokenization_compatible": compatibility_scores.get("tokenization", 0) >= 90,
                "unsloth_loadable": compatibility_scores.get("unsloth_loading", 0) >= 100,
                "ready_for_training": overall_score >= 85
            }
        }
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"unsloth_compatibility_report_{timestamp}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(compatibility_report, f, indent=2, ensure_ascii=False)
        
        # ê²°ê³¼ ì¶œë ¥
        logger.info("=" * 60)
        logger.info("Unsloth í˜¸í™˜ì„± ê²€ì¦ ê²°ê³¼")
        logger.info("=" * 60)
        logger.info(f"ì „ì²´ í˜¸í™˜ì„± ì ìˆ˜: {overall_score:.1f}/100 (ë“±ê¸‰: {compatibility_grade})")
        logger.info(f"ë°ì´í„° êµ¬ì¡°: {compatibility_scores.get('data_structure', 0):.1f}/100")
        logger.info(f"í•„ë“œ ìœ íš¨ì„±: {compatibility_scores.get('field_validation', 0):.1f}/100")
        logger.info(f"í¬ë§· í˜¸í™˜ì„±: {compatibility_scores.get('format_compatibility', 0):.1f}/100")
        logger.info(f"ë°ì´í„° í’ˆì§ˆ: {compatibility_scores.get('data_quality', 0):.1f}/100")
        logger.info(f"í† í°í™” í˜¸í™˜ì„±: {compatibility_scores.get('tokenization', 0):.1f}/100")
        logger.info(f"Unsloth ë¡œë”©: {compatibility_scores.get('unsloth_loading', 0):.1f}/100")
        logger.info("")
        
        summary = compatibility_report["summary"]
        logger.info("í˜¸í™˜ì„± ìš”ì•½:")
        logger.info(f"  ë°ì´í„° êµ¬ì¡° ìœ íš¨: {'âœ“' if summary['data_structure_valid'] else 'âœ—'}")
        logger.info(f"  í•„ë“œ ìœ íš¨ì„±: {'âœ“' if summary['fields_valid'] else 'âœ—'}")
        logger.info(f"  í¬ë§· ì¤€ìˆ˜: {'âœ“' if summary['format_compliant'] else 'âœ—'}")
        logger.info(f"  í’ˆì§ˆ ìˆ˜ì¤€: {'âœ“' if summary['quality_acceptable'] else 'âœ—'}")
        logger.info(f"  í† í°í™” í˜¸í™˜: {'âœ“' if summary['tokenization_compatible'] else 'âœ—'}")
        logger.info(f"  Unsloth ë¡œë”©: {'âœ“' if summary['unsloth_loadable'] else 'âœ—'}")
        logger.info(f"  í›ˆë ¨ ì¤€ë¹„: {'âœ“' if summary['ready_for_training'] else 'âœ—'}")
        logger.info("")
        
        logger.info("ê¶Œì¥ì‚¬í•­:")
        for i, rec in enumerate(recommendations, 1):
            logger.info(f"  {i}. {rec}")
        
        logger.info(f"\nìƒì„¸ ë¦¬í¬íŠ¸: {report_file}")
        logger.info("=" * 60)
        
        return compatibility_report


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("Unsloth í˜¸í™˜ì„± ê²€ì¦ í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    print("ì´ í…ŒìŠ¤íŠ¸ëŠ” ìƒì„±ëœ ë°ì´í„°ì…‹ì´ Unsloth í”„ë ˆì„ì›Œí¬ì™€ í˜¸í™˜ë˜ëŠ”ì§€ ê²€ì¦í•©ë‹ˆë‹¤.\n")
    
    tester = UnslothCompatibilityTest()
    
    try:
        compatibility_report = await tester.run_compatibility_tests()
        
        overall_score = compatibility_report.get("overall_score", 0)
        grade = compatibility_report.get("compatibility_grade", "F")
        ready_for_training = compatibility_report.get("summary", {}).get("ready_for_training", False)
        
        if ready_for_training:
            print(f"\nğŸ‰ Unsloth í˜¸í™˜ì„± ê²€ì¦ ì™„ë£Œ! ì ìˆ˜: {overall_score:.1f}/100 (ë“±ê¸‰: {grade})")
            print("ë°ì´í„°ì…‹ì´ Unsloth í›ˆë ¨ì— ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!")
        elif overall_score >= 70:
            print(f"\nâœ… Unsloth í˜¸í™˜ì„± ê²€ì¦ ì™„ë£Œ! ì ìˆ˜: {overall_score:.1f}/100 (ë“±ê¸‰: {grade})")
            print("ë°ì´í„°ì…‹ì´ ê¸°ë³¸ì ì¸ í˜¸í™˜ì„±ì„ ë§Œì¡±í•˜ì§€ë§Œ ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            print(f"\nâš ï¸ Unsloth í˜¸í™˜ì„± ê²€ì¦ ì™„ë£Œ! ì ìˆ˜: {overall_score:.1f}/100 (ë“±ê¸‰: {grade})")
            print("ë°ì´í„°ì…‹ í˜¸í™˜ì„± ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ì£¼ìš” í˜¸í™˜ì„± ì§€í‘œ ì¶œë ¥
        summary = compatibility_report.get("summary", {})
        print(f"\nğŸ“‹ í˜¸í™˜ì„± ì²´í¬ë¦¬ìŠ¤íŠ¸:")
        print(f"  âœ“ ë°ì´í„° êµ¬ì¡°: {'í†µê³¼' if summary.get('data_structure_valid', False) else 'ì‹¤íŒ¨'}")
        print(f"  âœ“ í•„ë“œ ìœ íš¨ì„±: {'í†µê³¼' if summary.get('fields_valid', False) else 'ì‹¤íŒ¨'}")
        print(f"  âœ“ í¬ë§· ì¤€ìˆ˜: {'í†µê³¼' if summary.get('format_compliant', False) else 'ì‹¤íŒ¨'}")
        print(f"  âœ“ í’ˆì§ˆ ìˆ˜ì¤€: {'í†µê³¼' if summary.get('quality_acceptable', False) else 'ì‹¤íŒ¨'}")
        print(f"  âœ“ í† í°í™” í˜¸í™˜: {'í†µê³¼' if summary.get('tokenization_compatible', False) else 'ì‹¤íŒ¨'}")
        print(f"  âœ“ Unsloth ë¡œë”©: {'í†µê³¼' if summary.get('unsloth_loadable', False) else 'ì‹¤íŒ¨'}")
        
    except Exception as e:
        logger.error(f"í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"\nâŒ í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    print("\n=== Unsloth í˜¸í™˜ì„± ê²€ì¦ ì™„ë£Œ ===")


if __name__ == "__main__":
    asyncio.run(main())