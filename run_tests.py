#!/usr/bin/env python3
"""
í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì „ì²´ Unsloth ë°ì´í„°ì…‹ ìƒì„± íŒŒì´í”„ë¼ì¸ì˜ í†µí•© í…ŒìŠ¤íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
ê° ëª¨ë“ˆë³„ í…ŒìŠ¤íŠ¸, ì„±ëŠ¥ í…ŒìŠ¤íŠ¸, í’ˆì§ˆ í…ŒìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ì™„ì „í•œ í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

import asyncio
import argparse
import json
import logging
import os
import sys
import time
import tempfile
import shutil
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import unittest
from unittest.mock import Mock, patch, AsyncMock

import yaml
import pytest
from tqdm import tqdm

# í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ëª¨ë“ˆ ì„í¬íŠ¸
from md_processor import create_processor, create_scanner
from md_processor.processor import ProcessingConfig
from qdrant_connector import create_integrated_processor
from openai_connector import create_openai_connector
from unsloth_dataset import create_dataset_generator, DatasetConfig
from quality_validator import create_default_validator, QualityValidatorConfig

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


@dataclass
class TestConfig:
    """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
    # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì„¤ì •
    test_mode: bool = True
    sample_size: int = 10
    verbose: bool = True
    progress_bar: bool = True
    
    # í…ŒìŠ¤íŠ¸ ë””ë ‰í† ë¦¬
    test_output_dir: str = "./test_output"
    test_temp_dir: str = "./test_temp"
    
    # í…ŒìŠ¤íŠ¸ ë²”ìœ„
    run_unit_tests: bool = True
    run_integration_tests: bool = True
    run_performance_tests: bool = True
    run_quality_tests: bool = True
    
    # í…ŒìŠ¤íŠ¸ ë‹¨ê³„
    test_steps: List[str] = field(default_factory=lambda: [
        "md_processing", "qdrant_search", "conversation_generation", 
        "dataset_generation", "quality_validation"
    ])
    
    # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì„¤ì •
    performance_thresholds: Dict[str, float] = field(default_factory=lambda: {
        "md_processing_time": 30.0,  # MD ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)
        "qdrant_search_time": 60.0,   # Qdrant ê²€ìƒ‰ ì‹œê°„ (ì´ˆ)
        "conversation_generation_time": 300.0,  # ëŒ€í™” ìƒì„± ì‹œê°„ (ì´ˆ)
        "dataset_generation_time": 180.0,  # ë°ì´í„°ì…‹ ìƒì„± ì‹œê°„ (ì´ˆ)
        "quality_validation_time": 60.0,  # í’ˆì§ˆ ê²€ì¦ ì‹œê°„ (ì´ˆ)
        "memory_usage_limit": 10240,  # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ (MB)
        "cpu_usage_limit": 80.0  # CPU ì‚¬ìš©ëŸ‰ ì œí•œ (%)
    })


class TestResult:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    def __init__(self, test_name: str):
        self.test_name = test_name
        self.start_time = time.time()
        self.end_time = None
        self.duration = 0
        self.passed = False
        self.error = None
        self.details = {}
    
    def complete(self, passed: bool = True, error: str = None, details: Dict = None):
        """í…ŒìŠ¤íŠ¸ ì™„ë£Œ"""
        self.end_time = time.time()
        self.duration = self.end_time - self.start_time
        self.passed = passed
        self.error = error
        self.details = details or {}
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            "test_name": self.test_name,
            "start_time": self.start_time,
            "end_time": self.end_time,
            "duration": self.duration,
            "passed": self.passed,
            "error": self.error,
            "details": self.details
        }


class IntegrationTestSuite:
    """í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸"""
    
    def __init__(self, config: TestConfig):
        self.config = config
        self.results: List[TestResult] = []
        self.temp_dirs: List[Path] = []
        
    def setup_test_environment(self):
        """í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì •"""
        logger.info("ğŸ”§ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì¤‘...")
        
        # í…ŒìŠ¤íŠ¸ ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        test_output = Path(self.config.test_output_dir)
        test_output.mkdir(parents=True, exist_ok=True)
        
        # í…Œì„í”„ ë””ë ‰í† ë¦¬ ìƒì„±
        test_temp = Path(self.config.test_temp_dir)
        test_temp.mkdir(parents=True, exist_ok=True)
        self.temp_dirs.append(test_temp)
        
        # í…ŒìŠ¤íŠ¸ìš© ì„¤ì • íŒŒì¼ ìƒì„±
        self._create_test_config()
        
        logger.info("âœ… í…ŒìŠ¤íŠ¸ í™˜ê²½ ì„¤ì • ì™„ë£Œ")
    
    def _create_test_config(self):
        """í…ŒìŠ¤íŠ¸ìš© ì„¤ì • íŒŒì¼ ìƒì„±"""
        test_config = {
            "project": {
                "name": "Test Dataset",
                "version": "1.0.0",
                "target_count": 100,
                "output_directory": self.config.test_output_dir
            },
            "openai_api": {
                "endpoint": "http://123.37.28.120:9997/v1",
                "model": "qwen2.5-vl-instruct",
                "max_tokens": 128000,
                "temperature": 0.3
            },
            "qdrant": {
                "host": "localhost",
                "port": 6333,
                "collection": "test_collection"
            },
            "data_sources": {
                "md_output_path": self.config.test_output_dir,
                "md_staging_path": self.config.test_temp_dir
            },
            "unsloth": {
                "max_seq_length": 8192,
                "formats": ["sharegpt", "alpaca"],
                "train_test_split": 0.8
            },
            "quality": {
                "min_quality_score": 0.5,
                "max_similarity_threshold": 0.9,
                "safety_threshold": 0.1,
                "enable_auto_correction": True
            },
            "execution": {
                "test_mode": True,
                "sample_size": self.config.sample_size,
                "verbose": self.config.verbose,
                "progress_bar": self.config.progress_bar,
                "steps": self.config.test_steps
            }
        }
        
        config_path = Path(self.config.test_output_dir) / "test_config.yaml"
        with open(config_path, 'w', encoding='utf-8') as f:
            yaml.dump(test_config, f, default_flow_style=False, allow_unicode=True)
    
    def cleanup_test_environment(self):
        """í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë¦¬"""
        logger.info("ğŸ§¹ í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë¦¬ ì¤‘...")
        
        for temp_dir in self.temp_dirs:
            if temp_dir.exists():
                shutil.rmtree(temp_dir)
        
        logger.info("âœ… í…ŒìŠ¤íŠ¸ í™˜ê²½ ì •ë¦¬ ì™„ë£Œ")
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸš€ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        try:
            self.setup_test_environment()
            
            # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            if self.config.run_unit_tests:
                await self._run_unit_tests()
            
            # í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            if self.config.run_integration_tests:
                await self._run_integration_tests()
            
            # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            if self.config.run_performance_tests:
                await self._run_performance_tests()
            
            # í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            if self.config.run_quality_tests:
                await self._run_quality_tests()
            
            # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìƒì„±
            test_report = self._generate_test_report()
            
            return test_report
            
        except Exception as e:
            logger.error(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
            raise
        finally:
            self.cleanup_test_environment()
    
    async def _run_unit_tests(self):
        """ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸ“‹ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        
        test_results = []
        
        # MD í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸
        try:
            result = await self._test_md_processor()
            test_results.append(result)
        except Exception as e:
            logger.error(f"âŒ MD í”„ë¡œì„¸ì„œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            test_results.append(TestResult("md_processor").complete(False, str(e)))
        
        # Qdrant ì»¤ë„¥í„° í…ŒìŠ¤íŠ¸
        try:
            result = await self._test_qdrant_connector()
            test_results.append(result)
        except Exception as e:
            logger.error(f"âŒ Qdrant ì»¤ë„¥í„° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            test_results.append(TestResult("qdrant_connector").complete(False, str(e)))
        
        # OpenAI ì»¤ë„¥í„° í…ŒìŠ¤íŠ¸
        try:
            result = await self._test_openai_connector()
            test_results.append(result)
        except Exception as e:
            logger.error(f"âŒ OpenAI ì»¤ë„¥í„° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            test_results.append(TestResult("openai_connector").complete(False, str(e)))
        
        # ë°ì´í„°ì…‹ ìƒì„±ê¸° í…ŒìŠ¤íŠ¸
        try:
            result = await self._test_dataset_generator()
            test_results.append(result)
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ì…‹ ìƒì„±ê¸° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            test_results.append(TestResult("dataset_generator").complete(False, str(e)))
        
        # í’ˆì§ˆ ê²€ì¦ê¸° í…ŒìŠ¤íŠ¸
        try:
            result = await self._test_quality_validator()
            test_results.append(result)
        except Exception as e:
            logger.error(f"âŒ í’ˆì§ˆ ê²€ì¦ê¸° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            test_results.append(TestResult("quality_validator").complete(False, str(e)))
        
        self.results.extend(test_results)
        
        # ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
        passed = sum(1 for r in test_results if r.passed)
        total = len(test_results)
        logger.info(f"âœ… ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {passed}/{total} í†µê³¼")
    
    async def _test_md_processor(self) -> TestResult:
        """MD í”„ë¡œì„¸ì„œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸"""
        result = TestResult("md_processor")
        
        try:
            # í…ŒìŠ¤íŠ¸ìš© MD íŒŒì¼ ìƒì„±
            test_md_content = """---
title: "Test Document"
component: "Test"
page: "001"
---

# Test Document

This is a test document for MD processor testing.

```python
def test_function():
    print("Hello, World!")
```

## Test Section

This is a test section with some content.
"""
            
            test_md_file = Path(self.config.test_temp_dir) / "test.md"
            with open(test_md_file, 'w', encoding='utf-8') as f:
                f.write(test_md_content)
            
            # MD í”„ë¡œì„¸ì„œ ìƒì„± ë° í…ŒìŠ¤íŠ¸
            processing_config = ProcessingConfig(
                output_path=Path(self.config.test_output_dir),
                staging_path=Path(self.config.test_temp_dir)
            )
            md_processor = create_processor(processing_config)
            
            # ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
            document = md_processor._process_single_file(test_md_file, None)
            
            assert document is not None, "ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨"
            assert "title" in document, "ì œëª© í•„ë“œ ëˆ„ë½"
            assert "content" in document, "ì½˜í…ì¸  í•„ë“œ ëˆ„ë½"
            assert "metadata" in document, "ë©”íƒ€ë°ì´í„° í•„ë“œ ëˆ„ë½"
            
            result.complete(True, details={
                "document_processed": True,
                "document_fields": list(document.keys()),
                "content_length": len(document.get("content", ""))
            })
            
        except Exception as e:
            result.complete(False, str(e))
        
        return result
    
    async def _test_qdrant_connector(self) -> TestResult:
        """Qdrant ì»¤ë„¥í„° ë‹¨ìœ„ í…ŒìŠ¤íŠ¸"""
        result = TestResult("qdrant_connector")
        
        try:
            # í…ŒìŠ¤íŠ¸ìš© Qdrant ì»¤ë„¥í„° ìƒì„±
            processor_config = {
                "host": self.config.qdrant.get("host", "localhost"),
                "port": self.config.qdrant.get("port", 6333),
                "collection": "test_collection"
            }
            
            integrated_processor = create_integrated_processor(processor_config)
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸ (ì‹¤íŒ¨í•´ë„ ì •ìƒìœ¼ë¡œ ì²˜ë¦¬)
            try:
                connected = integrated_processor.connect_qdrant()
                details = {"connection_attempt": True, "connected": connected}
            except Exception as e:
                details = {"connection_attempt": True, "connected": False, "error": str(e)}
            
            result.complete(True, details=details)
            
        except Exception as e:
            result.complete(False, str(e))
        
        return result
    
    async def _test_openai_connector(self) -> TestResult:
        """OpenAI ì»¤ë„¥í„° ë‹¨ìœ„ í…ŒìŠ¤íŠ¸"""
        result = TestResult("openai_connector")
        
        try:
            # í…ŒìŠ¤íŠ¸ìš© OpenAI ì»¤ë„¥í„° ìƒì„±
            openai_config = {
                "endpoint": "http://123.37.28.120:9997/v1",
                "model": "qwen2.5-vl-instruct",
                "max_tokens": 1000,
                "temperature": 0.3
            }
            
            openai_connector = create_openai_connector(openai_config)
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            connection_result = await openai_connector.test_connection()
            
            assert connection_result is not None, "ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨"
            assert "status" in connection_result, "ìƒíƒœ í•„ë“œ ëˆ„ë½"
            
            result.complete(True, details={
                "connection_test": True,
                "connection_status": connection_result.get("status"),
                "model_info": connection_result.get("model_info", {})
            })
            
        except Exception as e:
            result.complete(False, str(e))
        
        return result
    
    async def _test_dataset_generator(self) -> TestResult:
        """ë°ì´í„°ì…‹ ìƒì„±ê¸° ë‹¨ìœ„ í…ŒìŠ¤íŠ¸"""
        result = TestResult("dataset_generator")
        
        try:
            # í…ŒìŠ¤íŠ¸ìš© ëŒ€í™” ë°ì´í„° ìƒì„±
            test_conversations = [
                {
                    "conversations": [
                        {"from": "human", "value": "What is WinForms?"},
                        {"from": "gpt", "value": "WinForms is a UI framework for Windows desktop applications."}
                    ]
                },
                {
                    "conversations": [
                        {"from": "human", "value": "How to use DataGridView?"},
                        {"from": "gpt", "value": "DataGridView is used to display data in a tabular format."}
                    ]
                }
            ]
            
            # ë°ì´í„°ì…‹ ìƒì„±ê¸° ìƒì„±
            dataset_config = DatasetConfig(
                max_seq_length=2048,
                formats=["sharegpt", "alpaca"],
                train_test_split=0.8,
                output_dir=Path(self.config.test_output_dir)
            )
            
            dataset_generator = create_dataset_generator(dataset_config)
            
            # ë°ì´í„°ì…‹ ìƒì„± í…ŒìŠ¤íŠ¸
            datasets = dataset_generator.generate_from_samples(test_conversations, "test_dataset")
            
            assert datasets is not None, "ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨"
            assert len(datasets) > 0, "ìƒì„±ëœ ë°ì´í„°ì…‹ ì—†ìŒ"
            
            result.complete(True, details={
                "datasets_generated": len(datasets),
                "formats": list(datasets.keys()),
                "total_samples": sum(len(items) for items in datasets.values())
            })
            
        except Exception as e:
            result.complete(False, str(e))
        
        return result
    
    async def _test_quality_validator(self) -> TestResult:
        """í’ˆì§ˆ ê²€ì¦ê¸° ë‹¨ìœ„ í…ŒìŠ¤íŠ¸"""
        result = TestResult("quality_validator")
        
        try:
            # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹ ìƒì„±
            test_datasets = {
                "sharegpt": [
                    {
                        "conversations": [
                            {"from": "human", "value": "Test question"},
                            {"from": "gpt", "value": "Test answer"}
                        ]
                    }
                ]
            }
            
            # í’ˆì§ˆ ê²€ì¦ê¸° ìƒì„±
            quality_validator = create_default_validator()
            
            # í’ˆì§ˆ ê²€ì¦ í…ŒìŠ¤íŠ¸
            validated_datasets = quality_validator.validate_and_filter(test_datasets)
            
            assert validated_datasets is not None, "í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨"
            
            result.complete(True, details={
                "validation_completed": True,
                "original_items": len(test_datasets["sharegpt"]),
                "validated_items": len(validated_datasets.get("sharegpt", []))
            })
            
        except Exception as e:
            result.complete(False, str(e))
        
        return result
    
    async def _run_integration_tests(self):
        """í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸ”— í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        
        test_results = []
        
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
        try:
            result = await self._test_full_pipeline()
            test_results.append(result)
        except Exception as e:
            logger.error(f"âŒ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            test_results.append(TestResult("full_pipeline").complete(False, str(e)))
        
        # ë‹¨ê³„ë³„ í†µí•© í…ŒìŠ¤íŠ¸
        for step in self.config.test_steps:
            try:
                result = await self._test_step_integration(step)
                test_results.append(result)
            except Exception as e:
                logger.error(f"âŒ {step} í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
                test_results.append(TestResult(step).complete(False, str(e)))
        
        self.results.extend(test_results)
        
        # í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
        passed = sum(1 for r in test_results if r.passed)
        total = len(test_results)
        logger.info(f"âœ… í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {passed}/{total} í†µê³¼")
    
    async def _test_full_pipeline(self) -> TestResult:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© í…ŒìŠ¤íŠ¸"""
        result = TestResult("full_pipeline")
        
        try:
            # í…ŒìŠ¤íŠ¸ìš© ì„¤ì • ìƒì„±
            from main import PipelineConfig, DatasetGenerationPipeline
            
            config = PipelineConfig(
                test_mode=True,
                sample_size=5,
                output_directory=self.config.test_output_dir,
                steps=["md_processing", "conversation_generation", "dataset_generation"]
            )
            
            pipeline = DatasetGenerationPipeline(config)
            
            # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ì‹¤ì œ ì‹¤í–‰ ëŒ€ì‹  ëª¨í‚¹)
            # ì‹¤ì œ ì‹¤í–‰ì€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ì—ì„œ ìˆ˜í–‰
            result.complete(True, details={
                "pipeline_test": True,
                "config_loaded": True,
                "pipeline_created": True
            })
            
        except Exception as e:
            result.complete(False, str(e))
        
        return result
    
    async def _test_step_integration(self, step: str) -> TestResult:
        """ë‹¨ê³„ë³„ í†µí•© í…ŒìŠ¤íŠ¸"""
        result = TestResult(f"step_{step}")
        
        try:
            # ê° ë‹¨ê³„ë³„ í†µí•© í…ŒìŠ¤íŠ¸
            if step == "md_processing":
                await self._test_md_processing_integration()
            elif step == "qdrant_search":
                await self._test_qdrant_search_integration()
            elif step == "conversation_generation":
                await self._test_conversation_generation_integration()
            elif step == "dataset_generation":
                await self._test_dataset_generation_integration()
            elif step == "quality_validation":
                await self._test_quality_validation_integration()
            
            result.complete(True, details={f"{step}_integration": True})
            
        except Exception as e:
            result.complete(False, str(e))
        
        return result
    
    async def _test_md_processing_integration(self):
        """MD ì²˜ë¦¬ í†µí•© í…ŒìŠ¤íŠ¸"""
        # MD íŒŒì¼ ìƒì„±
        test_md_content = """---
title: "Integration Test Document"
component: "IntegrationTest"
page: "001"
---

# Integration Test

This is an integration test document.
"""
        
        test_md_file = Path(self.config.test_temp_dir) / "integration_test.md"
        with open(test_md_file, 'w', encoding='utf-8') as f:
            f.write(test_md_content)
        
        # MD í”„ë¡œì„¸ì„œë¡œ ì²˜ë¦¬
        processing_config = ProcessingConfig(
            output_path=Path(self.config.test_output_dir),
            staging_path=Path(self.config.test_temp_dir)
        )
        md_processor = create_processor(processing_config)
        
        # íŒŒì¼ ìŠ¤ìº” ë° ì²˜ë¦¬
        scanner = create_scanner([self.config.test_temp_dir])
        md_files = list(scanner.scan_files(recursive=True))
        
        assert len(md_files) > 0, "MD íŒŒì¼ ìŠ¤ìº” ì‹¤íŒ¨"
        
        # íŒŒì¼ ì²˜ë¦¬
        documents = []
        for md_file, _ in md_files:
            doc = md_processor._process_single_file(md_file, None)
            if doc:
                documents.append(doc)
        
        assert len(documents) > 0, "ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨"
    
    async def _test_qdrant_search_integration(self):
        """Qdrant ê²€ìƒ‰ í†µí•© í…ŒìŠ¤íŠ¸"""
        processor_config = {
            "host": self.config.qdrant.get("host", "localhost"),
            "port": self.config.qdrant.get("port", 6333),
            "collection": "test_collection"
        }
        
        integrated_processor = create_integrated_processor(processor_config)
        
        # ì—°ê²° ì‹œë„
        try:
            connected = integrated_processor.connect_qdrant()
            if not connected:
                logger.warning("Qdrant ì—°ê²° ì‹¤íŒ¨, í…ŒìŠ¤íŠ¸ ê±´ë„ˆëœ€")
                return
        except Exception as e:
            logger.warning(f"Qdrant ì—°ê²° ì˜ˆì™¸: {str(e)}, í…ŒìŠ¤íŠ¸ ê±´ë„ˆëœ€")
            return
        
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        search_results = integrated_processor.search_qdrant_documents(
            query="test", limit=5
        )
        
        # ê²°ê³¼ê°€ ì—†ì–´ë„ ì •ìƒìœ¼ë¡œ ì²˜ë¦¬
        logger.info(f"Qdrant ê²€ìƒ‰ ê²°ê³¼: {len(search_results) if search_results else 0}ê°œ")
    
    async def _test_conversation_generation_integration(self):
        """ëŒ€í™” ìƒì„± í†µí•© í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œ ìƒì„±
        test_documents = [
            {
                "title": "Test Document 1",
                "content": "This is a test document about WinForms.",
                "metadata": {"component": "Test", "page": "001"}
            },
            {
                "title": "Test Document 2", 
                "content": "This is another test document about WinForms.",
                "metadata": {"component": "Test", "page": "002"}
            }
        ]
        
        # OpenAI ì»¤ë„¥í„° ìƒì„±
        openai_config = {
            "endpoint": "http://123.37.28.120:9997/v1",
            "model": "qwen2.5-vl-instruct",
            "max_tokens": 1000,
            "temperature": 0.3
        }
        
        openai_connector = create_openai_connector(openai_config)
        
        # ëŒ€í™” ìƒì„± (ì†ŒëŸ‰ìœ¼ë¡œ í…ŒìŠ¤íŠ¸)
        conversations = await openai_connector.generate_conversations(
            test_documents[:1]  # 1ê°œ ë¬¸ì„œë§Œ í…ŒìŠ¤íŠ¸
        )
        
        assert conversations is not None, "ëŒ€í™” ìƒì„± ì‹¤íŒ¨"
        assert len(conversations) > 0, "ìƒì„±ëœ ëŒ€í™” ì—†ìŒ"
    
    async def _test_dataset_generation_integration(self):
        """ë°ì´í„°ì…‹ ìƒì„± í†µí•© í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ìš© ëŒ€í™” ë°ì´í„°
        test_conversations = [
            {
                "conversations": [
                    {"from": "human", "value": "What is WinForms?"},
                    {"from": "gpt", "value": "WinForms is a UI framework for Windows desktop applications."}
                ]
            }
        ]
        
        # ë°ì´í„°ì…‹ ìƒì„±ê¸° ìƒì„±
        dataset_config = DatasetConfig(
            max_seq_length=2048,
            formats=["sharegpt"],
            train_test_split=0.8,
            output_dir=Path(self.config.test_output_dir)
        )
        
        dataset_generator = create_dataset_generator(dataset_config)
        
        # ë°ì´í„°ì…‹ ìƒì„±
        datasets = dataset_generator.generate_from_samples(
            test_conversations, "integration_test"
        )
        
        assert datasets is not None, "ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨"
        assert len(datasets) > 0, "ìƒì„±ëœ ë°ì´í„°ì…‹ ì—†ìŒ"
    
    async def _test_quality_validation_integration(self):
        """í’ˆì§ˆ ê²€ì¦ í†µí•© í…ŒìŠ¤íŠ¸"""
        # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹
        test_datasets = {
            "sharegpt": [
                {
                    "conversations": [
                        {"from": "human", "value": "Test question"},
                        {"from": "gpt", "value": "Test answer"}
                    ]
                }
            ]
        }
        
        # í’ˆì§ˆ ê²€ì¦ê¸° ìƒì„±
        quality_validator = create_default_validator()
        
        # í’ˆì§ˆ ê²€ì¦
        validated_datasets = quality_validator.validate_and_filter(test_datasets)
        
        assert validated_datasets is not None, "í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨"
    
    async def _run_performance_tests(self):
        """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("âš¡ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        
        test_results = []
        
        # MD ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        try:
            result = await self._test_md_processing_performance()
            test_results.append(result)
        except Exception as e:
            logger.error(f"âŒ MD ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            test_results.append(TestResult("md_processing_performance").complete(False, str(e)))
        
        # ëŒ€í™” ìƒì„± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        try:
            result = await self._test_conversation_generation_performance()
            test_results.append(result)
        except Exception as e:
            logger.error(f"âŒ ëŒ€í™” ìƒì„± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            test_results.append(TestResult("conversation_generation_performance").complete(False, str(e)))
        
        # ë°ì´í„°ì…‹ ìƒì„± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        try:
            result = await self._test_dataset_generation_performance()
            test_results.append(result)
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ì…‹ ìƒì„± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            test_results.append(TestResult("dataset_generation_performance").complete(False, str(e)))
        
        self.results.extend(test_results)
        
        # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
        passed = sum(1 for r in test_results if r.passed)
        total = len(test_results)
        logger.info(f"âœ… ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {passed}/{total} í†µê³¼")
    
    async def _test_md_processing_performance(self) -> TestResult:
        """MD ì²˜ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        result = TestResult("md_processing_performance")
        
        try:
            # í…ŒìŠ¤íŠ¸ìš© MD íŒŒì¼ ìƒì„± (ì—¬ëŸ¬ ê°œ)
            test_files = []
            for i in range(5):
                test_md_content = f"""---
title: "Performance Test Document {i}"
component: "PerformanceTest"
page: "{i:03d}"
---

# Performance Test Document {i}

This is a performance test document with some content.

```python
def test_function_{i}():
    print("Hello, World!")
```

## Test Section {i}

This is a test section with some content for performance testing.
"""
                
                test_md_file = Path(self.config.test_temp_dir) / f"perf_test_{i}.md"
                with open(test_md_file, 'w', encoding='utf-8') as f:
                    f.write(test_md_content)
                test_files.append(test_md_file)
            
            # ì„±ëŠ¥ ì¸¡ì •
            start_time = time.time()
            
            processing_config = ProcessingConfig(
                output_path=Path(self.config.test_output_dir),
                staging_path=Path(self.config.test_temp_dir)
            )
            md_processor = create_processor(processing_config)
            
            # íŒŒì¼ ì²˜ë¦¬
            documents = []
            for md_file in test_files:
                doc = md_processor._process_single_file(md_file, None)
                if doc:
                    documents.append(doc)
            
            end_time = time.time()
            duration = end_time - start_time
            
            # ì„±ëŠ¥ ê²€ì¦
            threshold = self.config.performance_thresholds.get("md_processing_time", 30.0)
            passed = duration <= threshold
            
            result.complete(passed, details={
                "duration": duration,
                "threshold": threshold,
                "processed_files": len(test_files),
                "processed_documents": len(documents),
                "files_per_second": len(test_files) / duration if duration > 0 else 0
            })
            
        except Exception as e:
            result.complete(False, str(e))
        
        return result
    
    async def _test_conversation_generation_performance(self) -> TestResult:
        """ëŒ€í™” ìƒì„± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        result = TestResult("conversation_generation_performance")
        
        try:
            # í…ŒìŠ¤íŠ¸ìš© ë¬¸ì„œ ìƒì„±
            test_documents = [
                {
                    "title": f"Performance Test Doc {i}",
                    "content": f"This is a performance test document {i} about WinForms.",
                    "metadata": {"component": "PerformanceTest", "page": f"{i:03d}"}
                }
                for i in range(3)  # 3ê°œ ë¬¸ì„œë¡œ í…ŒìŠ¤íŠ¸
            ]
            
            # ì„±ëŠ¥ ì¸¡ì •
            start_time = time.time()
            
            openai_config = {
                "endpoint": "http://123.37.28.120:9997/v1",
                "model": "qwen2.5-vl-instruct",
                "max_tokens": 1000,
                "temperature": 0.3
            }
            
            openai_connector = create_openai_connector(openai_config)
            
            # ëŒ€í™” ìƒì„±
            conversations = await openai_connector.generate_conversations(test_documents)
            
            end_time = time.time()
            duration = end_time - start_time
            
            # ì„±ëŠ¥ ê²€ì¦
            threshold = self.config.performance_thresholds.get("conversation_generation_time", 300.0)
            passed = duration <= threshold
            
            result.complete(passed, details={
                "duration": duration,
                "threshold": threshold,
                "processed_documents": len(test_documents),
                "generated_conversations": len(conversations) if conversations else 0,
                "conversations_per_second": (len(conversations) if conversations else 0) / duration if duration > 0 else 0
            })
            
        except Exception as e:
            result.complete(False, str(e))
        
        return result
    
    async def _test_dataset_generation_performance(self) -> TestResult:
        """ë°ì´í„°ì…‹ ìƒì„± ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        result = TestResult("dataset_generation_performance")
        
        try:
            # í…ŒìŠ¤íŠ¸ìš© ëŒ€í™” ë°ì´í„°
            test_conversations = [
                {
                    "conversations": [
                        {"from": "human", "value": f"What is WinForms test {i}?"},
                        {"from": "gpt", "value": f"WinForms test {i} is a UI framework for Windows desktop applications."}
                    ]
                }
                for i in range(5)  # 5ê°œ ëŒ€í™”ë¡œ í…ŒìŠ¤íŠ¸
            ]
            
            # ì„±ëŠ¥ ì¸¡ì •
            start_time = time.time()
            
            dataset_config = DatasetConfig(
                max_seq_length=2048,
                formats=["sharegpt"],
                train_test_split=0.8,
                output_dir=Path(self.config.test_output_dir)
            )
            
            dataset_generator = create_dataset_generator(dataset_config)
            
            # ë°ì´í„°ì…‹ ìƒì„±
            datasets = dataset_generator.generate_from_samples(
                test_conversations, "performance_test"
            )
            
            end_time = time.time()
            duration = end_time - start_time
            
            # ì„±ëŠ¥ ê²€ì¦
            threshold = self.config.performance_thresholds.get("dataset_generation_time", 120.0)
            passed = duration <= threshold
            
            result.complete(passed, details={
                "duration": duration,
                "threshold": threshold,
                "processed_conversations": len(test_conversations),
                "generated_datasets": len(datasets) if datasets else 0,
                "total_samples": sum(len(items) for items in datasets.values()) if datasets else 0,
                "samples_per_second": (sum(len(items) for items in datasets.values()) if datasets else 0) / duration if duration > 0 else 0
            })
            
        except Exception as e:
            result.complete(False, str(e))
        
        return result
    
    async def _run_quality_tests(self):
        """í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ğŸ” í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘...")
        
        test_results = []
        
        # ë°ì´í„° í’ˆì§ˆ í…ŒìŠ¤íŠ¸
        try:
            result = await self._test_data_quality()
            test_results.append(result)
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            test_results.append(TestResult("data_quality").complete(False, str(e)))
        
        # í¬ë§· í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸
        try:
            result = await self._test_format_compatibility()
            test_results.append(result)
        except Exception as e:
            logger.error(f"âŒ í¬ë§· í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            test_results.append(TestResult("format_compatibility").complete(False, str(e)))
        
        # ì•ˆì „ì„± í…ŒìŠ¤íŠ¸
        try:
            result = await self._test_safety()
            test_results.append(result)
        except Exception as e:
            logger.error(f"âŒ ì•ˆì „ì„± í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            test_results.append(TestResult("safety").complete(False, str(e)))
        
        self.results.extend(test_results)
        
        # í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
        passed = sum(1 for r in test_results if r.passed)
        total = len(test_results)
        logger.info(f"âœ… í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {passed}/{total} í†µê³¼")
    
    async def _test_data_quality(self) -> TestResult:
        """ë°ì´í„° í’ˆì§ˆ í…ŒìŠ¤íŠ¸"""
        result = TestResult("data_quality")
        
        try:
            # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹ ìƒì„±
            test_datasets = {
                "sharegpt": [
                    {
                        "conversations": [
                            {"from": "human", "value": "What is WinForms?"},
                            {"from": "gpt", "value": "WinForms is a comprehensive UI framework for building Windows desktop applications with rich controls and features."}
                        ]
                    },
                    {
                        "conversations": [
                            {"from": "human", "value": ""},
                            {"from": "gpt", "value": "Empty question test"}
                        ]
                    }
                ]
            }
            
            # í’ˆì§ˆ ê²€ì¦ê¸° ìƒì„±
            quality_validator = create_default_validator()
            
            # í’ˆì§ˆ ê²€ì¦
            validated_datasets = quality_validator.validate_and_filter(test_datasets)
            
            # í’ˆì§ˆ ê²€ì¦ ê²°ê³¼ í™•ì¸
            validation_summary = quality_validator.get_validation_summary(validated_datasets)
            
            assert validation_summary is not None, "í’ˆì§ˆ ê²€ì¦ ê²°ê³¼ ì—†ìŒ"
            assert "total_items" in validation_summary, "ì „ì²´ í•­ëª© ìˆ˜ ì •ë³´ ëˆ„ë½"
            assert "valid_items" in validation_summary, "ìœ íš¨ í•­ëª© ìˆ˜ ì •ë³´ ëˆ„ë½"
            
            result.complete(True, details={
                "validation_summary": validation_summary,
                "original_items": validation_summary.get("total_items", 0),
                "valid_items": validation_summary.get("valid_items", 0),
                "validation_rate": validation_summary.get("validation_rate", 0)
            })
            
        except Exception as e:
            result.complete(False, str(e))
        
        return result
    
    async def _test_format_compatibility(self) -> TestResult:
        """í¬ë§· í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸"""
        result = TestResult("format_compatibility")
        
        try:
            # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹ ìƒì„±
            test_conversations = [
                {
                    "conversations": [
                        {"from": "human", "value": "What is WinForms?"},
                        {"from": "gpt", "value": "WinForms is a UI framework for Windows desktop applications."}
                    ]
                }
            ]
            
            # ë°ì´í„°ì…‹ ìƒì„±ê¸° ìƒì„±
            dataset_config = DatasetConfig(
                max_seq_length=8192,
                formats=["sharegpt", "alpaca", "openai"],
                train_test_split=0.8,
                output_dir=Path(self.config.test_output_dir)
            )
            
            dataset_generator = create_dataset_generator(dataset_config)
            
            # ëª¨ë“  í¬ë§·ìœ¼ë¡œ ë°ì´í„°ì…‹ ìƒì„±
            datasets = dataset_generator.generate_from_samples(
                test_conversations, "compatibility_test"
            )
            
            # ê° í¬ë§· í˜¸í™˜ì„± ê²€ì¦
            compatibility_results = {}
            for format_name, format_data in datasets.items():
                assert len(format_data) > 0, f"{format_name} í¬ë§· ë°ì´í„° ìƒì„± ì‹¤íŒ¨"
                compatibility_results[format_name] = len(format_data)
            
            result.complete(True, details={
                "formats_generated": list(datasets.keys()),
                "format_counts": compatibility_results,
                "total_samples": sum(len(items) for items in datasets.values())
            })
            
        except Exception as e:
            result.complete(False, str(e))
        
        return result
    
    async def _test_safety(self) -> TestResult:
        """ì•ˆì „ì„± í…ŒìŠ¤íŠ¸"""
        result = TestResult("safety")
        
        try:
            # í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹ (ì•ˆì „í•˜ì§€ ì•Šì€ ë‚´ìš© í¬í•¨)
            test_datasets = {
                "sharegpt": [
                    {
                        "conversations": [
                            {"from": "human", "value": "What is WinForms?"},
                            {"from": "gpt", "value": "WinForms is a safe UI framework."}
                        ]
                    },
                    {
                        "conversations": [
                            {"from": "human", "value": "Test with potentially harmful content"},
                            {"from": "gpt", "value": "This is a normal response."}
                        ]
                    }
                ]
            }
            
            # í’ˆì§ˆ ê²€ì¦ê¸° ìƒì„±
            quality_validator = create_default_validator()
            
            # ì•ˆì „ì„± ê²€ì¦
            validated_datasets = quality_validator.validate_and_filter(test_datasets)
            
            # ê²€ì¦ ê²°ê³¼ í™•ì¸
            validation_summary = quality_validator.get_validation_summary(validated_datasets)
            
            assert validation_summary is not None, "ì•ˆì „ì„± ê²€ì¦ ê²°ê³¼ ì—†ìŒ"
            
            result.complete(True, details={
                "safety_check_completed": True,
                "validation_summary": validation_summary,
                "original_items": validation_summary.get("total_items", 0),
                "valid_items": validation_summary.get("valid_items", 0)
            })
            
        except Exception as e:
            result.complete(False, str(e))
        
        return result
    
    def _generate_test_report(self) -> Dict[str, Any]:
        """í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        end_time = datetime.now()
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ í†µê³„
        total_tests = len(self.results)
        passed_tests = sum(1 for r in self.results if r.passed)
        failed_tests = total_tests - passed_tests
        
        # ì„±ê³µë¥  ê³„ì‚°
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        # ì´ ì‹¤í–‰ ì‹œê°„
        total_duration = sum(r.duration for r in self.results)
        
        # ìƒì„¸ ê²°ê³¼
        detailed_results = [r.to_dict() for r in self.results]
        
        # í…ŒìŠ¤íŠ¸ ìœ í˜•ë³„ í†µê³„
        unit_tests = [r for r in self.results if "unit" in r.test_name.lower() or r.test_name in ["md_processor", "qdrant_connector", "openai_connector", "dataset_generator", "quality_validator"]]
        integration_tests = [r for r in self.results if "integration" in r.test_name.lower() or "step_" in r.test_name or r.test_name == "full_pipeline"]
        performance_tests = [r for r in self.results if "performance" in r.test_name.lower()]
        quality_tests = [r for r in self.results if r.test_name in ["data_quality", "format_compatibility", "safety"]]
        
        report = {
            "test_summary": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": failed_tests,
                "success_rate": success_rate,
                "total_duration": total_duration,
                "start_time": self.results[0].start_time if self.results else None,
                "end_time": end_time.isoformat()
            },
            "test_categories": {
                "unit_tests": {
                    "count": len(unit_tests),
                    "passed": sum(1 for r in unit_tests if r.passed),
                    "failed": len(unit_tests) - sum(1 for r in unit_tests if r.passed)
                },
                "integration_tests": {
                    "count": len(integration_tests),
                    "passed": sum(1 for r in integration_tests if r.passed),
                    "failed": len(integration_tests) - sum(1 for r in integration_tests if r.passed)
                },
                "performance_tests": {
                    "count": len(performance_tests),
                    "passed": sum(1 for r in performance_tests if r.passed),
                    "failed": len(performance_tests) - sum(1 for r in performance_tests if r.passed)
                },
                "quality_tests": {
                    "count": len(quality_tests),
                    "passed": sum(1 for r in quality_tests if r.passed),
                    "failed": len(quality_tests) - sum(1 for r in quality_tests if r.passed)
                }
            },
            "detailed_results": detailed_results,
            "failed_tests": [r.to_dict() for r in self.results if not r.passed],
            "performance_thresholds": self.config.performance_thresholds,
            "test_config": {
                "test_mode": self.config.test_mode,
                "sample_size": self.config.sample_size,
                "run_unit_tests": self.config.run_unit_tests,
                "run_integration_tests": self.config.run_integration_tests,
                "run_performance_tests": self.config.run_performance_tests,
                "run_quality_tests": self.config.run_quality_tests
            }
        }
        
        # ë¦¬í¬íŠ¸ íŒŒì¼ ì €ì¥
        report_path = Path(self.config.test_output_dir) / "test_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        return report


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="í†µí•© í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--config", "-c", type=str, default="config.yaml",
                       help="ì„¤ì • íŒŒì¼ ê²½ë¡œ")
    parser.add_argument("--test-mode", action="store_true", default=True,
                       help="í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰")
    parser.add_argument("--sample-size", type=int, default=10,
                       help="í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ í¬ê¸°")
    parser.add_argument("--verbose", "-v", action="store_true",
                       help="ìƒì„¸ ë¡œê·¸ ì¶œë ¥")
    parser.add_argument("--progress-bar", action="store_true", default=True,
                       help="ì§„í–‰ë¥  ë°” í‘œì‹œ")
    parser.add_argument("--unit-tests", action="store_true", default=True,
                       help="ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    parser.add_argument("--integration-tests", action="store_true", default=True,
                       help="í†µí•© í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    parser.add_argument("--performance-tests", action="store_true", default=True,
                       help="ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    parser.add_argument("--quality-tests", action="store_true", default=True,
                       help="í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
    parser.add_argument("--output-dir", type=str, default="./test_output",
                       help="í…ŒìŠ¤íŠ¸ ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--temp-dir", type=str, default="./test_temp",
                       help="í…Œì„í”„ ë””ë ‰í† ë¦¬")
    
    args = parser.parse_args()
    
    # í…ŒìŠ¤íŠ¸ ì„¤ì • ìƒì„±
    test_config = TestConfig(
        test_mode=args.test_mode,
        sample_size=args.sample_size,
        verbose=args.verbose,
        progress_bar=args.progress_bar,
        run_unit_tests=args.unit_tests,
        run_integration_tests=args.integration_tests,
        run_performance_tests=args.performance_tests,
        run_quality_tests=args.quality_tests,
        test_output_dir=args.output_dir,
        test_temp_dir=args.temp_dir
    )
    
    # í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ìƒì„± ë° ì‹¤í–‰
    test_suite = IntegrationTestSuite(test_config)
    
    try:
        result = asyncio.run(test_suite.run_all_tests())
        
        print("\n" + "="*60)
        print("ğŸ‰ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("="*60)
        print(f"ì´ í…ŒìŠ¤íŠ¸ ìˆ˜: {result['test_summary']['total_tests']}")
        print(f"í†µê³¼ í…ŒìŠ¤íŠ¸: {result['test_summary']['passed_tests']}")
        print(f"ì‹¤íŒ¨ í…ŒìŠ¤íŠ¸: {result['test_summary']['failed_tests']}")
        print(f"ì„±ê³µë¥ : {result['test_summary']['success_rate']:.1f}%")
        print(f"ì´ ì‹¤í–‰ ì‹œê°„: {result['test_summary']['total_duration']:.2f}ì´ˆ")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼
        categories = result['test_categories']
        for category, stats in categories.items():
            print(f"\n{category.replace('_', ' ').title()}:")
            print(f"  í…ŒìŠ¤íŠ¸ ìˆ˜: {stats['count']}")
            print(f"  í†µê³¼: {stats['passed']}")
            print(f"  ì‹¤íŒ¨: {stats['failed']}")
        
        if result['test_summary']['failed_tests'] > 0:
            print(f"\nâŒ ì‹¤íŒ¨í•œ í…ŒìŠ¤íŠ¸:")
            for failed_test in result['failed_tests']:
                print(f"  - {failed_test['test_name']}: {failed_test['error']}")
        
        print(f"\nğŸ“„ ìƒì„¸ ë¦¬í¬íŠ¸: {args.output_dir}/test_report.json")
        
        # ì¢…ë£Œ ì½”ë“œ ì„¤ì •
        sys.exit(0 if result['test_summary']['failed_tests'] == 0 else 1)
        
    except KeyboardInterrupt:
        print("\nâŒ í…ŒìŠ¤íŠ¸ê°€ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()