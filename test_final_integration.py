#!/usr/bin/env python3
"""
ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸
OpenAI ì»¤ë„¥í„° ëª¨ë“ˆì˜ ëª¨ë“  ê¸°ëŠ¥ì„ ì¢…í•©ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
"""

import asyncio
import json
import logging
from datetime import datetime
from pathlib import Path

from openai_connector import (
    create_openai_client,
    create_prompt_engine,
    create_token_manager,
    create_conversation_generator,
    OpenAIAPIClientConfig,
    PromptEngineConfig,
    TokenConfig,
    ConversationConfig,
    GenerationMode
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


async def test_complete_workflow():
    """ì™„ì „í•œ ì›Œí¬í”Œë¡œìš° í…ŒìŠ¤íŠ¸"""
    print("=== OpenAI ì»¤ë„¥í„° ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸ ===\n")
    
    # 1. ì„¤ì • ë° ì´ˆê¸°í™”
    logger.info("1. ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
    
    client_config = OpenAIAPIClientConfig(
        endpoint="http://localhost:9997/v1",
        model="qwen2.5-vl-instruct",
        api_key="integration-test-key",
        max_tokens=150,
        temperature=0.3,
        max_concurrent=2,
        timeout=30
    )
    
    prompt_config = PromptEngineConfig(
        generation_mode="balanced",
        max_turns=3,
        batch_size=2
    )
    
    token_config = TokenConfig(
        max_context_length=8192,
        max_request_tokens=2000,
        enable_optimization=True,
        track_usage_history=True
    )
    
    conversation_config = ConversationConfig(
        generation_mode=GenerationMode.LLM_ASSISTED,
        target_count=3,
        max_turns=2,
        batch_size=2,
        max_concurrent=2,
        temperature=0.3,
        include_metadata=True
    )
    
    # ì»´í¬ë„ŒíŠ¸ ìƒì„±
    client = create_openai_client(client_config)
    prompt_engine = create_prompt_engine(prompt_config)
    token_manager = create_token_manager(token_config)
    conversation_generator = create_conversation_generator(
        client, prompt_engine, token_manager, conversation_config
    )
    
    logger.info("âœ“ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    # 2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„
    logger.info("2. í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    
    test_documents = [
        {
            'id': 'sf_grid_001',
            'component': 'GridControl',
            'title': 'GridControl ë°ì´í„° ë°”ì¸ë”©',
            'content': 'Syncfusion GridControlì˜ ë°ì´í„° ë°”ì¸ë”© ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ì»¬ë ‰ì…˜ì˜ ë°ì´í„°ë¥¼ ê·¸ë¦¬ë“œì— í‘œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
            'quality_score': 0.85,
            'difficulty': 'Intermediate',
            'category': 'DataBinding'
        },
        {
            'id': 'sf_chart_001',
            'component': 'ChartControl',
            'title': 'ChartControl ì‹¤ì‹œê°„ ì°¨íŠ¸',
            'content': 'ChartControlì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ëŠ” ì°¨íŠ¸ë¥¼ ìƒì„±í•˜ê³  ë‹¤ì–‘í•œ ì‹œê°í™” ì˜µì…˜ì„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.',
            'quality_score': 0.90,
            'difficulty': 'Advanced',
            'category': 'Visualization'
        },
        {
            'id': 'sf_tree_001',
            'component': 'TreeViewAdv',
            'title': 'TreeViewAdv ë…¸ë“œ ê´€ë¦¬',
            'content': 'TreeViewAdvì—ì„œ ë™ì ìœ¼ë¡œ ë…¸ë“œë¥¼ ì¶”ê°€, ì‚­ì œ, ìˆ˜ì •í•˜ê³  ì²´í¬ë°•ìŠ¤ ë° ì´ë¯¸ì§€ë¥¼ í™œìš©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.',
            'quality_score': 0.88,
            'difficulty': 'Beginner',
            'category': 'Navigation'
        }
    ]
    
    logger.info(f"âœ“ {len(test_documents)}ê°œ í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì¤€ë¹„ ì™„ë£Œ")
    
    # 3. API ì—°ê²° ë° ê¸°ë³¸ ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    logger.info("3. API ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
    
    async with client as api_client:
        try:
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            health = await api_client.test_connection()
            
            if health["status"] != "success":
                logger.error(f"API ì—°ê²° ì‹¤íŒ¨: {health}")
                return False
            
            logger.info(f"âœ“ API ì—°ê²° ì„±ê³µ - {health['model']}")
            
            # 4. í”„ë¡¬í”„íŠ¸ ì—”ì§„ í…ŒìŠ¤íŠ¸
            logger.info("4. í”„ë¡¬í”„íŠ¸ ì—”ì§„ í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            prompt_test_results = []
            for doc in test_documents:
                # í”„ë¡¬í”„íŠ¸ ìƒì„±
                prompts = prompt_engine.generate_conversation_prompt(doc)
                
                # í’ˆì§ˆ ê²€ì¦
                if len(prompts) > 1:
                    user_prompt = prompts[1]['content']
                    quality = prompt_engine.validate_prompt_quality(user_prompt)
                    
                    prompt_test_results.append({
                        'component': doc['component'],
                        'prompt_count': len(prompts),
                        'quality_score': quality['score'],
                        'is_high_quality': quality['is_high_quality']
                    })
            
            avg_quality = sum(r['quality_score'] for r in prompt_test_results) / len(prompt_test_results)
            high_quality_count = sum(1 for r in prompt_test_results if r['is_high_quality'])
            
            logger.info(f"âœ“ í”„ë¡¬í”„íŠ¸ ì—”ì§„ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            logger.info(f"  - í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_quality:.1f}/100")
            logger.info(f"  - ê³ í’ˆì§ˆ í”„ë¡¬í”„íŠ¸: {high_quality_count}/{len(prompt_test_results)}")
            
            # 5. í† í° ê´€ë¦¬ì í…ŒìŠ¤íŠ¸
            logger.info("5. í† í° ê´€ë¦¬ì í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            token_test_results = []
            for doc in test_documents:
                prompts = prompt_engine.generate_conversation_prompt(doc)
                estimated_tokens = token_manager.estimate_tokens(prompts)
                can_process = token_manager.check_token_limit(estimated_tokens)
                
                token_test_results.append({
                    'component': doc['component'],
                    'estimated_tokens': estimated_tokens,
                    'can_process': can_process
                })
            
            total_estimated_tokens = sum(r['estimated_tokens'] for r in token_test_results)
            processable_count = sum(1 for r in token_test_results if r['can_process'])
            
            logger.info(f"âœ“ í† í° ê´€ë¦¬ì í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            logger.info(f"  - ì´ ì˜ˆìƒ í† í°: {total_estimated_tokens}")
            logger.info(f"  - ì²˜ë¦¬ ê°€ëŠ¥í•œ ë¬¸ì„œ: {processable_count}/{len(token_test_results)}")
            
            # 6. ëŒ€í™” ìƒì„± í…ŒìŠ¤íŠ¸
            logger.info("6. ëŒ€í™” ìƒì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            start_time = datetime.now()
            conversations = await conversation_generator.generate_conversations(
                test_documents,
                target_count=3
            )
            end_time = datetime.now()
            
            processing_time = (end_time - start_time).total_seconds()
            
            if conversations:
                logger.info(f"âœ“ ëŒ€í™” ìƒì„± ì„±ê³µ")
                logger.info(f"  - ìƒì„±ëœ ëŒ€í™” ìˆ˜: {len(conversations)}")
                logger.info(f"  - ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
                logger.info(f"  - ì²˜ë¦¬ ì†ë„: {len(conversations) / processing_time:.2f} ëŒ€í™”/ì´ˆ")
                
                # ëŒ€í™” í’ˆì§ˆ ë¶„ì„
                total_turns = sum(len(conv.conversations) for conv in conversations)
                total_tokens = sum(conv.metadata.get('tokens_used', 0) for conv in conversations)
                avg_tokens_per_conversation = total_tokens / len(conversations)
                avg_turns_per_conversation = total_turns / len(conversations)
                
                logger.info(f"  - í‰ê·  í„´ ìˆ˜: {avg_turns_per_conversation:.1f}")
                logger.info(f"  - í‰ê·  í† í° ì‚¬ìš©ëŸ‰: {avg_tokens_per_conversation:.1f}")
                
                # 7. ê²°ê³¼ ê²€ì¦
                logger.info("7. ê²°ê³¼ ê²€ì¦ ì¤‘...")
                
                validation_results = {
                    'valid_conversations': 0,
                    'valid_sharegpt_format': 0,
                    'valid_metadata': 0,
                    'component_coverage': set()
                }
                
                for conv in conversations:
                    # ê¸°ë³¸ êµ¬ì¡° ê²€ì¦
                    if hasattr(conv, 'id') and hasattr(conv, 'conversations') and hasattr(conv, 'metadata'):
                        validation_results['valid_conversations'] += 1
                    
                    # ShareGPT í¬ë§· ê²€ì¦
                    if all('from' in turn and 'value' in turn for turn in conv.conversations):
                        validation_results['valid_sharegpt_format'] += 1
                    
                    # ë©”íƒ€ë°ì´í„° ê²€ì¦
                    required_metadata = ['source_documents', 'model', 'tokens_used', 'generation_mode']
                    if all(key in conv.metadata for key in required_metadata):
                        validation_results['valid_metadata'] += 1
                    
                    # ì»´í¬ë„ŒíŠ¸ ì»¤ë²„ë¦¬ì§€
                    doc_info = conv.metadata.get('document_info', {})
                    component = doc_info.get('component')
                    if component:
                        validation_results['component_coverage'].add(component)
                
                logger.info(f"âœ“ ê²°ê³¼ ê²€ì¦ ì™„ë£Œ")
                logger.info(f"  - ìœ íš¨í•œ ëŒ€í™”: {validation_results['valid_conversations']}/{len(conversations)}")
                logger.info(f"  - ShareGPT í¬ë§· ì¤€ìˆ˜: {validation_results['valid_sharegpt_format']}/{len(conversations)}")
                logger.info(f"  - ë©”íƒ€ë°ì´í„° ì™„ì„±ë„: {validation_results['valid_metadata']}/{len(conversations)}")
                logger.info(f"  - ì»´í¬ë„ŒíŠ¸ ì»¤ë²„ë¦¬ì§€: {len(validation_results['component_coverage'])}/3")
                
                # 8. ê²°ê³¼ ì €ì¥
                logger.info("8. ê²°ê³¼ ì €ì¥ ì¤‘...")
                
                output_data = {
                    'test_info': {
                        'timestamp': datetime.now().isoformat(),
                        'test_type': 'final_integration_test',
                        'total_documents': len(test_documents),
                        'target_conversations': 3,
                        'processing_time_seconds': processing_time
                    },
                    'conversations': [conv.to_dict() for conv in conversations],
                    'statistics': {
                        'prompt_engine': {
                            'avg_quality_score': avg_quality,
                            'high_quality_count': high_quality_count
                        },
                        'token_manager': {
                            'total_estimated_tokens': total_estimated_tokens,
                            'processable_documents': processable_count
                        },
                        'conversation_generator': {
                            **conversation_generator.get_stats(),
                            'config': {
                                **conversation_generator.get_stats()['config'],
                                'generation_mode': conversation_generator.get_stats()['config']['generation_mode'].value
                            }
                        },
                        'validation': validation_results
                    }
                }
                
                # ì»´í¬ë„ŒíŠ¸ ì»¤ë²„ë¦¬ì§€ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (JSON ì§ë ¬í™”ë¥¼ ìœ„í•´)
                output_data['statistics']['validation']['component_coverage'] = list(validation_results['component_coverage'])
                
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_file = f"final_integration_test_{timestamp}.json"
                
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(output_data, f, indent=2, ensure_ascii=False)
                
                logger.info(f"âœ“ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}")
                logger.info(f"  - íŒŒì¼ í¬ê¸°: {Path(output_file).stat().st_size} bytes")
                
                # 9. ìµœì¢… í†µê³„
                logger.info("9. ìµœì¢… í†µê³„ ìš”ì•½...")
                
                client_stats = api_client.get_stats()
                token_stats = token_manager.get_stats()
                
                logger.info(f"âœ“ ìµœì¢… í†µê³„:")
                logger.info(f"  API í´ë¼ì´ì–¸íŠ¸:")
                logger.info(f"    - ì´ ìš”ì²­: {client_stats['total_requests']}")
                logger.info(f"    - ì„±ê³µë¥ : {client_stats['success_rate']:.1f}%")
                logger.info(f"  í† í° ê´€ë¦¬:")
                logger.info(f"    - ì´ í† í° ì‚¬ìš©ëŸ‰: {token_stats['usage_stats']['total_tokens']}")
                logger.info(f"    - í‰ê·  ìš”ì²­ë‹¹ í† í°: {token_stats['usage_stats']['avg_tokens_per_request']:.1f}")
                logger.info(f"  ëŒ€í™” ìƒì„±:")
                logger.info(f"    - ì„±ê³µë¥ : {conversation_generator.get_stats()['success_rate']:.1f}%")
                logger.info(f"    - ì´ í† í° ì‚¬ìš©ëŸ‰: {conversation_generator.get_stats()['total_tokens_used']}")
                
                return True
                
            else:
                logger.error("âœ— ëŒ€í™” ìƒì„± ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("OpenAI ì»¤ë„¥í„° ìµœì¢… í†µí•© í…ŒìŠ¤íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\n")
    
    success = await test_complete_workflow()
    
    if success:
        print("\nğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("OpenAI í˜¸í™˜ API ì—°ë™ ëª¨ë“ˆì´ ì •ìƒì ìœ¼ë¡œ êµ¬í˜„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        print("ë¡œê·¸ë¥¼ í™•ì¸í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ì„¸ìš”.")
    
    print("\n=== í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")


if __name__ == "__main__":
    asyncio.run(main())